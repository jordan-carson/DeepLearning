{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Notes\n",
    "\n",
    "Training some neural network architectures can take several weeks even on multiple GPUs, so it would be good if we can find a way to use what these models have learned to a new task. \n",
    "\n",
    "All about using a pre-trained network and apply it to a task of your own design, transferring what it's learned from one task to another.\n",
    "\n",
    "The approch will depend on how similar a dataset is to the dataset that a pre-trained network has seen. In other words, how transferable can certain knowledge be?\n",
    "\n",
    "### Example: VGG Network\n",
    "\n",
    "While VGG has learned to distinguish between the thousand different categories that are present in ImageNet. \n",
    "\n",
    "As VGG trained each of its convolutional layers learn to extract some info about the shapes and colors that distinguish these different objects?\n",
    "\n",
    "We know that the convolutional filters in a trained CNN are arranged in a kind of hierarchy. The filters in the first layer often detect edges or blocks of color. The second layer might detect circles, stripes and rectangles. These are still very general features that are useful in analyzing any image in almost any dataset. The filters in the final layer are much more specific. \n",
    "\n",
    "Its easy to remove the final layers of the network that are very specific to the training dataset while keeping the earlier layers. This way we can use the convolutional and pooling layers in a pre-trained network like a feature extractor that discoveres shape and color-based features from our dataset. Then we can add one or two more linear layers at the end for final classification. \n",
    "\n",
    "Method will depend on both:\n",
    "- Size of the dataset\n",
    "- level of similarity it shares with the ImageNet database\n",
    "\n",
    "**How do we use transfer learning if the dataset is very large and somewhat different than whats in ImageNet database?**\n",
    "\n",
    "So instead of using the pre-trained model as a fixed feature extractor, use the pre-trained model as a starting point and then train the entire network by modifying all the weights such that they were tuned to the correct classification task. (sebastien thurn uses Inception to create a Skin Cancer detection CNN) \n",
    "\n",
    "the head start was given from pre-trained on ImageNet. This technique is called fine tuning because it requires slightly changing or tuning all the existing parameter in a pre-trained network. Fine-Tuning often works best if the data set you are interested in is quite large. "
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
