{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Convolutional Neural Networks\n",
    "\n",
    "Any gray scale image is interpreted by a computer as an array. A grid of values where each grid cell is a pixel. for example mnist provides a image of 28x28. \n",
    "\n",
    "### Data Normalization\n",
    "\n",
    "Data normalization is an important pre-processing step. It ensures that each input (each pixel value, in this case) comes from a standard distribution. That is, the range of pixel values in one input image are the same as the range in another image. This standardization makes our model train and reach a minimum error, faster!\n",
    "\n",
    "Data normalization is typically done by subtracting the mean (the average of all pixel values) from each pixel, and then dividing the result by the standard deviation of all the pixel values. Sometimes you'll see an approximation here, where we use a mean and standard deviation of 0.5 to center the pixel values.\n",
    "\n",
    "The distribution of such data should resemble a Gaussian function centered at zero. For image inputs we need the pixel numbers to be positive, so we often choose to scale the data in a normalized range [0,1].\n",
    "\n",
    "We want to normalize each pixel value, for example in grey scale images our pixels are 0 - 255, so we want to divide them by 255. and this is important because our network relies on gradient calculations. Normalizing the pixel values helps these gradient calculations stay consistent and not get so large that they prevent a network from training. \n",
    "\n",
    "### MLPs\n",
    "\n",
    "They require flattened images. So after converting our matrix into a vector, they can be fed into the input layer of an MLP\n",
    "\n",
    "**Class Scores** Indicate how sure the network is that a given input is of a specific class. They are often represented as a vector of values or even as a bar graph indicating the relative strengths of the scores. \n",
    "\n",
    "### Loss & Optimization\n",
    "We measure any mistakes that it makes using a loss function, whose job is to measure the difference between the predicted and true class labels. Then using backpropagation we can compute the gradient of the loss with respect to the models' weights. This way we quantify how bad a particular weight is, and find out which weights in the network are responsible for any errors. Finally, using that calculation, we can choose an optimization function like *gradient descent* to give us a way to calculate a better weight value. \n",
    "\n",
    "Softmax function is a good function to use at the end to turn your logits into a probability of class scores. \n",
    "\n",
    "So our goal is to update the weights of the network in response to the mistakes it makes so that next time it sees the input image, it will predict the most likely label. We need to define some measure of exactly how far off the model currently is from perfection (ground truth). \n",
    "\n",
    "Calculate the categorical cross-entropy loss\n",
    "\n",
    "loss = lower when the prediction and label agree, and higher when the prediction and label disagree.\n",
    "\n",
    "So a loss function and backpropagation give us a way to quantify how bad a particular network weight is, based on how close a predicted and the true class label are from one another. Next, we need a way to calculate a better weight value - so we can think of the error as a mountain, where we want to descend to the lowest value. This is the role of an optimizer, the standard mthod for minimizing the loss and optimizing for the best weight values is called 'gradient descent'. We have already seen a number of ways to perform gradient descent and each method has a corresponding optimizer. \n",
    "\n",
    "### Model Validation\n",
    "\n",
    "So far we have seen how our model performs based on how the loss changes over each epoch. But the exact number of how many epochs to train for is sometimes not a given. How many epochs should we train for so that our model is accurate but its not overfitting the training data?\n",
    "\n",
    "One method thats used commonly involves breaking the dataset into three sets called training validation and test sets. And each is treated separately by the model. The model looks only at the training set when it's actively training and deciding how to modify its weights. After every training epoch, we check how the model is doing by looking at the training loss and the loss on the validation set. But its important to know that the model does not use any part of the validation set for the back propagation step. We use the training set to find all the patterns we can, and to update and determine the weights of our model. \n",
    "\n",
    "The validation set only tells us if that model is doing well on the validation set. And in this way it gives us an idea of how well the model generalizes to a set of that that is separate from the training set. The idea is, since the model doesn't use the validation set for deciding its weights, that it can tell us if we're overfitting the training set of data. Finally the test set of data is saved for checking the accuracy of the trained model. \n",
    "\n",
    "For example let's say we see after epoch 200 that our validation loss is actually beginning to increase, because its not generalizing well enough to also perform well on the validation set. So if we see this divide and how the training and validation losses decrease, you'll want to stop changing the weights of the network around the epoch we see the increase beginning / or ignore or throw away the weights from later epochs where there's evidence of overfitting. This can also be useful if we have multiple architectures to choose from. Even though the model doesn't use the validation set to update its weights, our model selection process is based on how the model performs on both the training and validation sets. So, in the end, the model is biased in favor of the validation set. Thus, we need a separate test set of data to truly see how our selected model generalizes and performs when given data it really has not seen before. \n",
    "\n",
    "### Validation Loss\n",
    "\n",
    "We Create a validation set to:\n",
    "1. Measure how well a model generalizes during training\n",
    "2. Tell us when to stop training a model; when the validation loss stops decreasing (and especially when the validation loss starts increasing and the training loss is still decreasing)\n",
    "\n",
    "### Image classification steps\n",
    "\n",
    "1. **Visualize Data**: Load and visualize the data we are working with\n",
    "2. **Pre-Process**: the data by normalizing it and converting it into a tensor, so that its prepped for futher processing by the layers of a neural network.\n",
    "3. **Define a model**: Do your research, has anyone approached this? Use this to decide on a model architecture and define it. \n",
    "4. **Train your Model**: Decide on loss and optimization functions and proceed with training your model. \n",
    "5. **Save the Best Model**: Choose to use a validation dataset to select and save the best model during training. \n",
    "6. **Test Your Model**: on previously unseen data. \n",
    "\n",
    "## MLPs vs CNNs\n",
    "\n",
    "So far, we've investigated how to train an MLP to classify handwritten digits in the MNIST dataset, the best algorithms are the ones with the least test error are the approaches that use Convolutional Neural Networks or CNNs. For most image classification tasks, they do not even compare. \n",
    "\n",
    "For MLPs we need to input out data as a simple vector of numbers with no special structure. It has no knowledge of the fact that these numbers were originally spatially arranged in a grid. \n",
    "\n",
    "CNNs, in contrast, are built for the exact purpose of working with or elucidating the patterns in multidimensional data. Unlike MLPs, CNNs understand the fact that image pixels that are closer in proximity to each other are model heavily related than pixels that are far apart. \n",
    "\n",
    "### Local Connectivity\n",
    "\n",
    "MPLs (Issues)\n",
    "- First, use a lot of parameters. **Only uses fully connected layers**\n",
    "- We get rid of all the 2D Information contained in an image when we flatten its matrix to a vector. The spatial information or knowledge of where the pixels are located in reference to each other is relevant to understanding the image and could aid significantly towards elucidating the patterns contained in the pixel values. \n",
    "\n",
    "This suggests that we need an entirely new way of processing image input, where the 2D information is not entirely lost. \n",
    "\n",
    "CNNs\n",
    "- Also use sparsely connected layers - it will address the problems by user layers that are more sparsely connected, where the connections between layers are informed by the 2D structure of the image matrix. \n",
    "- Accept our matrix as input \n",
    "\n",
    "In a MLP does every hidden node need to be connected to every pixel in the original image? Perhaps not.\n",
    "\n",
    "Instead we use a regional breakdown and the assignment of small local groups of pixels to different hidden nodes, every hidden node finds patterns in only one of the four regions in the image. Then, each hidden node still reports to the output layer where the output layer combines the findings for the discovered patterns learned separately in each region. This so called locally connected layer uses far fewer parameters than a densely connected layer. Its less prone to overfitting and truly understands how to tease out the patterns contained in image data. We can rearrange each of these vectors as a matrix, where now the relationships between the nodes in each layer are more obvious. We can expand the number of patterns that we're able to detect while still making use of the 2D structure to selectively and conservatively add weights to the model by introducing more hidden nodes. Where each is still confined to analyzing a single small region within the image. \n",
    "\n",
    "We can now have two collections of hidden nodes where each collection contains nodes responsible for examining a different region of the image. It will be useful for each of the nodes within a collection share a common group of weights. The idea being that different regions within the image can share the same kind of information. In other words every pattern thats relevant towards understanding the image could appear anywhere within the image. \n",
    "\n",
    "### Filters and the Convolutional Layer \n",
    "\n",
    "CNNs can remember spatial information. The neural networks that we've seen so far only lookat individual inputs. But CNNs can look at images as a whole or in patches and analyze groups of pixels at a time. The key to preserving the spatial information is something called the convolutional layer. A convolutional layer applies a series of different image filters also known as convolutional kernels to an input image. the resulting filtered images have different appearances, the filters may have extracted features like the edges of objects in that image, or the colors that distinguish the different classes of images. \n",
    "\n",
    "### Filters \n",
    "\n",
    "Spatial patterns in an image, shape or color. \n",
    "\n",
    "Shape can be thought of as patterns of intensity in an image. Intensity is a measure of light and dark, similar to brightness, and we can use this knowledge to detect the shape of object in an image. You can often identify the edges of an object by looking at abrupt changes in intensity. \n",
    "\n",
    "### Frequency in images\n",
    "\n",
    "Frequency in images is a **rate of change**., Images change in space, and a high frequency image is one where the intensity changes a lot. And the level of brightness changes quickly from one pixel to the next. A low frequency image may be one that is relatively uniform in brightness or changes very slowly. \n",
    "\n",
    "Most images have both high-frequency and low-frequency components. \n",
    "\n",
    "### High pass filters\n",
    "\n",
    "Filters are used to filter out unwanted or irrelevant information in an image or to amplify features like object boundaries or other distinguishing traits. **High pass filters** are used to make an image appear _sharper_ and enhance high-frequency parts of an image. Where parts of the image neighboring pixels rapidly change like from very dark to very light pixels. \n",
    "\n",
    "This will emphasize edges, which are areas in an image whre the intensity changes very quickly and these edges often indicate object boundaries. \n",
    "\n",
    "Filters here, are in the form of matrices, often called **convolutional kernels** which are just grids of numbers that modify an image. \n",
    "\n",
    "For example, edge dectection is a 3x3 kernel whose elements all sum to zero. \n",
    "\n",
    "$$\\begin{matrix}\n",
    "0 & -1 & 0 \\\\\n",
    "-1 & 4 & {-1} \\\\\n",
    "0 & -1 & 0\n",
    "\\end{matrix}$$\n",
    "\n",
    "Its important they sum to zero because this filter is computing the difference or change between neighboring pixels. To apply this input image $F(x,y)$ is convolved with this kernel, with $k$ - the kernel. This is called kernel convolution and convolution is represented by an asterisk $*$. $K * F(x, y) = $ output image. Kernel convolution is an important problem in computer vision applications and its the basis for convolutional neural networks. So from changing the kernel numbers we can create many different effects from edge detection to blurring an image. \n",
    "\n",
    "So what we do is multiply each value in the kernel to their corresponding pixel value in the image, and then the new values are then added up to provide us the new pixel value. The multipliers in our kernel are often called weights because they determine how important or how weighty a pixel is in forming a new output image. In this case, for edge detection, the center pixel is the most important followed by its closest pixels on the top, bottom, left and right. Which are negative weights which increase the contrast in the image. The corners are the farthest away from the center pixel and in this example, we do not give them any weight (they are zero). So the weighted sum becomes the value at the corresponding pixel at the same location xy in the output image. \n",
    "\n",
    "The kernel cannot be nicely laid over 3x3 pixel values everywhere. \n",
    "So what do we do in the corners? Below is from Udacity.\n",
    "\n",
    "**Edge Handling**\n",
    "Kernel convolution relies on centering a pixel and looking at it's surrounding neighbors. So, what do you do if there are no surrounding pixels like on an image corner or edge? Well, there are a number of ways to process the edges, which are listed below. It’s most common to use padding, cropping, or extension. In extension, the border pixels of an image are copied and extended far enough to result in a filtered image of the same size as the original image.\n",
    "\n",
    "**Extend** The nearest border pixels are conceptually extended as far as necessary to provide values for the convolution. Corner pixels are extended in 90° wedges. Other edge pixels are extended in lines.\n",
    "\n",
    "**Padding** The image is padded with a border of 0's, black pixels.\n",
    "\n",
    "**Crop** Any pixel in the output image which would require values from beyond the edge is skipped. This method can result in the output image being slightly smaller, with the edges having been cropped.\n",
    "\n",
    "#### Quiz\n",
    "\n",
    "The kernel which would be best for finding and enhancing horizontal edges and lines in an image, would be \n",
    "\n",
    "$$\\begin{matrix}\n",
    "-1 & -2 & -1 \\\\\n",
    "0 & 0 & 0 \\\\\n",
    "1 & 2 & 1\n",
    "\\end{matrix}$$\n",
    "\n",
    "Notice that the commonality on the first row and third row, while the middle row is empty. \n",
    "\n",
    "### The importance of filters\n",
    "\n",
    "CNNs are a kind of deep learning model that can learn to do things like image classification and object recognition. They keep track of spatial information and learn to extract features like the edges of objects in something called a **convolution layer**. \n",
    "\n",
    "The convolutional layer is produced by applying a series of many different image filters, also known as convolutional kernels, to an input image. \n",
    "\n",
    "![cnnlayer](..data\\convolutional_layer.png)\n",
    "\n",
    "### Learning\n",
    "We have been setting the values of filter weights explicitly, but NNs will actually learn the best filter weights as they train on a set of image data. \n",
    "\n",
    "### Convolutional Layers\n",
    "\n",
    "Each Filter we will consider as either a feature map or as an activation map. After visualizing the feature maps we can see that they look like filtered images. So a filtered image after a feature map has been convolved on the original image outputs a much simpler image with less information. Lighter values in the feature map mean that the pattern in the filter was detected in the image. Edges in images appear as a line of lighter pixels next to a line of darker pixels. \n",
    "\n",
    "Grayscale images are interpreted by a computer as a 2D array, with height and width. \n",
    "\n",
    "Color images are interpreted by the computer as a 3D array with heigh, width, and depth. \n",
    "\n",
    "With the case of RGB images, the depth is 3. And the 3D array can be best conceptualized as a stack of three two-dimensional matrices, where we have matrices corresponding to the red, green and blue channels of the image. **So how do we perform a convolution on a color image?**\n",
    "As with grayscale images we still move a filter horizontally and vertically across the image. Only now the filter is itself three dimensional to have a value for each color channel at each horizontal and vertical location in the image array. Just as we think of the color image as a stack of three two-dimensional matrices, you can also think of the filter as a stack o three two-dimensional matrices. Both the color image and the filter have red, green and blue channels. Now to obtain the values of the nodes in the feature map corresponding to this filter, we do the same thing as we did before. Only now, our sum is over three times as many terms. With ReLU activation after the summation. \n",
    "\n",
    "Now, if we wanted to picture the case of a color image with multiple filters, instead of having a single 3D array, which corresponds to one filter, we would define multiple 3D arrays each defining a filter. \n",
    "\n",
    "We can think about each of the feature maps in a convolutional layer along the same lines as an image channel and stack them to get a 3D array. Then, we can use this 3D array as input to still another convolutional layer to discover patterns within the patterns that we discovered in the first convolutional layer. We can then do this again to discover patterns within patterns within patterns. \n",
    "\n",
    "Convolutional layers are not too different then Dense Layers (we saw previously), Dense layers are fully connected meaning that the nodes are connected to every node in the previous layer. Whereas, convolutional layers are locally connected where their nodes are connected to only a small subset of the previous layers' nodes. Convolutional layers also had this added perimeter sharing. But in both cases with convolutional and dense layers, inference works the same way. Both have weights and biases that are initially randomly generated. So in the case of CNNs where the weights take the form of convolutional filters, those filters are randomly generated and so are the patterns that they're initially designed to detect. As with MLPs, when we construct to CNN we will always specify a loss function. In the case of multiclass classification, this will be categorical cross-entropy loss. Then as we train the model through back propagation, the filters are updated at each epoch to take on values that minimize the loss function. In other words, the CNN determines what kind of patterns it needs to detect based on the loss function. \n",
    "\n",
    "So for CNNs we won't specify the values of the filters or tell the CNN what kind of patterns it needs to detect. These will be learned from the data. \n",
    "\n",
    "http://setosa.io/ev/image-kernels/\n",
    "\n",
    "### Stride and Padding\n",
    "\n",
    "We have seen you can control the behavior of a Convolutional Layer by specifying the number of filters and size of each filter, for instance. To increase the number of nodes in a convolutional layer, you could increase the number of filters. To increase the size of the detected patterns, you could increase the size of your filter. \n",
    "\n",
    "One of these hyperparameters is referred to as the side of the convolution. the **stride** is just the amount by which the filter slides over the image. A stride of 1 means we move the convolution window horizontally and vertically across the image one pixel at a time. A stride of 1 makes the convolutional layer roughly the same width and height as the input image. If we make the stride 2, the convolutional layer is about half the width and height of the image. Roughly because it depends on what you do at the edge of your image. To see how the treatment of the edges will matter, consider our toy example of a 5x5 gray scale image. \n",
    "**What do we do if the filter extends outside the image?**\n",
    "Do we still want to keep the corresponding convolutional node? How do we deal with these nodes where the filter extended outside the image? One option is to get rid of them, if we choose this option than its possible our convolutional layer has no information about some regions of the image. Another option would be to plan ahead by padding the image with zeros to give the filter more space to move. Now we get contributions from every region in the image. \n",
    "\n",
    "### Pooling Layers\n",
    "\n",
    "We're now ready to introduce to the second and final type of layer that we'll need to introduce before building our convolutional neural networks. These so-called pooling layers often take convolutional layers as input. \n",
    "\n",
    "Recall a convolutional layer is a stack of feature maps where we have one feature map for each filter. A complicated dataset with many different object categories will require a large number of filters, many different object categories will require a large number of filters, each responsible for finding a pattern in the image. more filters means a bigger stack, which means that the dimensionality of our convolutional layers can get quire large. Higher dimensionality means, we'll need to use more parameters, which can lead to over-fitting. Thus, we need a method for reducing this dimensionality. This is the role of pooling layers within a convolutional neural network. \n",
    "\n",
    "We'll focus on two different types of pooling layers. The first type is a **max pooling layer**, max pooling layers will take a stack of feature maps as input. As with convolutional layers, we define a window size and stride. To construct the max pooling layer we'll work with each feature map separately. Let's begin with the first feature map, we start with our window in the top left corner of the image. The value of the corresponding node in the max pooling layer is calculating by just taking the _maximum of the pixels contained in the window_. If we continue this process and do it for all of our feature maps, the output is a stack with the same number of feature maps, but each feature map has been reduced in width and height. In this case since the stride is 2, the widht and height are half of that of the previous convolutional layer. \n",
    "\n",
    "The other type of pooling is to use **average pooling**, which chooses to average pixel values in a given window size so in a 2x2 window, this operation will see 4 pixel values and return a single average of those 4. This is typically not used for image classification problems because maxpooling is better at noticing the most important details about edges and other features in an image, but you may see this used in application for which _smoothing_ an image is preferable. \n",
    "\n",
    "### Capsule Networks\n",
    "\n",
    "Its important to note that pooling operations do throw away some image information. That is, they discard pixel information in order to get a smaller, feature-level representation of an image. This works quite well in tasks like image classification, but it can cause some issues. \n",
    "\n",
    "In the case of facial recognition, when you think of how you identify a face, you might think about noticing features, two eyes, a nose and a mouth, for example. And those pieces, together, form a complete face. A typical CNN that is trained to do facial recognition, should also learn to identify these features. Only by distilling an image into a feature-level representation, you might get a weird result. So there has been research into classification methods that do not discard spatial information (as in the pooling layers) and instead learn the spatial relationships between parts (like between eyes, nose and mouth). \n",
    "One method for learning spatial relationships between parts, is the **capsule network**. \n",
    "\n",
    "Capsule networks provide a way to detect parts of objects in an image and represent spatial relationships between those parts. This means a capsule networks are able to recognize the same object, like a face, in a variety of different poses and with the typical number of features (eyes, nose, mouth) even if they have not seen that pose in training data. Capsule networks are made of parent and child nodes that build up a complete picture of an object. \n",
    "\n",
    "**What are capsules?**\n",
    "Capsules are essentially a collection of nodes, each of which contains information about a specific part; part properties like width, orientation, color and so on. The important thing to note is that each capsule **outputs a vector** with some magnitude and orientation. \n",
    "\n",
    "- Magnitude (m) = the probability that a part exists, a value between 0 and 1. \n",
    "\n",
    "- Orientation (theta) = the state of the part properties. \n",
    "\n",
    "These output vectors allow us to do some powerful routing math to build up a parse tree that recognizes whole objects as comprised of several, smaller parts!\n",
    "\n",
    "The magnitude is a special part property that should stay very high even when an object is in a different orientation, as shown below. \n",
    "\n",
    "https://s3.amazonaws.com/video.udacity-data.com/topher/2018/November/5bfdca4f_dynamic-routing/dynamic-routing.pdf\n",
    "\n",
    "### Increasing Depth\n",
    "\n",
    "Convolutional layers detect regional patterns in an image using a series of image filters. We've seen how typically a ReLU activation function is applied to the output of these filters to standardize their output values. Then, we learned about max pooling layers, which appear after convolutional layers to reduce the dimensionality of our input arrays. \n",
    "\n",
    "**Data Processing**\n",
    "Similar to MLPs, the CNNs we discuss will require a fixed size input. \n",
    "so we have to pick an image size and resize all of images to that same size before doing anything else. \n",
    "\n",
    "This is another data preprocessing step alongside normalization and conversion to a tensor datatype. \n",
    "\n",
    "Its also very popular to resize each image to a square with the spatial dimensions equal to a power of two, or else a number that's divisible by a large power of two. \n",
    "\n",
    "The input array will always be much taller and wider than it is deep. Such that RGB images have a depth of 3, while we can think of grayscale images also being three dimensional with a depth of 1. \n",
    "\n",
    "Our CNN architecture will be designed with the goal of taking that array and gradually making it much deeper than it is tall or wide. Convolutional layers will be used to make the array deeper as it passes through the network, and max pooling layers will be used to decrease the XY dimensions. As the network gets deeper its actually extracting more and more complex patterns and features that help identify the content and the objects in an image, and it's actually discarding some spatial information about features like a smooth background and so on that do not help identify the image. \n",
    "\n",
    "## Why do we need padding?\n",
    "\n",
    "When we create a convolutional layer, we move a square filter around an image, using a center-pixel as an anchor. So, this kernel cannot perfectly overlay the edges/corners of images. The nice feature of padding is that it will allow us to control the spatial size of the output volumes (most commonly as we will soon see we will use it to exactly preserve the spatial size of the input volume so the input and output width and height are the same). \n",
    "\n",
    "The most common methods of padding are padding an image with all 0-pixels (**zero padding**) or padding with the nearest pixel value. \n",
    "http://cs231n.github.io/convolutional-networks/#conv\n",
    "\n",
    "## Quix: \n",
    "\n",
    "- **How might you define a Maxpooling layer, such that it down-samples an input by a factor of 4? **\n",
    "\n",
    "```python\n",
    ">>> nn.MaxPool2d(2, 4)\n",
    ">>> nn.MaxPool2d(4, 4)\n",
    "```\n",
    "\n",
    "The _best_ choice would be to use a kernel and stride of 4, so that the maxpooling function sees every input pixel once, but **any layer** with a stride of 4 will down-sample an input by that factor. \n",
    "\n",
    "- **If you want to define a convolutional layer that is the same x-y size as an input array, what padding should you have for a** `kernel_size` **of 7? All other parameters are left as their default values.**\n",
    "\n",
    "```python\n",
    ">>> padding=3\n",
    "\n",
    "```\n",
    "\n",
    "If you overlay a 7x7 kernel so that its center-pixel is at the right edge of an image, you will have 3 kernel columns that do not overlay anything! So, that's how big your padding needs to be. \n",
    "\n",
    "## PyTorch Layer Doc\n",
    "\n",
    "### Convolutional Layers\n",
    "\n",
    "We typically define a convolutional layer in PyTorch using `nn.Conv2d` with the following parameters specified. \n",
    "\n",
    "```python\n",
    ">>> nn.Conv2d(in_channels, out_channels, kernel_size, stride=1, padding=0)\n",
    "```\n",
    "\n",
    "- `in_channels` refers to the depth of an input, for grayscale image this depth = 1\n",
    "- `out_channels` refers to the desired depth of the output, or the number of filtered images you want to get as output\n",
    "- `kernel_size` is the size of your convolutional kernel (most commonly 3 for a 3x3 kernel)\n",
    "- `stride` and `padding` have default values, but should be set depending on how large you want your output to be in the spatial dimensions x, y\n",
    "\n",
    "### Pooling Layers\n",
    "Maxpooling layers commonly come after convolutional layers to shrink the x-y dimensions of an input.\n",
    "\n",
    "https://classroom.udacity.com/nanodegrees/nd101/parts/2e8d3b5d-aa70-4376-946f-0cdc37127d7d/modules/19a75d10-547d-4497-ae68-609ca1a235fc/lessons/807590ea-abd5-4581-b91d-9eede9a0aad2/concepts/62a4771f-ea82-44a4-afac-dd6bacda27bc\n",
    "\n",
    "### Shape of a Convolutional Layer\n",
    "\n",
    "The shape of a conv layer depends on the supplied values of the kernel_size, input_shape, padding and stride.\n",
    "- K = the number of filters in the conv layer\n",
    "- F = the height and width of the conv filters\n",
    "- S = the stride of the convolution\n",
    "- P = the padding\n",
    "- W_in = the width/height (square) of the previous layer \n",
    "\n",
    "To calculate the output shape of a convolutional layer. \n",
    "\n",
    "The **depth** of the convolutional layer will always equal the number of filters K.\n",
    "\n",
    "$$ shape = [ (Win - F + 2P)/S] + 1 $$\n",
    "\n",
    "\n",
    "## Quiz \n",
    "\n",
    "For the following quiz, consider an input image that is `130x130 (x,y) and 3` in depth RGB. Say this image goes through the following layers in order: \n",
    "\n",
    "```\n",
    "nn.Conv2d(3, 10, 3)\n",
    "nn.MaxPool(4, 4)\n",
    "nn.Conv2d(10, 20, 5, padding=2)\n",
    "nn.MaxPool2d(2, 2)\n",
    "```\n",
    "\n",
    "### Question 1:\n",
    "\n",
    "- After going through all four of these layers in sequence, what is the depth of the final output?\n",
    "\n",
    "```python\n",
    ">>> 20\n",
    "```\n",
    "- What is the x-y size of the output of the final maxpooling layer? Careful to look at how the 130x130 image passes through (and shrinks) as it moved through each convolutional and pooling layer.\n",
    "\n",
    "```python\n",
    ">>> 16\n",
    "```\n",
    "\n",
    "- How many parameters, total, will be left after an image passes through all four of the above layers in sequence?\n",
    "\n",
    "```python\n",
    ">>> 16*16*20\n",
    "```\n",
    "\n",
    "## Image Augmentation\n",
    "\n",
    "When we design an algorithm to classify objects in images, we have to deal with a lot of irrelevant information. We really only want our algorithm to determine if an object is present in the image or not. The size, the angle, or if we move the object to the left or right does not matter (remember its a computer thats going to interpret this and this doesn't matter to a computer). We can say that we want our algorithm to lean an invariant representation of an image. We don't want our model to change its prediction based on the size of the object. This is called **scale invariance** Likewise, we don't want the angle of the object to matter this is called **rotation invariance**, if we shift the image to the left or right, the object is still in the image, this is called **translation invariance**.  \n",
    "CNN's do have some built-in **translation invariance**, to see this we need to recall how we calculate max-pooling layers. Remember at each window location, we took the maximum of the pixels contained in the window. This maximum value can occur anywhere within the window. The value of the max-pooling node would be the same if we translated the image a little to the left, to the right, up, down as log as the maximum value stays within the window. The effect of applying many max-pooling layers in a sequence each one following a convolutional layer, is that we could translate the object quite far to the left, to the top of the image, to the bottom of the image, and still our network will be able to make sense of it all. This is a **non-trivial** problem. (The computer only sees a matrix of pixels) Transforming an object's scale, rotation, or position in the image has a huge effect on the pixel values. Theres a convenient way to making our algorithms more statistically invariant, but it will feel a little bit like cheating. The idea is this, if you want your CNN to be rotation invariant, well, then you can just add some images to your training set created by doing random rotations on your training images. if you want more translation invariance you can also just add new images created by doing random translations of your training images. When we do this we say that we have expanding the training set by augmenting the data. Data augmentation will also help us to avoid overfitting. This is because the model is seeing many new images, Thus it should be better at generalizing and we should get better performance on the test dataset. \n",
    "\n",
    "```python\n",
    "transforms = transforms.Compose([\n",
    "    transforms.RandomHorizontalFlip(), #random flip and rotate\n",
    "    transforms.RandomRotation(10),\n",
    "    transforms.ToTensor(),\n",
    "    trnasforms.Normalize((0.5, 0.5, 0.5), (0.5, 0.5, 0.5))\n",
    "])\n",
    "\n",
    "```\n",
    "\n",
    "That's all you need to do to give your images some geometric variation. \n",
    "\n",
    "## Visualizing CNNs\n",
    "\n",
    "We have seen that CNNs have sometimes achieve superhuman performance in object classification tasks. We don't really have an understanding of how these CNNs discover patterns in raw image pixels. If you've already trained your own CNNs youve noticed that some architectures work while some just don't. \n",
    "\n",
    "As one technique for digging deeper into understanding how a CNN is working. There are many implementations of this online. You can start with an image containing random noise and then gradually amend the pixels, and each step changing them to values that make the filter more highly activated. When you do this, you will notice that the first three layers are pretty general \n",
    "\n",
    "https://blog.keras.io/how-convolutional-neural-networks-see-the-world.html\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Common CNN Libraries in Python\n",
    "\n",
    "## OpenCV\n",
    "\n",
    "Is a computer vision and machine learning software library that includes many common image analysis algorithms that will help us build custom, intelligent computer vision applications. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/anaconda3/lib/python3.6/site-packages/numpy/core/numeric.py:492: ComplexWarning: Casting complex values to real discards the imaginary part\n",
      "  return array(a, dtype, copy=False, order=order)\n"
     ]
    },
    {
     "data": {
      "image/png": "iVBORw0KGgoAAAANSUhEUgAAAZQAAAEKCAYAAAA1qaOTAAAABHNCSVQICAgIfAhkiAAAAAlwSFlz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\n+P8BZf9UXMnAI2kAAAAASUVORK5CYII=\n",
      "text/plain": [
       "<matplotlib.figure.Figure at 0x10e0cfc88>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "import numpy as np\n",
    "from matplotlib import pyplot as plt\n",
    "#setting\n",
    "k = 2\n",
    "res = 400\n",
    "t = np.linspace(0, res, res)\n",
    "time = np.linspace(0, 1, res)\n",
    "hz = np.exp(-2j * np.pi * k * t / len(t))\n",
    "plt.ylabel(\"Amplitude\")\n",
    "plt.xlabel(\"Time\")\n",
    "plt.plot(t, hz)\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
