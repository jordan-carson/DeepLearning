{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Convolutional Neural Networks\n",
    "\n",
    "Any gray scale image is interpreted by a computer as an array. A grid of values where each grid cell is a pixel. for example mnist provides a image of 28x28. \n",
    "\n",
    "### Data Normalization\n",
    "\n",
    "Data normalization is an important pre-processing step. It ensures that each input (each pixel value, in this case) comes from a standard distribution. That is, the range of pixel values in one input image are the same as the range in another image. This standardization makes our model train and reach a minimum error, faster!\n",
    "\n",
    "Data normalization is typically done by subtracting the mean (the average of all pixel values) from each pixel, and then dividing the result by the standard deviation of all the pixel values. Sometimes you'll see an approximation here, where we use a mean and standard deviation of 0.5 to center the pixel values.\n",
    "\n",
    "The distribution of such data should resemble a Gaussian function centered at zero. For image inputs we need the pixel numbers to be positive, so we often choose to scale the data in a normalized range [0,1].\n",
    "\n",
    "We want to normalize each pixel value, for example in grey scale images our pixels are 0 - 255, so we want to divide them by 255. and this is important because our network relies on gradient calculations. Normalizing the pixel values helps these gradient calculations stay consistent and not get so large that they prevent a network from training. \n",
    "\n",
    "### MLPs\n",
    "\n",
    "They require flattened images. So after converting our matrix into a vector, they can be fed into the input layer of an MLP\n",
    "\n",
    "**Class Scores** Indicate how sure the network is that a given input is of a specific class. They are often represented as a vector of values or even as a bar graph indicating the relative strengths of the scores. \n",
    "\n",
    "### Loss & Optimization\n",
    "We measure any mistakes that it makes using a loss function, whose job is to measure the difference between the predicted and true class labels. Then using backpropagation we can compute the gradient of the loss with respect to the models' weights. This way we quantify how bad a particular weight is, and find out which weights in the network are responsible for any errors. Finally, using that calculation, we can choose an optimization function like *gradient descent* to give us a way to calculate a better weight value. \n",
    "\n",
    "Softmax function is a good function to use at the end to turn your logits into a probability of class scores. \n",
    "\n",
    "So our goal is to update the weights of the network in response to the mistakes it makes so that next time it sees the input image, it will predict the most likely label. We need to define some measure of exactly how far off the model currently is from perfection (ground truth). \n",
    "\n",
    "Calculate the categorical cross-entropy loss\n",
    "\n",
    "loss = lower when the prediction and label agree, and higher when the prediction and label disagree.\n",
    "\n",
    "So a loss function and backpropagation give us a way to quantify how bad a particular network weight is, based on how close a predicted and the true class label are from one another. Next, we need a way to calculate a better weight value - so we can think of the error as a mountain, where we want to descend to the lowest value. This is the role of an optimizer, the standard mthod for minimizing the loss and optimizing for the best weight values is called 'gradient descent'. We have already seen a number of ways to perform gradient descent and each method has a corresponding optimizer. \n",
    "\n",
    "### Model Validation\n",
    "\n",
    "So far we have seen how our model performs based on how the loss changes over each epoch. But the exact number of how many epochs to train for is sometimes not a given. How many epochs should we train for so that our model is accurate but its not overfitting the training data?\n",
    "\n",
    "One method thats used commonly involves breaking the dataset into three sets called training validation and test sets. And each is treated separately by the model. The model looks only at the training set when it's actively training and deciding how to modify its weights. After every training epoch, we check how the model is doing by looking at the training loss and the loss on the validation set. But its important to know that the model does not use any part of the validation set for the back propagation step. We use the training set to find all the patterns we can, and to update and determine the weights of our model. \n",
    "\n",
    "The validation set only tells us if that model is doing well on the validation set. And in this way it gives us an idea of how well the model generalizes to a set of that that is separate from the training set. The idea is, since the model doesn't use the validation set for deciding its weights, that it can tell us if we're overfitting the training set of data. Finally the test set of data is saved for checking the accuracy of the trained model. \n",
    "\n",
    "For example let's say we see after epoch 200 that our validation loss is actually beginning to increase, because its not generalizing well enough to also perform well on the validation set. So if we see this divide and how the training and validation losses decrease, you'll want to stop changing the weights of the network around the epoch we see the increase beginning / or ignore or throw away the weights from later epochs where there's evidence of overfitting. This can also be useful if we have multiple architectures to choose from. Even though the model doesn't use the validation set to update its weights, our model selection process is based on how the model performs on both the training and validation sets. So, in the end, the model is biased in favor of the validation set. Thus, we need a separate test set of data to truly see how our selected model generalizes and performs when given data it really has not seen before. \n",
    "\n",
    "### Validation Loss\n",
    "\n",
    "We Create a validation set to:\n",
    "1. Measure how well a model generalizes during training\n",
    "2. Tell us when to stop training a model; when the validation loss stops decreasing (and especially when the validation loss starts increasing and the training loss is still decreasing)\n",
    "\n",
    "### Image classification steps\n",
    "\n",
    "1. **Visualize Data**: Load and visualize the data we are working with\n",
    "2. **Pre-Process**: the data by normalizing it and converting it into a tensor, so that its prepped for futher processing by the layers of a neural network.\n",
    "3. **Define a model**: Do your research, has anyone approached this? Use this to decide on a model architecture and define it. \n",
    "4. **Train your Model**: Decide on loss and optimization functions and proceed with training your model. \n",
    "5. **Save the Best Model**: Choose to use a validation dataset to select and save the best model during training. \n",
    "6. **Test Your Model**: on previously unseen data. \n",
    "\n",
    "## MLPs vs CNNs\n",
    "\n",
    "So far, we've investigated how to train an MLP to classify handwritten digits in the MNIST dataset, the best algorithms are the ones with the least test error are the approaches that use Convolutional Neural Networks or CNNs. For most image classification tasks, they do not even compare. \n",
    "\n",
    "For MLPs we need to input out data as a simple vector of numbers with no special structure. It has no knowledge of the fact that these numbers were originally spatially arranged in a grid. \n",
    "\n",
    "CNNs, in contrast, are built for the exact purpose of working with or elucidating the patterns in multidimensional data. Unlike MLPs, CNNs understand the fact that image pixels that are closer in proximity to each other are model heavily related than pixels that are far apart. \n",
    "\n",
    "### Local Connectivity\n",
    "\n",
    "MPLs (Issues)\n",
    "- First, use a lot of parameters. **Only uses fully connected layers**\n",
    "- We get rid of all the 2D Information contained in an image when we flatten its matrix to a vector. The spatial information or knowledge of where the pixels are located in reference to each other is relevant to understanding the image and could aid significantly towards elucidating the patterns contained in the pixel values. \n",
    "\n",
    "This suggests that we need an entirely new way of processing image input, where the 2D information is not entirely lost. \n",
    "\n",
    "CNNs\n",
    "- Also use sparsely connected layers - it will address the problems by user layers that are more sparsely connected, where the connections between layers are informed by the 2D structure of the image matrix. \n",
    "- Accept our matrix as input \n",
    "\n",
    "In a MLP does every hidden node need to be connected to every pixel in the original image? Perhaps not.\n",
    "\n",
    "Instead we use a regional breakdown and the assignment of small local groups of pixels to different hidden nodes, every hidden node finds patterns in only one of the four regions in the image. Then, each hidden node still reports to the output layer where the output layer combines the findings for the discovered patterns learned separately in each region. This so called locally connected layer uses far fewer parameters than a densely connected layer. Its less prone to overfitting and truly understands how to tease out the patterns contained in image data. We can rearrange each of these vectors as a matrix, where now the relationships between the nodes in each layer are more obvious. We can expand the number of patterns that we're able to detect while still making use of the 2D structure to selectively and conservatively add weights to the model by introducing more hidden nodes. Where each is still confined to analyzing a single small region within the image. \n",
    "\n",
    "We can now have two collections of hidden nodes where each collection contains nodes responsible for examining a different region of the image. It will be useful for each of the nodes within a collection share a common group of weights. The idea being that different regions within the image can share the same kind of information. In other words every pattern thats relevant towards understanding the image could appear anywhere within the image. \n",
    "\n",
    "### Filters and the Convolutional Layer \n",
    "\n",
    "CNNs can remember spatial information. The neural networks that we've seen so far only lookat individual inputs. But CNNs can look at images as a whole or in patches and analyze groups of pixels at a time. The key to preserving the spatial information is something called the convolutional layer. A convolutional layer applies a series of different image filters also known as convolutional kernels to an input image. the resulting filtered images have different appearances, the filters may have extracted features like the edges of objects in that image, or the colors that distinguish the different classes of images. \n",
    "\n",
    "### Filters \n",
    "\n",
    "Spatial patterns in an image, shape or color. \n",
    "\n",
    "Shape can be thought of as patterns of intensity in an image. Intensity is a measure of light and dark, similar to brightness, and we can use this knowledge to detect the shape of object in an image. You can often identify the edges of an object by looking at abrupt changes in intensity. \n",
    "\n",
    "### Frequency in images\n",
    "\n",
    "Frequency in images is a **rate of change**., Images change in space, and a high frequency image is one where the intensity changes a lot. And the level of brightness changes quickly from one pixel to the next. A low frequency image may be one that is relatively uniform in brightness or changes very slowly. \n",
    "\n",
    "Most images have both high-frequency and low-frequency components. \n",
    "\n",
    "### High pass filters\n",
    "\n",
    "Filters are used to filter out unwanted or irrelevant information in an image or to amplify features like object boundaries or other distinguishing traits. **High pass filters** are used to make an image appear _sharper_ and enhance high-frequency parts of an image. Where parts of the image neighboring pixels rapidly change like from very dark to very light pixels. \n",
    "\n",
    "This will emphasize edges, which are areas in an image whre the intensity changes very quickly and these edges often indicate object boundaries. \n",
    "\n",
    "Filters here, are in the form of matrices, often called **convolutional kernels** which are just grids of numbers that modify an image. \n",
    "\n",
    "For example, edge dectection is a 3x3 kernel whose elements all sum to zero. \n",
    "\n",
    "$$\\begin{matrix}\n",
    "0 & -1 & 0 \\\\\n",
    "-1 & 4 & {-1} \\\\\n",
    "0 & -1 & 0\n",
    "\\end{matrix}$$\n",
    "\n",
    "Its important they sum to zero because this filter is computing the difference or change between neighboring pixels. To apply this input image $F(x,y)$ is convolved with this kernel, with $k$ - the kernel. This is called kernel convolution and convolution is represented by an asterisk $*$. $K * F(x, y) = $ output image. Kernel convolution is an important problem in computer vision applications and its the basis for convolutional neural networks. So from changing the kernel numbers we can create many different effects from edge detection to blurring an image. \n",
    "\n",
    "So what we do is multiply each value in the kernel to their corresponding pixel value in the image, and then the new values are then added up to provide us the new pixel value. The multipliers in our kernel are often called weights because they determine how important or how weighty a pixel is in forming a new output image. In this case, for edge detection, the center pixel is the most important followed by its closest pixels on the top, bottom, left and right. Which are negative weights which increase the contrast in the image. The corners are the farthest away from the center pixel and in this example, we do not give them any weight (they are zero). So the weighted sum becomes the value at the corresponding pixel at the same location xy in the output image. \n",
    "\n",
    "The kernel cannot be nicely laid over 3x3 pixel values everywhere. \n",
    "So what do we do in the corners? Below is from Udacity.\n",
    "\n",
    "**Edge Handling**\n",
    "Kernel convolution relies on centering a pixel and looking at it's surrounding neighbors. So, what do you do if there are no surrounding pixels like on an image corner or edge? Well, there are a number of ways to process the edges, which are listed below. It’s most common to use padding, cropping, or extension. In extension, the border pixels of an image are copied and extended far enough to result in a filtered image of the same size as the original image.\n",
    "\n",
    "**Extend** The nearest border pixels are conceptually extended as far as necessary to provide values for the convolution. Corner pixels are extended in 90° wedges. Other edge pixels are extended in lines.\n",
    "\n",
    "**Padding** The image is padded with a border of 0's, black pixels.\n",
    "\n",
    "**Crop** Any pixel in the output image which would require values from beyond the edge is skipped. This method can result in the output image being slightly smaller, with the edges having been cropped.\n",
    "\n",
    "#### Quiz\n",
    "\n",
    "The kernel which would be best for finding and enhancing horizontal edges and lines in an image, would be \n",
    "\n",
    "$$\\begin{matrix}\n",
    "-1 & -2 & -1 \\\\\n",
    "0 & 0 & 0 \\\\\n",
    "1 & 2 & 1\n",
    "\\end{matrix}$$\n",
    "\n",
    "Notice that the commonality on the first row and third row, while the middle row is empty. \n",
    "\n",
    "### The importance of filters\n",
    "\n",
    "CNNs are a kind of deep learning model that can learn to do things like image classification and object recognition. They keep track of spatial information and learn to extract features like the edges of objects in something called a **convolution layer**. \n",
    "\n",
    "The convolutional layer is produced by applying a series of many different image filters, also known as convolutional kernels, to an input image. \n",
    "\n",
    "![cnnlayer](data\\convolutional_layer.png)\n",
    "\n",
    "### Learning\n",
    "We have been setting the values of filter weights explicitly, but NNs will actually learn the best filter weights as they train on a set of image data. \n",
    "\n",
    "### Convolutional Layers\n",
    "\n",
    "Each Filter we will consider as either a feature map or as an activation map. After visualizing the feature maps we can see that they look like filtered images. So a filtered image after a feature map has been convolved on the original image outputs a much simpler image with less information. Lighter values in the feature map mean that the pattern in the filter was detected in the image. Edges in images appear as a line of lighter pixels next to a line of darker pixels. \n",
    "\n",
    "Grayscale images are interpreted by a computer as a 2D array, with height and width. \n",
    "\n",
    "Color images are interpreted by the computer as a 3D array with heigh, width, and depth. \n",
    "\n",
    "With the case of RGB images, the depth is 3. And the 3D array can be best conceptualized as a stack of three two-dimensional matrices, where we have matrices corresponding to the red, green and blue channels of the image. **So how do we perform a convolution on a color image?**\n",
    "As with grayscale images we still move a filter horizontally and vertically across the image. Only now the filter is itself three dimensional to have a value for each color channel at each horizontal and vertical location in the image array. Just as we think of the color image as a stack of three two-dimensional matrices, you can also think of the filter as a stack o three two-dimensional matrices. Both the color image and the filter have red, green and blue channels. Now to obtain the values of the nodes in the feature map corresponding to this filter, we do the same thing as we did before. Only now, our sum is over three times as many terms. With ReLU activation after the summation. \n",
    "\n",
    "Now, if we wanted to picture the case of a color image with multiple filters, instead of having a single 3D array, which corresponds to one filter, we would define multiple 3D arrays each defining a filter. \n",
    "\n",
    "We can think about each of the feature maps in a convolutional layer along the same lines as an image channel and stack them to get a 3D array. Then, we can use this 3D array as input to still another convolutional layer to discover patterns within the patterns that we discovered in the first convolutional layer. We can then do this again to discover patterns within patterns within patterns. \n",
    "\n",
    "Convolutional layers are not too different then Dense Layers (we saw previously), Dense layers are fully connected meaning that the nodes are connected to every node in the previous layer. Whereas, convolutional layers are locally connected where their nodes are connected to only a small subset of the previous layers' nodes. Convolutional layers also had this added perimeter sharing. But in both cases with convolutional and dense layers, inference works the same way. Both have weights and biases that are initially randomly generated. So in the case of CNNs where the weights take the form of convolutional filters, those filters are randomly generated and so are the patterns that they're initially designed to detect. As with MLPs, when we construct to CNN we will always specify a loss function. In the case of multiclass classification, this will be categorical cross-entropy loss. Then as we train the model through back propagation, the filters are updated at each epoch to take on values that minimize the loss function. In other words, the CNN determines what kind of patterns it needs to detect based on the loss function. \n",
    "\n",
    "So for CNNs we won't specify the values of the filters or tell the CNN what kind of patterns it needs to detect. These will be learned from the data. \n",
    "\n",
    "### Stride and Padding\n",
    "\n",
    "We have seen you can control the behavior of a Convolutional Layer by specifying the number of filters and size of each filter, for instance. To increase the number of nodes in a convolutional layer, you could increase the number of filters. To increase the size of the detected patterns, you could increase the size of your filter. \n",
    "\n",
    "One of these hyperparameters is referred to as the side of the convolution. the **stride** is just the amount by which the filter slides over the image. A stride of 1 means we move the convolution window horizontally and vertically across the image one pixel at a time. A stride of 1 makes the convolutional layer roughly the same width and height as the input image. If we make the stride 2, the convolutional layer is about half the width and height of the image. Roughly because it depends on what you do at the edge of your image. To see how the treatment of the edges will matter, consider our toy example of a 5x5 gray scale image. \n",
    "**What do we do if the filter extends outside the image?**\n",
    "Do we still want to keep the corresponding convolutional node? How do we deal with these nodes where the filter extended outside the image? One option is to get rid of them, if we choose this option than its possible our convolutional layer has no information about some regions of the image. Another option would be to plan ahead by padding the image with zeros to give the filter more space to move. Now we get contributions from every region in the image. \n",
    "\n",
    "### Pooling Layers\n",
    "\n",
    "We're now ready to introduce to the second and final type of layer that we'll need to introduce before building our convolutional neural networks. These so-called pooling layers often take convolutional layers as input. \n",
    "\n",
    "Recall a convolutional layer is a stack of feature maps where we have one feature map for each filter. A complicated dataset with many different object categories will require a large number of filters, many different object categories will require a large number of filters, each responsible for finding a pattern in the image. more filters means a bigger stack, which means that the dimensionality of our convolutional layers can get quire large. Higher dimensionality means, we'll need to use more parameters, which can lead to over-fitting. Thus, we need a method for reducing this dimensionality. This is the role of pooling layers within a convolutional neural network. \n",
    "\n",
    "We'll focus on two different types of pooling layers. The first type is a **max pooling layer**, max pooling layers will take a stack of feature maps as input. As with convolutional layers, we define a window size and stride. To construct the max pooling layer we'll work with each feature map separately. Let's begin with the first feature map, we start with our window in the top left corner of the image. The value of the corresponding node in the max pooling layer is calculating by just taking the _maximum of the pixels contained in the window_. If we continue this process and do it for all of our feature maps, the output is a stack with the same number of feature maps, but each feature map has been reduced in width and height. In this case since the stride is 2, the widht and height are half of that of the previous convolutional layer. \n",
    "\n",
    "The other type of pooling is to use **average pooling**, which chooses to average pixel values in a given window size so in a 2x2 window, this operation will see 4 pixel values and return a single average of those 4. This is typically not used for image classification problems because maxpooling is better at noticing the most important details about edges and other features in an image, but you may see this used in application for which _smoothing_ an image is preferable. \n",
    "\n",
    "### Capsule Networks\n",
    "\n",
    "Its important to note that pooling operations do throw away some image information. That is, they discard pixel information in order to get a smaller, feature-level representation of an image. This works quite well in tasks like image classification, but it can cause some issues. \n",
    "\n",
    "In the case of facial recognition, when you think of how you identify a face, you might think about noticing features, two eyes, a nose and a mouth, for example. And those pieces, together, form a complete face. A typical CNN that is trained to do facial recognition, should also learn to identify these features. Only by distilling an image into a feature-level representation, you might get a weird result. So there has been research into classification methods that do not discard spatial information (as in the pooling layers) and instead learn the spatial relationships between parts (like between eyes, nose and mouth). \n",
    "One method for learning spatial relationships between parts, is the **capsule network**. \n",
    "\n",
    "Capsule networks provide a way to detect parts of objects in an image and represent spatial relationships between those parts. This means a capsule networks are able to recognize the same object, like a face, in a variety of different poses and with the typical number of features (eyes, nose, mouth) even if they have not seen that pose in training data. Capsule networks are made of parent and child nodes that build up a complete picture of an object. \n",
    "\n",
    "**What are capsules?**\n",
    "Capsules are essentially a collection of nodes, each of which contains information about a specific part; part properties like width, orientation, color and so on. The important thing to note is that each capsule **outputs a vector** with some magnitude and orientation. \n",
    "\n",
    "- Magnitude (m) = the probability that a part exists, a value between 0 and 1. \n",
    "\n",
    "- Orientation (theta) = the state of the part properties. \n",
    "\n",
    "These output vectors allow us to do some powerful routing math to build up a parse tree that recognizes whole objects as comprised of several, smaller parts!\n",
    "\n",
    "The magnitude is a special part property that should stay very high even when an object is in a different orientation, as shown below. \n",
    "\n",
    "https://s3.amazonaws.com/video.udacity-data.com/topher/2018/November/5bfdca4f_dynamic-routing/dynamic-routing.pdf\n",
    "\n",
    "### Increasing Depth"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Common CNN Libraries in Python\n",
    "\n",
    "## OpenCV\n",
    "\n",
    "Is a computer vision and machine learning software library that includes many common image analysis algorithms that will help us build custom, intelligent computer vision applications. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
