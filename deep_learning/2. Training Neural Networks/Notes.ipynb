{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Training a Neural Network"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Overfitting and Underfitting\n",
    "\n",
    "Introducing a testing set. \n",
    "\n",
    "We will error on the side of an overly complicated models and then we'll apply certain techniques to prevent overfitting on it. \n",
    "\n",
    "On our first epoch, we will have a model with certain weights that probably made many errors. But as we get to let's say for 20 epochs we get a pretty good model. \n",
    "\n",
    "The plot is called the model complexity graph which plots the training error next to the testing error. Error, on the y-axis, while epoch on the x-axis (complexity). \n",
    "\n",
    "On the left, we have a high underfiting with a high training error and a high testing error, while on the  right we have a high testing error with a low training error. We need to pick something in the middle just right **goldilocks point**. \n",
    "\n",
    "So in short, we degrade in descent until the testing error stops decreasing and starts to increase. At that moment we stop. This algorithm is called **Early Stopping** and is widely used to train neural networks. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "You need to test both models, not just look at the error, because the model may be overfitting which could be giving you a small error even though the second model is better with a higher error."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Example:\n",
    "\n",
    "### Which gives a smaller error?\n",
    "\n",
    "$ Prediction: \\hat{y} = \\alpha\\left(w_1x_1 + w_2x_2 + b\\right)$\n",
    "\n",
    "Option 1: $x_1 + x_2$\n",
    "\n",
    "Option 2: $10x_1 + 10x_2$\n",
    "\n",
    "![error question](../data/error_question.png)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Recall that the prediction is a sigmoid of the linear function. So for the first case for the (1, 1) it would be sigmoid of\n",
    "\n",
    "*Solution 1*\n",
    "\n",
    "$\\sigma(1+1)$ = 0.88\n",
    "\n",
    "$\\sigma(-1-1)$ = 0.12\n",
    "\n",
    "*Solution 2*\n",
    "\n",
    "$\\sigma(1+1)$ = $0.99999997$\n",
    "\n",
    "$\\sigma(-1-1)$ = $2.1e^{-9}$\n",
    "\n",
    "\n",
    "For solution 2, we multiply the inputs with the constant, so the first is sigmoid of 20, which is 0.9999979 really close to 1, and the second is sigmoid of (-20) which is very small (0.00000000021). Both really close to 1 or 0, so they are great predictions. The second model is super accurate but does not mean its necessarily better.\n",
    "\n",
    "The problem is overfitting but in a subtle way. \n",
    "\n",
    "![error function2](..\\data\\error_question2.png)\n",
    "\n",
    "When we apply sigmoid to small values such as $x_1 + x_2$ we get the function on the left which has a nice slope to the gradient descent. When we multiply the linear funciton by 10 and take sigmoid of $10x_1 + 10x_2$, our predictions are much better since they're closer to zero and one. But the function becomes much steeper and it's much harder to do great descent here. Since the derivatives are mostly close to zero and then very large when we get to the middle of the curve. Therefore, in order to do gradient descent properly, we want a model like the one in the left more than a model like the one in the right. In a conceptual way, the model in the right is too certain and it gives little room for applying gradient descent. Also as we can imagine, the points that are classified incorrectly in the model in the right, will generate large errors and it will be hard to tune the model to correct them. \n",
    "\n",
    "Large Coefficients lead to Overfitting.\n",
    "\n",
    "**How do we prevent this type of overfitting from happening?**\n",
    "\n",
    "This seems to not be easy since the bad model gives smaller errors. Well, all we have to do is we have to tweak the error function a bit. Basically we want to punish high coefficients. Solution is **Regularization**. \n",
    "\n",
    "We Penalize large weights $\\left(w_1, \\cdots , w_n\\right)$ \n",
    "\n",
    "So what we do is take the old error function and add a term which is big when the weights are big. There are two ways to do this.\n",
    "\n",
    "\n",
    "Recall the error function is\n",
    "$$ Error-Function =  - \\frac{1}{m} \\sum_{i=1}^m (1-y_{i})\\ln(1 - \\hat{y_i}) + y_{i}\\ln \\hat{(y_i)} $$\n",
    "\n",
    "One way is to add the sums of absolute values of the weights times a constant lambda. The other one is to add the sum of the square of the weights times that same constant. \n",
    "\n",
    "**L1**\n",
    "$$ Error-Function =  - \\frac{1}{m} \\sum_{i=1}^m (1-y_{i})\\ln(1 - \\hat{y_i}) + y_{i}\\ln \\hat{(y_i)} + \\lambda\\left(\\lvert w_1 \\rvert + \\cdots + \\lvert w_n \\rvert \\right) $$\n",
    "\n",
    "**L2**\n",
    "$$ Error-Function =  - \\frac{1}{m} \\sum_{i=1}^m (1-y_{i})\\ln(1 - \\hat{y_i}) + y_{i}\\ln \\hat{(y_i)} + \\lambda\\left( w_1^2 + \\cdots + w_n^2 \\right) $$\n",
    "\n",
    "As you can see the function after $\\lambda$ these two are large if the weights are large. The lambda parameter will tell us how much we want to penalize the coefficients. If lambda is large we penalized them a lot. And if lambda is small then we don't as much. Both regularization methods are very popular and it depends on our goals of application. Several general guidelines for deciding between L1 and L2 regularization. \n",
    "\n",
    "![regularization](..\\data\\regularization.png)\n",
    "\n",
    "When we apply L1, we tend to end up with sparse vectors. That means, small weights will tend to go to zero. So if we want to reduce the number of weights and end up with a small set, we can use L1. This is also good for feature selections and sometimes we have a problem with hundreds of features, and L1 will help us select which ones are important, and turn the rest into zeros.\n",
    "\n",
    "L2 on the other hand, tends not to favor sparse vectors since it tries to maintain all the weights homogeneously small. This one normally gives better results for training models.\n",
    "\n",
    "But why does L1 give sparse vectors and L2 produce vectors with small homeogenous weights?\n",
    "\n",
    "If we take the vector (1,0), the sums of the absolute values of the weights are one, and the sums of the squares of the weights are also one. But if we take the vector (0.5, 0.5), the sums of the absolute values of the weights is still one, but the sums of the squares is 0.25+0.25, which is 0.5. Thus, L2 will prefer the vector point (0.5, 0.5) over the vector (1, 0) since this one produces a smaller sum of squares and in turn a smaller function. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def sigmoid(x):\n",
    "    import numpy as np\n",
    "    return 1 / (1 + np.exp(-x))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(0.9999999979388463, 2.0611536181902037e-09)"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "sigmoid(10+10), sigmoid(-10-10)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Dropout\n",
    "\n",
    "Sometimes when we train neural netrworks one part of the network has very large weights and it ends up dominating all the training, while another part of the network doesn't really play much of a role so it doesn't get trained. So, what we'll do to solve this is something during training. So sometimes during training we'll turn one part off and let the rest of the network train. More throughly, what we do is as we go through the epochs, we randomly turn off some of the nodes, in that case the other nodes must pick up the slack and take more part in the training. \n",
    "In the first epoch we don't use one random node, and we do our feat forward and out back propagation passes without using it. \n",
    "In the second epoch, we don't use two other random nodes, again we do our feed forward and back prop. And in the third epoch we can't us other nodes, etc..\n",
    "What we'll do to drop the nodes is we'll give the algorithm a parameter. This parameter is the probability that each node gets dropped at a particular epoch. For example, if we give it a 0.2 it means each epoch, each node gets turned off with a probability of 20 percent. Some nodes may get turned off more than "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Local Minima\n",
    "\n",
    "So what gradient descent does. What it does is it looks at the direction where you descend the most and then it takes a step in that direction. \n",
    "\n",
    "![local minima](../data/local_minima.png)\n",
    "\n",
    "One way to solve this is to use random restarts, we start from a few different random places and do gradient descent from all of them. This increases the probability that we'll get to a global minimum, or at least a good local minimum. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Vanishing Gradient\n",
    "\n",
    "Another problem that can occur. Take a look at the sigmoid function. The curve getfs pretty flat on the sides. So, if we calculate the derivative at a point way at the right or the left, this derivative is almost zero. \n",
    "\n",
    "![sigmoid](..\\data\\sigmoid.png)\n",
    "\n",
    "This is not good cause a derivative is what tells us in what direction to move. This gets even worse in most linear perceptrons. Recall that the derivative of the error function with respect to a weight was the product of all the derivatives calculated at the nodes in the corresponding path to the output. All these derivatives are derivatives as a sigmoid function, so they're small and the product of a bunch of small numbers is tiny. This makes the training difficult because basically gradint descent gives us very, very tiny changes to make on the weights, which means, we make very tiny steps. \n",
    "\n",
    "**The best way to fix this is to change the activation function.**\n",
    "\n",
    "$$ \\tanh (x) = \\frac{e^x -e^{-x}}{ e^x +e^{-x}}$$ \n",
    "\n",
    "Similar to sigmoid but since the range is between -1 to 1 the derivatives are larger. \n",
    "\n",
    "$$  relu(x) =\n",
    "\\begin{cases}\n",
    "x  & \\text{if $x \\ge 0$)} \\\\\n",
    "0 & \\text{if $x \\lt 0$}\n",
    "\\end{cases}\n",
    "$$\n",
    "\n",
    "Rectified Linear Unit or ReLU, its a very simple function it only says, if you're positive, I'll return the same value and if your negative I will return zero. This is used a lot instead of sigmoid and can increase the training significantly without sacrificing much accuracy, since the derivative is one if the number is positive. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "So the last activation function before the output determines what type of model you are trying to do. Classification or Regression (prediction value). So for these examples, when we switch ReLU as the activation value inside, the final activation function still requires a sigmoid as we need a probability between 0 and 1, a ReLU at the end is used in RNNs. \n",
    "\n",
    "![multilayer perceptron](..\\data\\multi_layer_perceptron.png)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Stochastic Gradient Descent\n",
    "\n",
    "### Learning Rate\n",
    "\n",
    "If the model is not learning, decrease the learning rate. The best learning rates are those which decrease as the model is getting closer to a solution. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
