{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Training a Neural Network"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Overfitting and Underfitting\n",
    "\n",
    "Introducing a testing set. \n",
    "\n",
    "We will error on the side of an overly complicated models and then we'll apply certain techniques to prevent overfitting on it. \n",
    "\n",
    "On our first epoch, we will have a model with certain weights that probably made many errors. But as we get to let's say for 20 epochs we get a pretty good model. \n",
    "\n",
    "The plot is called the model complexity graph which plots the training error next to the testing error. Error, on the y-axis, while epoch on the x-axis (complexity). \n",
    "\n",
    "On the left, we have a high underfiting with a high training error and a high testing error, while on the  right we have a high testing error with a low training error. We need to pick something in the middle just right **goldilocks point**. \n",
    "\n",
    "So in short, we degrade in descent until the testing error stops decreasing and starts to increase. At that moment we stop. This algorithm is called **Early Stopping** and is widely used to train neural networks. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
