{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Gradient Descent\n",
    "\n",
    "### Notes on Udacity"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Gradient Descent with Squared Errors\n",
    "We want to find the weights for our neural networks. Let's start by thinking about the goal. The network needs to make predictions as close as possible to the real values. To measure this, we use a metric of how wrong the predictions are, the error. A common metric is the sum of the squared errors (SSE):\n",
    "\n",
    "$$E = \\frac{1}{2}\\sum_{\\mu} \\sum_j \\left[ y^{\\mu}_j - \\hat{y} ^{\\mu}_j \\right]^2$$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": true
   },
   "source": [
    "where $\\hat y $ is the prediction and $y$is the true value, and you take the sum over all output units jj and another sum over all data points $\\mu$. This might seem like a really complicated equation at first, but it's fairly simple once you understand the symbols and can say what's going on in words."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "def sigmoid(x):\n",
    "    return 1/(1 + np.exp(-x))\n",
    "\n",
    "def sigmoid_prime(x):\n",
    "    return sigmoid(x) * (1 - sigmoid(x))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# input data\n",
    "\n",
    "x =np.array([0.1, 0.3])\n",
    "\n",
    "#target\n",
    "y = 0.2\n",
    "\n",
    "#input to output weights\n",
    "weights = np.array([-0.8, 0.5])\n",
    "\n",
    "# the learning rate, eta in the weight step equation\n",
    "learnrate = 0.5\n",
    "\n",
    "# the linear combination performed by the node (h in f(h) and f'(h))\n",
    "h = np.dot(x, weights)\n",
    "\n",
    "# the Neural Network output y-hat\n",
    "nn_output = sigmoid(h)\n",
    "\n",
    "# output error (y - y_hat)\n",
    "error = y - nn_output\n",
    "\n",
    "# output gradient (f'(h))\n",
    "output_grad = sigmoid_prime(h)\n",
    "\n",
    "# error_term (lowercase delta)\n",
    "error_term = error * output_grad\n",
    "\n",
    "# Gradient descent step \n",
    "del_w = [ learnrate * error_term * x[0],\n",
    "          learnrate * error_term * x[1]]\n",
    "# or learnrate * error_term * x"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Example implementing gradient descent\n",
    "\n",
    "Okay, now we know how to update our weights (example above) and formula:\n",
    "\n",
    "$$\\nabla{w_{ij}} = \\eta * \\delta_{j} * x_{i}$$\n",
    "\n",
    "You've seen how to implement that for a single update, but how do we translate that code to calculate many weight updates so our network will learn?\n",
    "\n",
    "We will train a network on graduate school admissions data found at http://www.ats.ucla.edu/stat/data/binary.csv. \n",
    "\n",
    "This dataset has three input features, GRE Score, GPA, and the rank of the undergraduate school (numbered 1 through 4). Institutions with rank 1 have the highest prestige, those with rank 4 have the lowest. \n",
    "\n",
    "The goal here is to predict if a student will be admitted to a graduate program based on these features. For this, we will use a network with one output layer and one unit. We will use a sigmoid function for the output unit activation. \n",
    "\n",
    "## Data Cleanup \n",
    "\n",
    "You might think there will be three input units, but we actually need to transform the data first. The rank feature is categorical, the numbers do not encode any sort of relative value. Rank 2 is not twice as much as rank 1, rank 3 is not 1.5 more than rank 2. \n",
    "\n",
    "Instead, we will use dummy variables to encode the rank, splitting the data into four new columns encoded with ones or zeros. Rows with rank 1 will have one in the rank 1 dummy column, and zeros in all other columns. This will go for each rank number etc.\n",
    "\n",
    "We will also need to standardize the GRE and GPA data, which means to scale the values such that they have zero mean and a standard deviation of 1. This is necessary because the sigmoid funtion squashes really small and really large inputs. The gradient of really small and large inputs is zero, which means that the gradient descent step will go to zero too. Since the GRE and GPA values are fairly large, we will have to be really careful about how we initalize the weights or the gradient descent steps will die off and the network won't train. Instead, if we standardize the data, we can initialize the weights easily and everyone is happy. \n",
    "\n",
    "**Mean Square Error**\n",
    "\n",
    "We're going to make a small change to how we calculate the error here. Instead of the SSE, we're going to use the **mean** of the square errors (MSE). Now that we're using a lot of data, summing up all the weight steps can lead to really large updates that make the gradient descent diverge. To compensate for this, you'd need to use a quite small learning rate. Instead, we can just divide by the number of records in our data, $m$ to take the average. This way, no matter how much data we use, our learning rates will typically be in the range of 0.01 to 0.001. Then, we can use the MSE (shown below) to calculate the gradient and the result is the same as before, just averaged instead of summed. \n",
    "\n",
    "$$ E = \\frac{1}{2m} \\sum_{\\mu} \\left(y^\\mu - \\hat{y}_{\\mu} \\right)^2 $$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Here's the general algorithm for updating the weights with gradient descent.\n",
    "\n",
    "1) Set the weight step to zero: $\\nabla w_i = 0$\n",
    "\n",
    "2) For each record in the training data:\n",
    "- Make a forward pass through the network, calculating the output\n",
    "   \n",
    "    $$\\hat{y} = f\\left(\\sum_i w_i x_i \\right)$$\n",
    "- Calculate the error term for the output unit\n",
    "\n",
    "$$ \\delta = \\left(y - \\hat{y} \\right) * f^\\prime \\left(\\sum_i w_i x_i \\right) $$\n",
    "\n",
    "- Update the weight step $\\nabla w_i = \\nabla w_i + \\delta x_i$\n",
    "\n",
    "3) Update the weights $w_i = w_i + \\eta \\nabla w_i / m$ where $\\eta$ is the learning rate and m is the number of records. Here we're averaging the weight steps to help reduce any large variations in the training data.\n",
    "\n",
    "4) Repeat for e epochs.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Implementing with NumPy\n",
    "\n",
    "First, you'll need to initalize the weights. We want these to be small such that the input to the sigmoid is in the linear region near 0 and not squashed at the high and low ends. It's also important to initialize them randomly so that they all have different starting values and diverge, breaking symmetry. So we'll initialize the weights from a normal distribution centered at 0. A good value for the scale is $ 1 / \\sqrt{n}$ where n is the number of input units. This keeps the input to the sigmoid low for increasing numbers of input units.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "n_features = 8\n",
    "weights = np.random.normal(scale = 1 / n_features **.5, size=n_features)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "NumPy provides a function np.dot() that calculates the dot product of two arrays, which conveniently calculates $h$ for us. The dot product multiplies two arrays element-wise, the first element in array 1 is multiplied by the first element in array 2, and so on. Then, each product is summed. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "#ouput_in = np.dot(weights, inputs)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "And finally, we can update $\\nabla w_i$ and $w_i$ by incrementing them with weights += ...."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Efficiency tip!\n",
    "\n",
    "You can save some calculations since we're using a sigmoid here. For the sigmoid function, $f^\\prime(h) = f(h)(1-f(h))$ That means that once you calculate $f(h)$, the activation of the output unit, you can use it to calculate the gradient for the error gradient."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Example: Admissions Data\n",
    "\n",
    "Here we will do the example in Gradient Descent lesson in Deep Learning Nanodegree.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Data Prep"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import pandas\n",
    "\n",
    "admissions = pandas.read_csv('../data/binary.csv')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Make dummy variables for rank\n",
    "data = pandas.concat([admissions, pandas.get_dummies(admissions['rank'], prefix='rank')], axis=1)\n",
    "data.drop('rank', axis=1, inplace=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Standardize features\n",
    "for field in ['gre', 'gpa']:\n",
    "    mean, std = data[field].mean(), data[field].std()\n",
    "    data.loc[:,field] = (data[field] - mean) / std"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/anaconda3/lib/python3.6/site-packages/ipykernel_launcher.py:4: DeprecationWarning: \n",
      ".ix is deprecated. Please use\n",
      ".loc for label based indexing or\n",
      ".iloc for positional indexing\n",
      "\n",
      "See the documentation here:\n",
      "http://pandas.pydata.org/pandas-docs/stable/indexing.html#ix-indexer-is-deprecated\n",
      "  after removing the cwd from sys.path.\n"
     ]
    }
   ],
   "source": [
    "# split off random_10% of the data for testing\n",
    "np.random.seed(42)\n",
    "sample = np.random.choice(data.index, size=int(len(data)*0.9), replace=False)\n",
    "data, test_data = data.ix[sample], data.drop(sample)\n",
    "\n",
    "#Split into features and targets\n",
    "features, targets = data.drop('admit', axis=1), data['admit']\n",
    "features_test, targets_test = test_data.drop('admit', axis=1), test_data['admit']"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Gradient Steps\n",
    "\n",
    "Programming exercise\n",
    "Below, you'll implement gradient descent and train the network on the admissions data. Your goal here is to train the network until you reach a minimum in the mean square error (MSE) on the training set. You need to implement:\n",
    "\n",
    "- The network output: output.\n",
    "- The output error: error.\n",
    "- The error term: error_term.\n",
    "- Update the weight step: del_w +=.\n",
    "- Update the weights: weights +=.\n",
    "\n",
    "After you've written these parts, run the training by pressing \"Test Run\". The MSE will print out, as well as the accuracy on a test set, the fraction of correctly predicted admissions.\n",
    "\n",
    "Feel free to play with the hyperparameters and see how it changes the MSE."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def sigmoid(x):\n",
    "    return 1 /(1+np.exp(-x))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# Set seed, records, features, and last loss\n",
    "np.random.seed(18)\n",
    "n_records, n_features = features.shape\n",
    "last_loss = None\n",
    "\n",
    "# Initialize weights\n",
    "weights = np.random.normal(scale=1 / n_features**.5, size=n_features)\n",
    "\n",
    "# Neural Network hyperparameters\n",
    "epochs = 1000\n",
    "learnrate = 0.5"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train loss:  0.2659720509102629\n",
      "Train loss:  0.2127815193115812\n",
      "Train loss:  0.20225122157508604\n",
      "Train loss:  0.1992456789974773\n",
      "Train loss:  0.19810007767962187\n",
      "Train loss:  0.19758332928381614\n",
      "Train loss:  0.19732274589024312\n",
      "Train loss:  0.19718067213447626\n",
      "Train loss:  0.19709871014961833\n",
      "Train loss:  0.19704941361886977\n",
      "Prediction Accuracy: 0.725\n"
     ]
    }
   ],
   "source": [
    "for e in range(epochs):\n",
    "    del_w = np.zeros(weights.shape)\n",
    "    for x, y in zip(features.values, targets):\n",
    "        # loop through all records, x is the input, y is the target\n",
    "        \n",
    "        # we have not included the h variable, as we did in other lessons\n",
    "        # we can add h if we want\n",
    "        # h = np.dot(weights, x)\n",
    "        \n",
    "        # the output is the sigmoid of h\n",
    "        # Rather than storing h as a separate variable, \n",
    "        # we multiply the inputs and the weights here\n",
    "        output = sigmoid(np.dot(weights, x))\n",
    "        \n",
    "        # calculate the error\n",
    "        error = y - output\n",
    "        \n",
    "        # calculate the error term\n",
    "        error_term = error * output * (1 - output)\n",
    "        \n",
    "        # calculate the change in weights for this sample\n",
    "            # and add it to the total weight change\n",
    "        del_w += error_term * x\n",
    "        \n",
    "    weights += learnrate * del_w / n_records\n",
    "    \n",
    "    # printing out the mean square error on the training data set\n",
    "    if e % (epochs / 10) == 0:\n",
    "        out = sigmoid(np.dot(features, weights))\n",
    "        loss = np.mean((out-targets) ** 2)\n",
    "        if last_loss and last_loss < loss:\n",
    "            print('Train loss:', loss, \" WARNING - Loss Increasing\")\n",
    "        else:\n",
    "            print(\"Train loss: \", loss)\n",
    "        last_loss = loss\n",
    "\n",
    "# calculate accuracy on test data\n",
    "test_out = sigmoid(np.dot(features_test, weights))\n",
    "predictions = test_out > 0.5\n",
    "accuracy = np.mean(predictions == targets_test)\n",
    "print(\"Prediction Accuracy: {:.3f}\".format(accuracy))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Multi-layer Perceptrons\n",
    "\n",
    "Before we were dealing with only one output node which made the code straightforward. However now that we have multiple input units and multiple hidden units, the weights between them will require two indices: $w_{ij}$ where i denotes input units and $j$ are the hidden units. \n",
    "\n",
    "For example, the following image shows our network, with its input units labeled $x_1, x_2$ and $x_3$, and its hidden notes labeled $h_1$ and $h_2$\n",
    "\n",
    "![Weights Network](..\\data\\weights_network1.png)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Before we were able to write the weights as an array, indexed as $wi$. \n",
    "\n",
    "But now, the weights need to be stored in a **matrix**, indeed as $w_{ij}$. Each **row** in the matrix will correspond to the weights leading out of a single input unit, and for each column will correspond to the weights leading in to a **single hidden unit**. For our three input units and two hidden units, the weights matrix looks like this:\n",
    "\n",
    "$ = \\left[\n",
    "\\begin{array}{cc}\n",
    "    w_{11} w_{12}\\\\\n",
    "    w_{21} w_{22}\\\\\n",
    "    w_{31} w_{32}\n",
    "\\end{array}\n",
    "\\right]$\n",
    "\n",
    "So $h_1$ to $x_1$ would be $w_{11}$, and $x_2$ would be $w_{21}$, while $h_2$ to $x_1$ would be $w_{12}$, while to $x_2$ would be $w_{22}$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "![title](..\\data\\weights_network.png)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Be sure to compare the matrix above with the diagram shown before it so you can see where the different weights in the network end up in the matrix.\n",
    "\n",
    "To initialize these weights in NumPy, we have to provide the shape of the matrix. If features is a 2D array containing the input data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# number of records and input units\n",
    "n_records, n_inputs = features.shape\n",
    "\n",
    "# number of hidden units\n",
    "n_hidden = 2\n",
    "weights_input_to_hidden = np.random.normal(0, n_inputs**-0.5, size=(n_inputs, n_hidden))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "This creates a 2D array (i.e. a matrix) named weights_input_to_hidden with dimension n_inputs by n_hidden. Remember how the input to a hidden unit is the sum of all the inputs multiplies by the hidden units weights. So for each hidden layer unit, $h_j$, we need to calculate the following:\n",
    "\n",
    "$$ h_j = \\sum_i w_{ij}x_i$$\n",
    "\n",
    "To do that we now use matrix multiplication. In this case, we are multiplying the inputs (a row vector here) by the weights. To do this, you take the dot (inner) product of the inputs with each column in the weights matrix. For example, to calculate the input to the first hidden unit, $j=1$, you'd take the dot product of the inputs with the first column of the weights matrix, like so:\n",
    "\n",
    "![first hidden](..\\data\\weights_firsthidden.png)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Calculating the input to the first hidden unit with the first column of the weights matrix.**\n",
    "\n",
    "$$h_1 = x_1 w_{11} + x_2 w_{21} + x_3 w_{31}$$\n",
    "\n",
    "And for the second hidden layer input, you calculate the dot product of the inputs with the second column. And so on and so forth. \n",
    "\n",
    "In NumPy, you can do this for all the inputs and all the outputs at once using np.dot\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "hiden_inputs = np.dot(inputs, weights_input_to_hidden)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "You could also define your weights matrix such that it has dimensions n_hidden by n_inputs then multiply like so where the inputs form a column vector:\n",
    "\n",
    "![hj](..\\data\\hj.png)\n",
    "\n",
    "**Note: ** The weight indices have changed in the above image and no longer match up with the labels used in earlier diagrams. That's beacuse, in matrix notation, **the row index always precedes the column index**, so it would be misleading to label them the way we did in the neural net diagram. Just keep in mind that this is the same weight matrix as before, but rotated so the first column is now the first row, and the second column is now the second row.\n",
    "\n",
    "The important thing with matrix multiplication is that _the dimension match_.\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The dot product can't be computed for a 3x2 matrix and 3-element array. That's because the 2 columns in the matrix don't match the number of elements in the array. \n",
    "\n",
    "The rule is that if you're multiplying an array from the left, the array must have the same number of elements as there are rows in the matrix. And if you're multiplying the matrix from the left, the number of columns in the matrix must equal the number of elements in the array on the right.\n",
    "\n",
    "## Making a column vector\n",
    "\n",
    "You see above that sometimes you'll want a column vector, even though by default NumPy arrays work like row vectors. It's possible to get the transpose of an array like so arr.T, byt for a 1D array, the transpose will return a row vector. Instead use arr[:,None] to create a column vector:\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Example: Multi-layer Perceptrons\n",
    "\n",
    "Programming quiz\n",
    "Below, you'll implement a forward pass through a 4x3x2 network, with sigmoid activation functions for both layers.\n",
    "\n",
    "Things to do:\n",
    "\n",
    "1. Calculate the input to the hidden layer.\n",
    "2. Calculate the hidden layer output.\n",
    "3. Calculate the input to the output layer.\n",
    "4. Calculate the output of the network."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Hidden-layer Output:\n",
      "[0.41492192 0.42604313 0.5002434 ]\n",
      "Output-layer Output:\n",
      "[0.49815196 0.48539772]\n"
     ]
    }
   ],
   "source": [
    "import numpy as np\n",
    "\n",
    "def sigmoid(x):\n",
    "    \"\"\"\n",
    "    Calculate sigmoid\n",
    "    \"\"\"\n",
    "    return 1/(1+np.exp(-x))\n",
    "\n",
    "# Network size\n",
    "N_input = 4\n",
    "N_hidden = 3\n",
    "N_output = 2\n",
    "\n",
    "np.random.seed(42)\n",
    "# Make some fake data\n",
    "X = np.random.randn(4)\n",
    "\n",
    "weights_input_to_hidden = np.random.normal(0, scale=0.1, size=(N_input, N_hidden))\n",
    "weights_hidden_to_output = np.random.normal(0, scale=0.1, size=(N_hidden, N_output))\n",
    "\n",
    "\n",
    "# TODO: Make a forward pass through the network\n",
    "\n",
    "hidden_layer_in = np.dot(X, weights_input_to_hidden)\n",
    "hidden_layer_out = sigmoid(hidden_layer_in)\n",
    "\n",
    "print('Hidden-layer Output:')\n",
    "print(hidden_layer_out)\n",
    "\n",
    "output_layer_in = np.dot(hidden_layer_out, weights_hidden_to_output)\n",
    "output_layer_out = sigmoid(output_layer_in)\n",
    "\n",
    "print('Output-layer Output:')\n",
    "print(output_layer_out)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Backpropagation\n",
    "\n",
    "Now we've come to the problem of how to make a multilayer neural network _learn_. Before, we saw how to update weights with gradient descent. THe backpropagation algorithm is just an extension of that, using the chain rule to find the error with respect to the weights connecting the input layer to the hidden layer (for a two layer network). \n",
    "\n",
    "To update the weights to hidden layers using gradient descent, you need to know how much error each of the hidden units contributed to the final output. Since the output of a layer is determined by the weights between layers, the error resulting from units is scaled by the weights going forward through the network. Since we know the error at the output, we can use the weights to work backwards to hidden layers. \n",
    "\n",
    "For example, in the output layer, you have errors $\\delta_k^o$ attributed to each output unit $k$. Then , the error attributed to hidden unit $j$ is the output errors, scaled by the weights between the output and hidden layers (and the gradient):\n",
    "\n",
    "$$\\delta_j^h = \\sum W_{jk} \\delta_k^o f^\\prime (h_j)$$\n",
    "\n",
    "Then, the gradient descent step is the same as before, just with new errors:\n",
    "\n",
    "$$\\nabla w_{ij} = \\eta \\delta_j^h x_i $$\n",
    "\n",
    "where $w_{ij}$ are the weights between the inputs and hidden layer and $x_i$ are input unit values. This form holds for however many layers there are. The weight steps are equal to the step size times the output error of the layer times the values of the inputs to that layer. \n",
    "\n",
    "$$\\nabla_{pq} = \\eta\\delta_{output}V_{in}$$\n",
    "\n",
    "Here, you get the output error, $\\delta_{output}$, by propagating the error backwards from the higher layers. And the input values, $V_{in}$ are the inputs to the layer, the hidden layer activations to the output unit for example. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Working through an example\n",
    "\n",
    "Let's walk through the steps of calcuating the weight updates for a simple two layer network. Suppose there are two input values, one hidden unit, and one output unit, with sigmoid activations on the hidden and output units. The following imade depits the network (**Note:** the input values are shown as nodes as the bottom of the imade, while the network's output value is shown as $\\hat{y}$ at the top. The inputs themselves do not count as a layer, which is why this is considered a two layer network. \n",
    "\n",
    "![network](..\\data\\network.png)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Assume we're trying to fit some binary data and the target is y = 1, We'll start with the forward pass, first calculating the input to the hidden unit\n",
    "\n",
    "$ h = \\sum_i w_i x_i = 0.1 \\times 0.4 - 0.2 \\times 0.3 = -0.02$\n",
    "\n",
    "and the **output of the hidden units**\n",
    "\n",
    "$ \\alpha = f(h) = sigmoid(-0.02) = 0.495$\n",
    "\n",
    "Using this as the input to the output unit, the **output of the network** is \n",
    "\n",
    "$\\hat{y} = f(W \\cdot a) = sigmoid(0.1 \\times 0.495) = 0.512$\n",
    "\n",
    "With the network output, we can start the backwards pass to calculate the weight updates for both layers. Using the fact that the sigmoid function \n",
    "\n",
    "$f^\\prime(W \\cdot a) = f(W \\cdot a)(1 - f(W \\cdot a))$, the error term for the **output unit** is\n",
    "\n",
    "$\\delta^o = (y - \\hat{y})f^\\prime(W\\cdot a) = (1 - 0.512) \\times 0.512 \\times (1 - 0.512) = 0.122$\n",
    "\n",
    "Now we need to calculate the error term for the hidden unit with backpropogation. Here we'll scale the error term from the output unit by the weight **W** connecting it to the hidden unit. For the hidden unit error term, $\\delta_j^h = \\sum_k W_{jk} \\delta_k^o f^\\prime (h_j)$, but since we have one hidden unit and one output unit, this is much simpler\n",
    "\n",
    "$\\delta^h = W \\delta^o f^\\prime(h) = 0.1 \\times 0.122 \\times 0.495 \\times (1 - 0.495) = 0.003$\n",
    "\n",
    "Now that twe have the errors, we can calculate the gradient descent steps. The hidden to output weight step is the learning rate, times the output unit error, times the hidden unit activation value. \n",
    "\n",
    "$\\nabla W = \\eta \\delta^o \\alpha = 0.5 \\times 0.122 \\times 0.495 = 0.0302$\n",
    "\n",
    "Then, for the input to hidden weights $w_i$, it's the learning rate times the hidden unit error, times the input values. \n",
    "\n",
    "$\\nabla w_i = \\eta \\delta^h x_i = (0.5 \\times 0.003 \\times 0.1, 0.5 \\times 0.003 \\times 0.3) = (0.00015, 0.00045)$\n",
    "\n",
    "From this example, you can see one of the effects of using the sigmoid function for the activations. The maximum derivation of the sigmoid function is 0.25, so the errors in the output layer get reduced by at least 75%, and errors in the hidden layer are scaled down by at least 93.75%! You can see that if you have a lot of layers, using a sigmoid activation function will quickly reduce the weight steps to tiny values in layers near the input. This is known as the **vanishing gradient** problem. Later in the course you'll learn about other activation functions that perform better in this regard and are more commonly used in modern network architectures. \n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Implementing in NumPy\n",
    "\n",
    "For the most we have everything we need, \n",
    "\n",
    "### Backpropagation exercise\n",
    "Below, you'll implement the code to calculate one backpropagation update step for two sets of weights. I wrote the forward pass - your goal is to code the backward pass.\n",
    "\n",
    "#### Things to do\n",
    "\n",
    "1. Calculate the network's output error.\n",
    "2. Calculate the output layer's error term.\n",
    "3. Use backpropagation to calculate the hidden layer's error term.\n",
    "4. Calculate the change in weights (the delta weights) that result from propagating the errors back through the network."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Change in weights for hidden layer to output layer:\n",
      "[0.00804047 0.00555918]\n",
      "Change in weights for input layer to hidden layer:\n",
      "[[ 1.77005547e-04 -5.11178506e-04]\n",
      " [ 3.54011093e-05 -1.02235701e-04]\n",
      " [-7.08022187e-05  2.04471402e-04]]\n"
     ]
    }
   ],
   "source": [
    "import numpy as np\n",
    "\n",
    "\n",
    "def sigmoid(x):\n",
    "    \"\"\"\n",
    "    Calculate sigmoid\n",
    "    \"\"\"\n",
    "    return 1 / (1 + np.exp(-x))\n",
    "\n",
    "\n",
    "x = np.array([0.5, 0.1, -0.2])\n",
    "target = 0.6\n",
    "learnrate = 0.5\n",
    "\n",
    "weights_input_hidden = np.array([[0.5, -0.6],\n",
    "                                 [0.1, -0.2],\n",
    "                                 [0.1, 0.7]])\n",
    "\n",
    "weights_hidden_output = np.array([0.1, -0.3])\n",
    "\n",
    "## Forward pass\n",
    "hidden_layer_input = np.dot(x, weights_input_hidden)\n",
    "hidden_layer_output = sigmoid(hidden_layer_input)\n",
    "\n",
    "output_layer_in = np.dot(hidden_layer_output, weights_hidden_output)\n",
    "output = sigmoid(output_layer_in)\n",
    "\n",
    "## Backwards pass\n",
    "## TODO: Calculate output error\n",
    "error = target - output\n",
    "\n",
    "# TODO: Calculate error term for output layer\n",
    "output_error_term = error * output * (1 - output)\n",
    "\n",
    "# TODO: Calculate error term for hidden layer\n",
    "hidden_error_term = np.dot(output_error_term, weights_hidden_output) * \\\n",
    "hidden_layer_output * (1 - hidden_layer_output) \n",
    "\n",
    "# TODO: Calculate change in weights for hidden layer to output layer\n",
    "delta_w_h_o = learnrate * output_error_term * hidden_layer_output\n",
    "\n",
    "# TODO: Calculate change in weights for input layer to hidden layer\n",
    "delta_w_i_h = learnrate * hidden_error_term * x[:, None]\n",
    "\n",
    "print('Change in weights for hidden layer to output layer:')\n",
    "print(delta_w_h_o)\n",
    "print('Change in weights for input layer to hidden layer:')\n",
    "print(delta_w_i_h)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Implementing backpropagation\n",
    "\n",
    "Now we've seen that the error term for the output layer is \n",
    "\n",
    "$\\delta_k = (y_k - \\hat{y}_k) f^\\prime(\\alpha_k)$\n",
    "\n",
    "and the error term for the hidden layer is \n",
    "\n",
    "$$ \\delta_j = \\sum \\left[w_{jk}\\delta_k\\right] f^\\prime(h_j)$$\n",
    "\n",
    "For now we'll only consider a simple network with one hidden layer and one output unit. Here's the general algorithm for updating the weights with backpropagation: \n",
    "\n",
    "**First, Set the weight steps for each layer to zero**\n",
    "\n",
    "- The input to hidden weights $\\nabla w_{ij} = 0$\n",
    "- The hidden to output weights $\\nabla W_j = 0$\n",
    "\n",
    "**Second, For each record in the training data:**\n",
    "\n",
    "- Make a forward pass through the network, calculating the output $\\hat{y}$\n",
    "- Calculate the error gradient in the output unit, $\\delta^o = (y - \\hat{y})\\ f^\\prime (z)$ where $z = \\sum_j W_j \\alpha_j$, the input to the output unit. \n",
    "- Propagate the errors to the hidden layer $\\delta_j^h = \\delta^o W_i \\ f^\\prime(h_j)$\n",
    "- Update the weight steps: \n",
    "    - $\\nabla W_j = \\nabla W_j + \\delta^o \\alpha_j$\n",
    "    - $\\nabla w_{ij} = \\nabla_{ij} + \\delta_j^h \\alpha_i$\n",
    "- Update the weights, where $\\eta$ is the learning rate and $m$ is the number of records:\n",
    "    - $W_j = W_j + \\eta\\nabla W_j / m$\n",
    "    - $w_{ij} = w_{ij} + \\eta\\nabla w_{ij} / m$\n",
    "    \n",
    "    \n",
    "- Repeat for $e$ epochs.\n",
    "\n",
    "    "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Backpropagation exercise\n",
    "Now you're going to implement the backprop algorithm for a network trained on the graduate school admission data. You should have everything you need from the previous exercises to complete this one.\n",
    "\n",
    "**Your goals here:**\n",
    "\n",
    "- Implement the forward pass.\n",
    "- Implement the backpropagation algorithm.\n",
    "- Update the weights."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/anaconda3/lib/python3.6/site-packages/ipykernel_launcher.py:18: DeprecationWarning: \n",
      ".ix is deprecated. Please use\n",
      ".loc for label based indexing or\n",
      ".iloc for positional indexing\n",
      "\n",
      "See the documentation here:\n",
      "http://pandas.pydata.org/pandas-docs/stable/indexing.html#ix-indexer-is-deprecated\n"
     ]
    }
   ],
   "source": [
    "import numpy as np\n",
    "import pandas as pd\n",
    "\n",
    "admissions = pd.read_csv('../data/binary.csv')\n",
    "\n",
    "# Make dummy variables for rank\n",
    "data = pd.concat([admissions, pd.get_dummies(admissions['rank'], prefix='rank')], axis=1)\n",
    "data = data.drop('rank', axis=1)\n",
    "\n",
    "# Standarize features\n",
    "for field in ['gre', 'gpa']:\n",
    "    mean, std = data[field].mean(), data[field].std()\n",
    "    data.loc[:,field] = (data[field]-mean)/std\n",
    "    \n",
    "# Split off random 10% of the data for testing\n",
    "np.random.seed(21)\n",
    "sample = np.random.choice(data.index, size=int(len(data)*0.9), replace=False)\n",
    "data, test_data = data.ix[sample], data.drop(sample)\n",
    "\n",
    "# Split into features and targets\n",
    "features, targets = data.drop('admit', axis=1), data['admit']\n",
    "features_test, targets_test = test_data.drop('admit', axis=1), test_data['admit']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train loss:  0.2529161892134775\n",
      "Train loss:  0.2521576505212607\n",
      "Train loss:  0.2514181077544339\n",
      "Train loss:  0.2506969893839888\n",
      "Train loss:  0.2499937410828779\n",
      "Train loss:  0.24930782538355514\n",
      "Train loss:  0.24863872132151002\n",
      "Train loss:  0.24798592406752137\n",
      "Train loss:  0.2473489445511789\n",
      "Train loss:  0.24672730907792267\n",
      "Prediction accuracy: 0.650\n"
     ]
    }
   ],
   "source": [
    "np.random.seed(18)\n",
    "\n",
    "def sigmoid(x):\n",
    "    \"\"\"\n",
    "    Calculate sigmoid\n",
    "    \"\"\"\n",
    "    return 1 / (1 + np.exp(-x))\n",
    "\n",
    "\n",
    "# Hyperparameters\n",
    "n_hidden = 2 # number of hidden units\n",
    "epochs = 900\n",
    "learnrate = 0.005\n",
    "\n",
    "n_records, n_features = features.shape\n",
    "last_loss = None\n",
    "# initalize weights\n",
    "\n",
    "weights_input_hidden = np.random.normal(scale=1 / n_features ** .5, \n",
    "                                       size=(n_features, n_hidden))\n",
    "weights_hidden_output = np.random.normal(scale=1 / n_features **.5,\n",
    "                                        size=n_hidden)\n",
    "\n",
    "for e in range(epochs):\n",
    "    del_w_input_hidden = np.zeros(weights_input_hidden.shape)\n",
    "    del_w_hidden_output = np.zeros(weights_hidden_output.shape)\n",
    "    for x, y in zip(features.values, targets):\n",
    "        \n",
    "        ## Forward Pass ##\n",
    "        # Calculate the output\n",
    "        hidden_input = np.dot(x, weights_input_hidden)\n",
    "        hidden_output = sigmoid(hidden_input)\n",
    "        output = sigmoid(np.dot(hidden_output, weights_hidden_output))\n",
    "        \n",
    "        ## Backward Pass ##\n",
    "        # Calculate the network's prediction error\n",
    "        error = y - output\n",
    "        \n",
    "        # Calculate error term for the output unit\n",
    "        output_error_term = error * output * (1 - output)\n",
    "        \n",
    "        ## propagate errors to hidden layer\n",
    "        \n",
    "        # Calculate the hidden layer's contribution to the error \n",
    "        hidden_error = np.dot(output_error_term, weights_hidden_output)\n",
    "        \n",
    "        # Calculate the error term for the hidden layer\n",
    "        hidden_error_term = hidden_error * hidden_output * (1 - hidden_output) \n",
    "        \n",
    "        # Update the change in weights\n",
    "        del_w_hidden_output += output_error_term * hidden_output\n",
    "        del_w_input_hidden += hidden_error_term * x[:, None]\n",
    "        \n",
    "    weights_input_hidden += learnrate * del_w_input_hidden / n_records\n",
    "    weights_hidden_output += learnrate * del_w_hidden_output / n_records\n",
    "    \n",
    "    if e % (epochs / 10) == 0:\n",
    "        hidden_ouput = sigmoid(np.dot(x, weights_input_hidden))\n",
    "        out = sigmoid(np.dot(hidden_output,\n",
    "                            weights_hidden_output))\n",
    "        loss = np.mean((out-targets) ** 2)\n",
    "        if last_loss and last_loss < loss:\n",
    "            print(\"Train loss: \", loss, \"  WARNING - Loss Increasing\")\n",
    "        else:\n",
    "            print(\"Train loss: \", loss)\n",
    "        last_loss = loss\n",
    "\n",
    "# Calculate accuracy on test data\n",
    "hidden = sigmoid(np.dot(features_test, weights_input_hidden))\n",
    "out = sigmoid(np.dot(hidden, weights_hidden_output))\n",
    "predictions = out > 0.5\n",
    "accuracy = np.mean(predictions == targets_test)\n",
    "print(\"Prediction accuracy: {:.3f}\".format(accuracy))\n",
    "\n",
    "        "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Further Reading\n",
    "\n",
    "1. https://medium.com/@karpathy/yes-you-should-understand-backprop-e2f06eab496b\n",
    "\n",
    "2. https://www.youtube.com/watch?v=59Hbtz7XgjM\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
