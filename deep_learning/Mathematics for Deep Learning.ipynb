{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Scalars \n",
    "\n",
    "Have zero dimensions, is a single value, a persons height.\n",
    "\n",
    "## Vectors\n",
    "\n",
    "theres two types row vectors and column vectors\n",
    "\n",
    "A row vector would be$\\left[\\begin{array}{ccc}\n",
    "    1 2 3\\\\\n",
    "\\end{array}\n",
    "\\right]$, while a column vector would be $\\left[\\begin{array}{c}\n",
    "    1\\\\\n",
    "    2\\\\\n",
    "    3\n",
    "\\end{array}\n",
    "\\right]$\n",
    "\n",
    "They can store the same thing. But has 1 dimension. \n",
    "\n",
    "Row, Column to get the Index\n",
    "\n",
    "## Matrices\n",
    "$\\left[\\begin{matrix}\n",
    "    1 \\ 2 \\ 3\\\\\n",
    "    4 \\ 5 \\ 6\\\\\n",
    "    7 \\ 8 \\ 9 \n",
    "\\end{matrix}\n",
    "\\right]$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The most common way to work with numbers is through an ndarray object. Similar to lists, but can have any number of dimension. Also supports fast math operations, because written in C. Any form, scalars, vectors, matrices, or tensors. **But every item in the array must have the same type**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "18\n",
      "[1 2 3 4]\n",
      "() Scalar Shape\n",
      "(4,) Vector Shape\n",
      "(3, 3) Matrix Shape\n",
      "(3, 3, 2, 1) Tensor Shape\n"
     ]
    }
   ],
   "source": [
    "import numpy as np\n",
    "\n",
    "# Scalar\n",
    "s = np.array(18)\n",
    "print(s)\n",
    "\n",
    "# Vector \n",
    "v = np.array([1,2,3,4])\n",
    "print(v)\n",
    "\n",
    "\n",
    "\n",
    "# Matrix\n",
    "m = np.array([[1,2,3], [4,5,6], [7,8,9]])\n",
    "\n",
    "# Tensor\n",
    "t = np.array([[[[1],[2]],[[3],[4]],[[5],[6]]],[[[7],[8]],\\\n",
    "    [[9],[10]],[[11],[12]]],[[[13],[14]],[[15],[16]],[[17],[17]]]])\n",
    "\n",
    "print(s.shape, 'Scalar Shape')\n",
    "print(v.shape, 'Vector Shape')\n",
    "print(m.shape, 'Matrix Shape')\n",
    "print(t.shape, 'Tensor Shape')\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Changing Shapes\n",
    "\n",
    "Sometimes you need to change the change of your data, without changing its contents. \n",
    "\n",
    "We can do this with `x.reshape()`.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(4, 1)\n"
     ]
    }
   ],
   "source": [
    "x = v.reshape(4,1)\n",
    "print(x.shape)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "One more thing should be noted. If you see code from experienced numpy users, you will often see them use a special slicing syntax instead of calling reshape, using this the previous two example would look like\n",
    "\n",
    "`x = v[None, :]`"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(1, 4)\n",
      "(4, 1)\n"
     ]
    }
   ],
   "source": [
    "x = v[None, :]\n",
    "print(x.shape)\n",
    "\n",
    "# or\n",
    "\n",
    "x_reshape = v[:, None]\n",
    "print(x_reshape.shape)\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Element Wise Matrices\n",
    "\n",
    "To do this you can simply do"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[[ 3  5  7]\n",
      " [ 9 11 13]\n",
      " [15 17 19]]\n"
     ]
    }
   ],
   "source": [
    "# Addition\n",
    "add = v + 5\n",
    "scalar_to_vector = np.multiply(v, 5)\n",
    "\n",
    "# Matrix element wise\n",
    "\n",
    "b = m + 1\n",
    "\n",
    "matrix_add = b + m\n",
    "print(matrix_add)\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "If you try doing math with matricies of different shapes you will see a ValueError"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [
    {
     "ename": "ValueError",
     "evalue": "operands could not be broadcast together with shapes (2,2) (3,3) ",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mValueError\u001b[0m                                Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-23-c3191a534aad>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m()\u001b[0m\n\u001b[1;32m     19\u001b[0m \u001b[0;31m#  (3, 3)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     20\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 21\u001b[0;31m \u001b[0ma\u001b[0m \u001b[0;34m+\u001b[0m \u001b[0mc\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m",
      "\u001b[0;31mValueError\u001b[0m: operands could not be broadcast together with shapes (2,2) (3,3) "
     ]
    }
   ],
   "source": [
    "a = np.array([[1,3],[5,7]])\n",
    "a\n",
    "# displays the following result:\n",
    "# array([[1, 3],\n",
    "#        [5, 7]])\n",
    "c = np.array([[2,3,6],[4,5,9],[1,8,7]])\n",
    "c\n",
    "# displays the following result:\n",
    "# array([[2, 3, 6],\n",
    "#        [4, 5, 9],\n",
    "#        [1, 8, 7]])\n",
    "\n",
    "a.shape\n",
    "# displays the following result:\n",
    "#  (2, 2)\n",
    "\n",
    "c.shape\n",
    "# displays the following result:\n",
    "#  (3, 3)\n",
    "\n",
    "a + c"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Matrix Multiplication\n",
    "\n",
    "Matrix Multiplication vs Matrix Product\n",
    "\n",
    "dot product is just the element wise matrix multiplication, then added up into a single number\n",
    "\n",
    "Whenever you multiply two matrices, you're actually dealing with the rows of the first matrix and the columns of the second matrix, when you do matrix multiplication you do the dot product for the first row, with first column and that becomes the first entry in the new matrix. the number of rows is equal to the number of rows in the first matrix, with the number of columns equal to the second matrix. \n",
    "\n",
    "**This means the rows of the first matrix need to be the same length as the columns of the second matrix.**\n",
    "\n",
    "**The number of columns in the left matrix must equal the number of rows in the right matrix.**\n",
    "\n",
    "Order Matters,\n",
    "\n",
    "Multiplying $A * B$ is different than $B * A$"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[ 7, 10],\n",
       "       [15, 22]])"
      ]
     },
     "execution_count": 28,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "a = np.array([[1,2],[3,4]])\n",
    "\n",
    "np.dot(a,a) # can do this\n",
    "a.dot(a) # or call it directly on the array\n",
    "\n",
    "np.matmul(a, a)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "You may sometimes see NumPy's dot function in places where you would expect a matmul. It turns out that the results of dot and matmul are the same if the matrices are two dimensional."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Matrix Transpose\n",
    "\n",
    "A matrix is the same matrix, but has the rows and columns switched. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Original:\n",
      "  [[ 1  2  3  4]\n",
      " [ 5  6  7  8]\n",
      " [ 9 10 11 12]]\n",
      "\n",
      "Transpose:\n",
      " [[ 1  5  9]\n",
      " [ 2  6 10]\n",
      " [ 3  7 11]\n",
      " [ 4  8 12]]\n"
     ]
    }
   ],
   "source": [
    "m = np.array([[1,2,3,4], [5,6,7,8], [9,10,11,12]])\n",
    "print('Original:\\n ', m)\n",
    "print()\n",
    "print('Transpose:\\n', m.T)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Real World Use Case\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[[-0.01299  0.00664  0.13494]]\n",
      "[[-0.01299]\n",
      " [ 0.00664]\n",
      " [ 0.13494]]\n"
     ]
    }
   ],
   "source": [
    "inputs = np.array([[-0.27,  0.45,  0.64, 0.31]])\n",
    "inputs\n",
    "# displays the following result:\n",
    "# array([[-0.27,  0.45,  0.64,  0.31]])\n",
    "\n",
    "inputs.shape\n",
    "# displays the following result:\n",
    "# (1, 4)\n",
    "\n",
    "weights = np.array([[0.02, 0.001, -0.03, 0.036], \\\n",
    "    [0.04, -0.003, 0.025, 0.009], [0.012, -0.045, 0.28, -0.067]])\n",
    "\n",
    "weights\n",
    "# displays the following result:\n",
    "# array([[ 0.02 ,  0.001, -0.03 ,  0.036],\n",
    "#        [ 0.04 , -0.003,  0.025,  0.009],\n",
    "#        [ 0.012, -0.045,  0.28 , -0.067]])\n",
    "\n",
    "weights.shape\n",
    "# displays the following result:\n",
    "# (3, 4)\n",
    "\n",
    "# np.matmul(inputs, weights)\n",
    "print(np.matmul(inputs, weights.T))\n",
    "\n",
    "# or swapping the order, and taking the transpose of inputs\n",
    "print(np.matmul(weights, inputs.T))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[[ 1  2  3  4  5]\n",
      " [ 6  7  8  9 10]]\n",
      "Help on function squeeze in module numpy.core.fromnumeric:\n",
      "\n",
      "squeeze(a, axis=None)\n",
      "    Remove single-dimensional entries from the shape of an array.\n",
      "    \n",
      "    Parameters\n",
      "    ----------\n",
      "    a : array_like\n",
      "        Input data.\n",
      "    axis : None or int or tuple of ints, optional\n",
      "        .. versionadded:: 1.7.0\n",
      "    \n",
      "        Selects a subset of the single-dimensional entries in the\n",
      "        shape. If an axis is selected with shape entry greater than\n",
      "        one, an error is raised.\n",
      "    \n",
      "    Returns\n",
      "    -------\n",
      "    squeezed : ndarray\n",
      "        The input array, but with all or a subset of the\n",
      "        dimensions of length 1 removed. This is always `a` itself\n",
      "        or a view into `a`.\n",
      "    \n",
      "    Raises\n",
      "    ------\n",
      "    ValueError\n",
      "        If `axis` is not `None`, and an axis being squeezed is not of length 1\n",
      "    \n",
      "    See Also\n",
      "    --------\n",
      "    expand_dims : The inverse operation, adding singleton dimensions\n",
      "    reshape : Insert, remove, and combine dimensions, and resize existing ones\n",
      "    \n",
      "    Examples\n",
      "    --------\n",
      "    >>> x = np.array([[[0], [1], [2]]])\n",
      "    >>> x.shape\n",
      "    (1, 3, 1)\n",
      "    >>> np.squeeze(x).shape\n",
      "    (3,)\n",
      "    >>> np.squeeze(x, axis=0).shape\n",
      "    (3, 1)\n",
      "    >>> np.squeeze(x, axis=1).shape\n",
      "    Traceback (most recent call last):\n",
      "    ...\n",
      "    ValueError: cannot select an axis to squeeze out which has size not equal to one\n",
      "    >>> np.squeeze(x, axis=2).shape\n",
      "    (1, 3)\n",
      "\n"
     ]
    }
   ],
   "source": [
    "vec_t = np.array([[1,2,3,4,5],[6,7,8,9,10]])\n",
    "\n",
    "print(np.squeeze(vec_t))\n",
    "\n",
    "help(np.squeeze)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Derivatives\n",
    "\n",
    "A derivative can be defined in two ways:\n",
    "1) Instantaneous rate of change (Physics)\n",
    "2) Slope of a line at a specific point (Geometry)\n",
    "\n",
    "Both represent the same principle, but for our purposes its easier to explain using the geometric deinition.\n",
    "\n",
    "**Definition**\n",
    "Slope represents the steepness of a line. It answers the question, how much does $y$ or $f(x)$ change given a specific change in $x$?\n",
    "\n",
    "### Steps:\n",
    "1. Given the function $f(x) = x^2$\n",
    "\n",
    "2. Increment $x$ by a very small value $h (h = \\nabla x) $ $f(x+h) = (x+h)^2$\n",
    "\n",
    "3. Apply the slope formula $f(x+h) - f(x) \\over h$\n",
    "4. Simplify the equation $x^2 + 2xh + h^2 -x^2 \\over h$ = $ 2xh+h^2 \\over h$ $=2x+h$\n",
    "5. Set h to 0 $(\\lim_{h\\to0})$\n",
    "$2x+0=2x$\n",
    "\n",
    "### Result\n",
    "So what does this mean? It means for $f(x) = x^2$, the slope at any point can be calculated using the function $2x$.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "\n",
    "\n",
    "\n",
    "\n",
    "### Derivative of a constant function\n",
    "\n",
    "The derivative of f(x) = c where c is a constant is given by\n",
    "\n",
    "$ f^\\prime (x) = 0 $\n",
    "\n",
    "**Example**\n",
    "\n",
    "$ f(x) = -10 $, then $ f^\\prime (x) = 0 $\n",
    "\n",
    "### Derivative of the power of two functions (Power Rule)\n",
    "\n",
    "The derivative of a power function. The derivative of $f(x) = x^r$ where r is a constant real number is\n",
    "\n",
    "$ f^\\prime(x) = r x^{r-1}$\n",
    "\n",
    "**Example**\n",
    "\n",
    "$ f(x) = x^{-2}$, then $ f^\\prime(x) = -2 x ^{-3} = \\frac{-2}{x^{3}} $\n",
    "\n",
    "### Derivative of a function multiplied by a constant\n",
    "\n",
    "The derivative of f(x) = cg(x) is given by\n",
    "\n",
    "$ f^\\prime{x} = cg^{\\prime}(x) $\n",
    "\n",
    "**Example**\n",
    "\n",
    "$ f(x) = 3x^3$\n",
    "let c = 3 and $g(x) = x^3$, then $ f^\\prime(x) = cg^\\prime(x) $\n",
    "\n",
    "$ = 3(3x^2) = 9x^2 $\n",
    "\n",
    "### Derivative of the sum of functions (Sum Rule) \n",
    "\n",
    "The derivative of $f(x) = g(x) + h(x)$ is given by\n",
    "\n",
    "$ f^\\prime(x) = g^\\prime(x) + h^\\prime(x) $\n",
    "\n",
    "**Example**\n",
    "\n",
    "$ f(x) = x^2 + 4 $\n",
    "\n",
    "let $g(x) = x^2$ and $h(x) = 4$, then $f^\\prime(x) = g^\\prime(x) + h^\\prime(x) = 2(x) + 0 = 2x$\n",
    "\n",
    "### Derivative of the difference of functions\n",
    "\n",
    "The derivative of $f(x) = g(x) - h(x) $ is given by\n",
    "\n",
    "$ f^\\prime(x) = g^\\prime(x) - h^\\prime(x) $\n",
    "\n",
    "**Example**\n",
    "\n",
    "$f(x) = x^3 - x^{-2} $\n",
    "\n",
    "let $g(x) = x^3$ and $h(x) = x^{-2}$, then\n",
    "\n",
    "$ f^\\prime(x) = g^\\prime(x) - h^\\prime(x) = 3x^2 - (-2x^{-3}) = 3x^2 + 2x^{-3} $\n",
    "\n",
    "### Derivative of the product of two functions (Product Rule)\n",
    "\n",
    "The derivative of $f(x) = g(x) h(x)$ is given by\n",
    "\n",
    "$ f^\\prime(x) = g(x) h^\\prime(x) + h(x) g^\\prime(x) $\n",
    "\n",
    "**Example**\n",
    "\n",
    "$ f(x) = (x^2 - 2x) (x-2) $ \n",
    "\n",
    "let $g(x) = (x^2 - 2x)$ and $h(x) = (x - 2)$, then\n",
    "\n",
    "$f^\\prime(x) = g(x)h^\\prime(x) + h(x)g^\\prime(x) = (x^2 - 2x)(1) + (x-2)(2x-2) $\n",
    "\n",
    "$= x^2 - 2x + 2x^2 - 6x + 4 = 3x^2 - 8x + 4$\n",
    "\n",
    "### Derivative of the quotient of two functions (Quotient Rule)\n",
    "\n",
    "The derivative of f(x) = g(x) / h(x) is given by\n",
    "\n",
    "$$ f^\\prime(x) = \\frac{(h(x)g^\\prime(x) - g(x)h^\\prime(x))}{h(x)^2} $$\n",
    "\n",
    "**Example**\n",
    "\n",
    "$f(x) = \\frac{(x-2)}{(x+1)} $\n",
    "\n",
    "let $g(x) = (x-2) $ and $h(x) = (x+1)$, then\n",
    "\n",
    "$$f^\\prime(x) = \\frac{(h(x)g^\\prime(x) - g(x)h^\\prime(x))}{h(x)^2}$$\n",
    "\n",
    "$$ = \\frac{(x+1)(1) - (x-2)(1)}{(x+1)^2} $$\n",
    "\n",
    "$$ = \\frac{3}{(x+1)^2} $$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Chain Rule\n",
    "\n",
    "\n",
    "The chain rule says, if you have a variable $x$ on a function $f$ that you want to apply to $x$ to get $f(x)$ which were going to call \n",
    "\n",
    "$A = f(x)$\n",
    "\n",
    "then another function g which you apply to f(x) to get\n",
    "\n",
    "$ B = g \\circ f(x) $\n",
    "\n",
    "if you want to find the partial derivative of B with respect to x, thats just a partial derivative of B with respect to A times the partial derivative of A with respect to x. \n",
    "\n",
    "$$ \\frac{\\partial B}{\\partial x} = \\frac{\\partial B}{\\partial A} \\frac{\\partial A}{\\partial x}$$\n",
    "\n",
    "when composing functions, that derivatives just multiply, and this is going to be super useful for us because feed forwarding is literally composing a bunch of functions, and back propagation is literally taking the derivative at each piece, and since taking the derivative of a composition is the same as multiplying the partial derivatives, then all we're going to do is multiply a bunch of partial derivatives to get what we want. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Popular Derivatives\n",
    "\n",
    "$ \\frac{d}{dx} \\sqrt x = \\frac{1}{2\\sqrt{x}}$ \n",
    "\n",
    "$ \\frac{d}{dx} \\ln x = \\frac{1}{x}$\n",
    "\n",
    "$ \\frac{d}{dx} \\log x = \\frac{1}{x}$\n",
    "\n",
    "$ \\frac{d}{dx} \\sin x = \\cos x$\n",
    "\n",
    "$ \\frac{d}{dx} \\cos x = - \\sin x$\n",
    "\n",
    "$ \\frac{d}{dx} \\tan x = \\sec^2 x$\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": true
   },
   "source": [
    "## Examples\n",
    "\n",
    "**Chain Rule**:\n",
    "\n",
    "This can be used whenever your function is a composition of more than 1 function. \n",
    "$ h(x) = (\\sin x)^2$\n",
    "\n",
    "If we are going to take the derivative of $x^2$ with respect to $x$. it would be\n",
    "\n",
    "$ \\frac{d}{dx} [x^2] $ = $2x$\n",
    "\n",
    "another example \n",
    "\n",
    "$ \\frac{d}{da} [a^2] $ = $2a$\n",
    "\n",
    "Now we are only replacing $x$, or $a$ with $(\\sin x)$\n",
    "\n",
    "$\\frac{d}{d(\\sin x)} [(\\sin x)^2] $ = $2\\sin x$\n",
    "\n",
    "Derivative of the outer function with respect to sin of x (the inner function). times the derivative of sine of x with respect to x. \n",
    "\n",
    "this would return\n",
    "\n",
    "$ \\frac{d}{dx)} (\\sin x)  = \\cos x$\n",
    "\n",
    "so $ h^\\prime(x) = \\frac{d}{dx} = 2\\sin x \\cos x$\n",
    "\n",
    "Derivative of the outer function with respect to the inner, multiplied times the inner function with respect to x.\n",
    "\n",
    "OR, can think of it as,\n",
    "\n",
    "$ \\frac{d}{dx}[f(g(x))] = f^\\prime (g(x)) \\times g^\\prime (x) $\n",
    "\n",
    "\"F prime of G(x) times g of x\"\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "$ \\frac{d}{dx)} (\\sin x)  = \\cos x$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Common Misconceptions\n",
    "\n",
    "$ \\frac{d}{dx}[f(g(x))] $\n",
    "\n",
    "With the chain rule, when your dealing with transcendental functions is just a fancy word for these functions like trigonometric functions like $ \\sin x$ or logarithmic functions like $ \\ln x$ that don't use standard algebraic operations. But when you see transcendental functions like this or compositions of them, many people confuse this with the product of functions. \n",
    "\n",
    "this is a composition\n",
    "$ \\frac{d}{dx}[\\ln (\\sin(x)] $\n",
    "\n",
    "So to do this we need to take the derivative of the outer with respect to the inner.\n",
    "\n",
    "$ f^\\prime (g(x)) = \\frac{1}{\\sin x}$\n",
    "\n",
    "$g(x) = \\sin x$\n",
    "\n",
    "$f^\\prime (x) = 1 / x $\n",
    "\n",
    "$g^\\prime (x) = \\cos x $\n",
    "\n",
    "which gives us our \n",
    "\n",
    "$ = \\frac{1}{\\sin x} \\times \\cos x$\n",
    "\n",
    "$ = \\frac{\\cos x}{\\sin x} $\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Question**\n",
    "\n",
    "Is $h(x)=\\cos^2(x)$ a composite function? If so, what are the \"inner\" and \"outer\" functions?"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Answer**\n",
    "\n",
    "Yes, $h(x)$ is composite. The \"inner\" function is $x^2$ and the \"outer\" function is $\\cos(x)$"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Gradient Math\n",
    "\n",
    "The gradient points in the direction of the deepest descent.\n",
    "\n",
    "#### https://www.khanacademy.org/math/multivariable-calculus/multivariable-derivatives/gradient-and-directional-derivatives/v/gradient-and-graphs\n",
    "\n",
    "$ f(x, y) = x^2 + y^2 $\n",
    "\n",
    "$ \\nabla f(x, y) = \\left[\n",
    "\\begin{array}{c}\n",
    "    \\frac{\\partial f}{\\partial x}\\\\\n",
    "    \\frac{\\partial f}{\\partial y}\n",
    "\\end{array}\n",
    "\\right]$\n",
    "\n",
    "$ = \\left[\n",
    "\\begin{array}{c}\n",
    "    2x\\\\\n",
    "    2y\n",
    "\\end{array}\n",
    "\\right]$\n",
    "\n",
    "What does the length mean?\n",
    "\n",
    "Whats the contour map?\n",
    "If a vector is crossing that contour line, its always perpendicular. Gradient is always perpendicular to contour lines. \n",
    "**The Directional Derivative**\n",
    "\n",
    "The partial derivative can be thought of as taking a look at the input space (x, y plane). You know that somehow this outputs to a line. Whether its a transformation, or maybe just a from x, y plane to a line.\n",
    "\n",
    "But when you take the partial derivative of f with respect to x, at a point like 1, 2\n",
    "\n",
    "The directional derivative is saying when you take a slight nudge in the direction of that vector, what is the resulting change to the output?\n",
    "\n",
    "$ \\vec{V} = \\left[\n",
    "\\begin{array}{c}\n",
    "    -1\\\\\n",
    "    2\n",
    "\\end{array}\n",
    "\\right]$\n",
    "\n",
    "$h \\vec{V} = \\left[\n",
    "\\begin{array}{c}\n",
    "    -h\\\\\n",
    "    2h\n",
    "\\end{array}\n",
    "\\right]$\n",
    "\n",
    "So, its kind of like you took negative one nudge in the x-direction, and then two nudges in the y-direction. You know, so for whatever your nudge in the v-direction, there, you take a negative one step by x, and then two of them up by y. \n",
    "\n",
    "**Notation**\n",
    "\n",
    "$\\nabla_{\\vec{v}} \\ f(x, y) = -\\frac{\\partial f}{\\partial x} + 2 \\frac{\\partial f}{\\partial y}$\n",
    "\n",
    "**Abstract**\n",
    "\n",
    "$ \\vec{w} = \\left[\n",
    "\\begin{array}{c}\n",
    "    a\\\\\n",
    "    b\n",
    "\\end{array}\n",
    "\\right]$\n",
    "\n",
    "\n",
    "$\\nabla_{\\vec{w}} \\ f(x, y) = a \\frac{\\partial f}{\\partial x} + b \\frac{\\partial f}{\\partial y}$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": true
   },
   "source": [
    "**Continued**\n",
    "\n",
    "If you take the dot product of the vectors, a, b, and the one that has the partial derivatives in it. So,\n",
    "\n",
    "$ \\left[\n",
    "\\begin{array}{c}\n",
    "    a\\\\\n",
    "    b\n",
    "\\end{array}\n",
    "\\right] \\cdot \\left[\n",
    "\\begin{array}{c}\n",
    "    \\frac{\\partial f}{\\partial x}\\\\\n",
    "    \\frac{\\partial f}{\\partial y}\n",
    "\\end{array}\n",
    "\\right]$\n",
    "\n",
    "so $a,b$ is just equivalent to $\\vec{w}$\n",
    "and the partial vector is just the gradient $\\nabla{f}$\n",
    "\n",
    "thus, the directional derivative can be found by\n",
    "$\\vec{w} \\cdot \\nabla{f}$\n",
    "\n",
    "you think of moving along that vector, by a tiny nudge, little value, multiplied by that vector, and saying \"How does that change the output and what's the ratio of the resulting change?\""
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Formal Definition: Directional Derivative\n",
    "\n",
    "This is the formal definition for the partial derivative of a two-variable function with respect to x, and we are going to build up to the formal definition of the directional derivative of that same function in the direction of some vector V, which will be some vector in the input space. \n",
    "\n",
    "F(1) $$ \\frac{\\partial f}{\\partial x} (a, b) = \\lim_{h \\to 0} \\frac{f(a + h, b) - f(a, b)}{h} $$\n",
    "\n",
    "F(2)\n",
    "$$ \\nabla_{\\vec{v}}f = ??$$\n",
    "\n",
    "F(3)\n",
    "$$ \\hat{i} = \\left[\n",
    "\\begin{array}{c}\n",
    "    1\\\\\n",
    "    0\n",
    "\\end{array}\n",
    "\\right]$$\n",
    "\n",
    "The unit vector in the x direction\n",
    "\n",
    "The way to read this formal definition is that you think of this variable h, you think of it as that change in your input space.\n",
    "\n",
    "Imagine some input space on a x, y coordinate plane, and you think of it somehow mapping over to the real number plane, which is where your output F lives and when you're taking the partial derivative at a point A, B, and essentially you want to know for a little nudge in the x-direction what will be the resulting nudge on the output plane. You can think of the size of that nudge as $\\partial x$ and the size of the resulting nudge in the output space as $\\partial f$.\n",
    "\n",
    "H in F(1) can be thought of as that change in your input space (that slight nudge). And you look at how that influences the function when you only change the X component here, whats the resulting change in F? Whats that $\\partial f$\n",
    "\n",
    "So rewriting F(1) into vector notation. \n",
    "\n",
    "F(4) $$ \\frac{\\partial f}{\\partial x} (\\vec{a}) = \\lim_{h \\to 0} \\frac{f(\\vec{a} + h\\hat{i}, b) - f(\\vec{a})}{h} $$\n",
    "\n",
    "So the change we are looking for is going to be F, evaluated at that initial input vector $\\vec{a} + h$, that scaling value that little nudge, multiplied by the vector whose direction we care about $\\vec{v}$, then we subtract off the value of F at that original input $f(\\vec{a})$\n",
    "\n",
    "Thus the formal definition for the directional derivative, much easier to write in vector notation because you're thinking of your input as a vector and your output as just some nudge by something. \n",
    "\n",
    "F(5) $$ \\nabla_{\\vec{v}} f(\\vec{a}) = \\lim_{h \\to 0} \\frac{f(\\vec{a} + h\\vec{v}) - f(\\vec{a})}{h} $$\n",
    "\n",
    "So with F(3) when we multiply h by the unit vector it will only be \n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Why the gradient is the direction of steepest ascent?\n",
    "\n",
    "From the directional derivative you can tell the rate at which the function changes as you move in this direction by taking a directional derivative of your function, lets say a point, (a, b), When you evaluate this at a,b. Just dotting the gradient of f, with that vector. \n",
    "\n",
    "$ \\nabla_{\\vec{v}} f(a,b) = \\nabla f (a,b) \\cdot \\vec{v} $\n",
    "\n",
    "So this is how you tell the rate of change, and when we initially say the directional derivative, we saw about dotting the gradient with a vector. (we can think of the vector here as\n",
    "$\\left[\n",
    "\\begin{array}{c}\n",
    "    1\\\\\n",
    "    2\n",
    "\\end{array}\n",
    "\\right]$ one step in the x-direction and two steps in the y direction. So the amount that it changes should be one times the change caused by a pure step in the x direction, plus two times a change caused by a pure step in the y direction. \n",
    "\n",
    "This just gives us the key of how to find the direction of deepest descent, because now, what we are really asking, when we say which one of these changes things the most, we are saying find the maximum for all unit vectors, that satisfy the property whose length is one, find the maximum of the dot product of the gradient of f with vector v\n",
    "\n",
    "$ \\max_{ \\parallel \\vec{v} \\parallel} \\nabla f(a,b)$\n",
    "\n",
    "now lets think about what the dot product represents, is you would project that vector directly, kind of a prependicular projection onto your gradient vector, and you'd say whats that length? So then you are searching to find what unit vector maximizes the above condition. So the answer to what vector maximuzes this, is well, it's the gradient itself, right? \n",
    "\n",
    "Its the gradient itself but you'd normalize it because we're only considering unit vectors, so we just divide it by whatever its magnitude is $ \\nabla f(a,b) \\over \\parallel \\nabla f(a,b) \\parallel $\n",
    "\n",
    "The most notable thing here is that the gradient is this tool for computing directional derivatives, the gradient is something that loves to be dotted against, and as a consequence of that, **the direction of steepest ascent is that vector itself **because anything, if you're saying what maximizes the dot product with that thing, its, well, the vector that points in the same direction as that thing. And, this can also give us an interpretation for the length of the gradient. We know the direction of deepest ascent but what is the length mean? So let's give the denominator above a name - the normalized version, call it W. So W will be the unit vector that points in the direction of the gradient. If you take the directional derivative in the direction of W of f, what that means is the gradient of f \n",
    "\n",
    "$\\nabla_{\\vec{w}}f = \\nabla f \\cdot \\vec{w} = \\frac{\\nabla f \\cdot \\nabla f}{\\parallel \\nabla f \\parallel} = \\frac{\\parallel \\nabla f \\parallel ^2}{\\parallel \\nabla f \\parallel} = \\  \\parallel \\nabla f \\parallel $ \n",
    "\n",
    "that means you're taking the gradient of the vector dotted with itself, but because its w and not the gradient, we're normalizing it with the gradient of f, and you can think of the gradient of f evaluated at a,b. So when you take the dot product  with itself, its the square of its magnitude, but the whole thing is divided by the magnitude, so basically the directional derivative in the direction of the gradient itself has a value equal to the magnitude of the gradient. So this tells you when you're moving in that direction, in the direction of the gradient, the rate at which the function changes is given by the magnitude of the gradient so it's this really magical vector. And the magnitude tells you the rate at which things change while you're moving in that direction of steepest ascent. \n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Gradients in Deep Learning\n",
    "\n",
    "The Gradient is just a derivative generalized to functions with more than one variable. We can use Calculus to find the gradient at any point in our error function, which depends on the input weights. \n",
    "\n",
    "At each st\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "[Gradient][~/Documents/Data_Science/data_science/deep_learning/gradient.png]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Appendix:"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": true
   },
   "source": [
    "## MathJax\n",
    "\n",
    "Nabla $\\nabla$\n",
    "\n",
    "cdots $\\cdots$\n",
    "\n",
    "Big Right (and the direction) $\\Biggr($\n",
    "Big Left (and the direction) $\\Biggl)$\n",
    "\n",
    "Epsilon $\\epsilon$\n",
    "\n",
    "Left arrow $\\leftarrow$\n",
    "\n",
    "Sin $\\sin$\n",
    "\n",
    "Prod $\\prod$\n",
    "\n",
    "Sum $\\sum$\n",
    "\n",
    "alpha to omega $\\alpha \\beta \\omega$\n",
    "Gamma to Omega $\\Gamma \\Omega$\n",
    "\n",
    "Int $\\int$\n",
    "\n",
    "Infinity $\\infty$\n",
    "\n",
    "Square Root $\\sqrt{x} $\n",
    "\n",
    "Limit $\\lim_{x\\to 0}$\n",
    "\n",
    "Space $ a\\ b$\n",
    "\n",
    "hat $\\hat{y}$\n",
    "\n",
    "bar $\\bar{x}$\n",
    "\n",
    "vec $\\vec{x}$\n",
    "\n",
    "arrays $ \\nabla f(x, y) = \\left[\n",
    "\\begin{array}{c}\n",
    "    \\frac{\\partial f}{\\partial x}\\\\\n",
    "    \\frac{\\partial f}{\\partial y}\n",
    "\\end{array}\n",
    "\\right]$"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "from math import cos, sin, radians\n",
    "import numpy as np\n",
    "# derivative(cos, 1)\n",
    "# import \n",
    "\n",
    "def derivative(func, x, h = None):\n",
    "    if h is None:\n",
    "        # Note the hard coded value found here is the square root of the\n",
    "        # floating point precision, which can be found from the function\n",
    "        # call np.sqrt(np.finfo(float).eps).\n",
    "        h = 1.49011611938477e-08\n",
    "    xph = x + h\n",
    "    dx = xph - x\n",
    "    return (func(xph) - func(x)) / dx\n",
    "np.sqrt(np.finfo(float).eps"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
