{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Gradient Descent\n",
    "\n",
    "## Errors\n",
    "\n",
    "Recall that we want to find the weights for our neural networks. Let's start by thinking about the goal. The network needs to make predictions as close as possible to the real values. To measure this, we use a metric of how wrong th predictions are, the **error**. A common metric is the **sum of the squared errors** (SSE): \n",
    "    $$ E = \\frac{1}{2} \\sum_{\\mu} \\sum_{j} [y_{j}^{\\mu} - \\hat{y}_{j}^{\\mu}]^2 $$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "where $\\hat{y}$ is the prediction and $y$ is the true value, and you take the sum over all output units $j$, and another sum over all data points $\\mu$. \n",
    "\n",
    "First, inside sum over $j$. This variable $j$ represents the output units of the network. So this is inside sum is saying for each output unit, find the difference between the true value $y$ and the predicted value from the netwrok $\\hat{y}$, then square the difference, then sum up all those squares. \n",
    "\n",
    "Then the other sum over $\\mu$ is a sum over all the data points. So, for each data point you calculate the inner sum of the squared differences for each output unit. Then you sum up all those squared differernces for each data point. That gives you the overall error for all the output predictions for all the data points. \n",
    "\n",
    "The SSE is a good choice for a few reasons. The square ensures the error is always positive and larger errors are penalized more than smaller errors. Also, it makes the math nice, always a plus.\n",
    "\n",
    "Remember that the output of a neural network, the prediction, depends on the weights\n",
    "\n",
    "$$ \\hat{y}_{j}^{\\mu} = f  \\Biggl( \\sum_{i} w_{ij}x_i^\\mu \\Biggr)$$\n",
    "\n",
    "and accordingly the error depends on the weights\n",
    "\n",
    "$$ E = \\frac{1}{2} \\sum_{\\mu} \\sum_{j} \\Biggl[ y_{j}^{\\mu} - f  \\Biggl( \\sum_{i} w_{ij}x_i^\\mu \\Biggr) \\Biggr]^2 $$\n",
    "\n",
    "We want the network's prediction error to be as small as possible and the weights are the knobs we can use to make that happen. Our goal is to find weights $w_{ij}$ that minimize the squared error E. To do this with a neural network, typically you'd use **gradient descent**."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
