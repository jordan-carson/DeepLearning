{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Recurrent Neural Networks\n",
    "## Feedforward Refresher\n",
    "\n",
    "So far we have been looking at convolutional neural networks and models that allows us to analyze the spatial information in a given input image. CNN's excel in taks that rely on finding spatial and visible patterns in training data. \n",
    "\n",
    "RNNs on the other hand give us a way to incorporate **memory** into our neural networks, and will be critical when analyzing sequential data. RNN's are most associated with text processing and text generation because of the way sentences are structured as a sequence of words. \n",
    "\n",
    "So far the neural network architectures we've seen have been trained using the current inputs only. We did not consider previous inputs when generating the current output. In other words, our systems did not have any memory elements. RNNs address this very basic and important issue by using **memory** (past inputs to the network) when producing the current output. \n",
    "\n",
    "RNNs are artificial neural networks, that can capture temporal dependencies, which are dependencies over time. They are called Recurrent neural networks, because recurent means occuring often or repeatedly, and we perform the same task for each element in the input sequence. \n",
    "\n",
    "**Feedforward neural networks** Nonlinear function approximation, training using backpropagation or stochastic gradient decent, and evaluation. \n",
    "\n",
    "Simple RNN aka the Elman Network, and learn how to train the network, also understanding the limitations of simple RNNs and how they can be overcome by using LSTMs. \n",
    "\n",
    "### RNNs History\n",
    "**Elman and Jordan Networks**\n",
    "\n",
    "It was recognized in the early 90s that all of these networks suffer from what we call, the **vanishing gradient problem**, in which contributions of information decayed geometrically over time. So capturing relationships that spanned more than 8 or 10 steps back was practically impossible. \n",
    "\n",
    "What does this mean? When training our network we use **backpropagation**, which is the process we adjust our weight matricies with the use of a **gradient**. In the process, gradients are calculated by continuous multiplications of derivatives. The value of these derivatives may be so small, that these continuous multiplications may cause the gradient to practically \"vanish\". **LSTM** is one option to overcome this issue. [More Resources on Vanishing gradients](https://en.wikipedia.org/wiki/Vanishing_gradient_problem)\n",
    "\n",
    "In the mid 90s, Long Short-Term Memory cells (LSTM), were invented to address this very problem. The key novelty in LSTMs was the idea that some signals, what we call state variables, can be kept fixed by using gates, and reintroduced or not at an appropriate time in the future. In this way, arbitrary time intervals can be represented, and temporal dependencies can be captured. Variations on LSTMs such as Gated Recurrent Neworks, or GRUs for short - Gated Recurrent Units, further refined this theme and nowadays, represent another mainstream approach to realizing RNNs, This give a solution to the vanishing gradient, by helping us apply networks that have temporal dependencies. In this lesson we will focus on RNNs and continue with LSTMs. \n",
    "\n",
    "### Applications\n",
    "\n",
    "Predicting stock price movements based on historical patterns of stock movements and potentially other market conditions, that change over time. \n",
    "\n",
    "Other NLPs for chatbots etc.\n",
    "Gesture Recognition\n",
    "\n",
    "### Feedforward Neural Network Reminder\n",
    "\n",
    "We can have many hiddlen layers between the inputs and the outputs, for simplicity lets use a single hidden layer in a FFNN.\n",
    "\n",
    "With a good understanding of the FFNN, the step towards RNNs becomes simple. \n",
    "\n",
    "When implementing your neural net, you will find that we can combine these techniques, Conv Net with a Recurrent Network. \n",
    "\n",
    "![cnns_with_rnns](cnns_with_rnns.png)\n",
    "\n",
    "One can use CNNs in the first few layers for the purpose of feature extraction, and then use RNNs in the final layer where memory needs to be considered. A popular application for this is in gesture recognition. \n",
    "\n",
    "When working on a feedforward neural network, we actually simulate an artificial neural network by using a nonlinear function approximation. \n",
    "\n",
    "$$ \\hat{y} = F(\\bar{x}, W)$$\n",
    "\n",
    "That function will act as a system that has $n$ number of inputs, weights, and $k$ number of outputs. \n",
    "Using $\\bar{x}$ as the input vector and $\\bar{y}$ as the output. Inputs and outputs can also be many-to-many, many-to-one, and one-to-many. \n",
    "\n",
    "So, when neural network essentially works as a nonlinear function approximator, what we do is we try to fit a smooth function between given points like x1, y1, x2 y2, and so on, in such a way that when have a new input ${x}^\\prime$ we can find the new ouput $y^\\prime$.\n",
    "\n",
    "There are two main types of applications. One is classification, where we identify which of a set of categories a new input belongs to. For example, an image classification where the neural network receives as an input an image and can know if it's a cat. The other application is regression, where we approximate a function, so the network produces continuous values following a supervised training process. A simple example can be time series forecasting, where we predict the price of a stock tomorrow based on the price of the stock over the past five days. The input to the network would be five values representing the price of the stock for each of the past five days, and the output we want is tomorrow's price. \n",
    "\n",
    "Our task in neural networks is to find the best set of weights that yield a good output where x represents the inputs, and W represents the weights. We start with random weights. In feedforward neural networks, we have static mapping from the inputs to the outputs. We use the word static as we have no memory and the output depends only on the inputs and the weights. In other words, for the same input and the same weights, we always receive the same output. \n",
    "\n",
    "![neural_task](neural_network_task.png)\n",
    "\n",
    "There are two primary phases\n",
    "- Training\n",
    "- Evaluation\n",
    "\n",
    "In the training phase, we take the dataset called the training set which includes many pairs of inputs and their corresponding targets or outputs. And the goal is to find a set of weights that would best map the inputs to the desired outputs. \n",
    "\n",
    "In other words, the goal of the training phase is to yield a network that generalizes beyond the train set. \n",
    "\n",
    "In the evaluation phase we use the network that was created in the training phase, apply our new inputs, and expect to obtain the desired outputs. \n",
    "\n",
    "Let's now look at a basic model of an artificial neural network, where we have only a single, hidden layer. The inputs are each connected to the neurons in the hidden layer and the neurons in the hidden layer are each connected to the neurons in the output layer where each neuron there represents a single output. We can look at it as a collection of mathematical functions. Each input is connected mathematically to a hidden layer, of neurons through a set of weights we need to modify, and each hidden layer neuron is connected to the output layer in a similar way. There is no limit to the number of inputs, number of hidden neurons in a layer, and number of ouputs, nor are there any correlations between those numbers, so we can have $n$ inputs, $m$ hidden neurons, and $k$ outputs. \n",
    "\n",
    "$$h_i = F(x_i, W_{ij}^1)$$\n",
    "\n",
    "$$y_i = F(h_i, W_{ij}^2)$$\n",
    "\n",
    "![network_output_map](network_function_map.png)\n",
    "\n",
    "We can see that each input is multiplied by its corresponding weight and added at the next layer's neuron with a bias as well. The bias is an external parameter of the neuron and can be modeled by adding an external fixed value input. This entire summation will usually go through an activation function to the next layer or to the output. \n",
    "\n",
    "**Whats our goal?**\n",
    "We can look at our system as a black box that has $n$ inputs and $k$ outputs. Our goal is to design the system in such a way that it will give us the correct output $y$ for a specific input $x$. Our job is to decide what's inside this black box. We knw that we will use artificial neural networks and need to train it to eventually have a system that will yield the correct output to a specific input. Essentially, what we really want is to find the optimal set of weights connecting the input to the hidden layer, and the optimal set of weights connecting the hidden layer to the output. We will never have a perfect estimation, but we can try to be as close to it as we can. To do that, we need to start a process you're already familiar with, and that is the training phase. So let's look at the training phase where we will find the best set of weights for our system.\n",
    "\n",
    "This phase will include two steps, feedforward and backpropagation, which we will repeat as many times as we need until we decide that our system is as best as it can be. In the feedforward part, we will calculate the output of the system. \n",
    "\n",
    "The output will be compared to the correct output giving us an indication of the error. There are a few ways to determine the error. \n",
    "\n",
    "In the backpropagation part, we will change the weights as we try to minimize the error, and start the feedforward process all over again. \n",
    "\n",
    "### Calculating the value of the hidden states\n",
    "\n",
    "We will look at the feedforward network, to make our computations easier, let's decide to have $n$ inputs, three neurons in a single hidden layer, and two outputs. In practice, we can have thousands of neurons in a single hidden layer. We will use $W_1$ as the set of weights from $x$ to $h$, and $W_2$ as the set of weights from $h$ to $y$.\n",
    "\n",
    "Since we have only one hidden layer, we will habe only two steps in each feedforward cycle. \n",
    "\n",
    "![feedforward_cycle](feedforward_cycle.png)\n",
    "\n",
    "- Step 1: We'll be finding $h$ from a given input and a set of weights $W_1$\n",
    "- Step 2: we'll be finding the output $y$ from the calculated $h$ and the set of weights $W_2$.\n",
    "\n",
    "You will find that other than the use of non-linear activation functions, all of the calculations involve linear combinations of inputs and weights. Or in other words, we will use matrix multiplications. \n",
    "\n",
    "Starting with **Step 1**. Finding $h$, \n",
    "\n",
    "Notice that if we have more than one neuron in the hidden layer, which is usually the case, h is actually a vector. We will have out initial inputs x, x is also a vector, and we want to find the values of the hidden neurons, h. Each input is connected to each neuron in the hidden layer. For simplicity, we will use the following indices: $W_{11}$ connects $x_1$ to $h_1$, and $W_{21}$ connects $x_2$ to $h_1$\n",
    "and so on..\n",
    "![w1](w1_math.png)\n",
    "\n",
    "The vector on the inputs $x_1, \\cdots , x_n$ is multiplied by the weight matrix $W_1$ to give us the hidden neurons $h$. So each vector $h$ equals vector $x$ multiplied by the weight matrix $W_1$. \n",
    "\n",
    "$$\\bar{h}^\\prime = \\bar{x} \\cdot W_1$$\n",
    "\n",
    "In this case we have a weight matrix with $n$ rows, as we have $n$ inputs, and three columns, as we have three neurons in the hidden layer. If you multiply the input vector by the weight matrix, you will have a simple linear combination for each neuron in the hidden layer giving us vector, h.\n",
    "\n",
    "_Equation 1_\n",
    "$$ \\left[\n",
    "    \\begin{array}\n",
    "       hh_{1}^\\prime & h_2^\\prime & h_3^\\prime\n",
    "    \\end{array} \\right]\n",
    " = \\left[\n",
    "     \\begin{array}\n",
    "       xx_1 & x_2 & \\cdots & x_n\n",
    "    \\end{array} \\right]\n",
    "\\cdot\n",
    "\\left[\n",
    "     \\begin{matrix}\n",
    "       W_{11} & W_{12} & W_{13} \\\\\n",
    "       W_{21} & W_{22} & W_{23} \\\\\n",
    "       W_{31} & W_{32} & W_{33} \\\\\n",
    "       \\vdots & \\vdots & \\vdots \\\\\n",
    "       W_{n1} & W_{n2} & W_{n3} \\\\\n",
    "    \\end{matrix} \\right]\n",
    "$$\n",
    "\n",
    "So for example..\n",
    "\n",
    "But we are not done with calculating the hidden layer yet. Notice the prime symbol we have been using? We are using it to remind us that we are not done with finding h yet. \n",
    "\n",
    "$$\n",
    "    h_1^\\prime = x_1 \\cdot W_{11} + x_2 \\cdot W_{21} + \\cdots x_n \\cdot W_{n1}\n",
    "$$\n",
    "\n",
    "To make sure that the values of h do not explore or increase to much in size, we need to use an activation function usually denoted by the Greek letter, phi. Next, we need to apply our **activation function**, we can use a hyperbolic tangent. Using this function will ensure that our outputs are between one and negative one. We can also use a sigmoid, using this function will ensure that our outputs are between one and zero. We can also use a rectified linear unit or ReLU. Where the negative values are nulled and the positive values remain as they are. Each activation function has its advantages and disadvantages. \n",
    "\n",
    "What they all share is that they allow the network to represent nonlinear relationships between its inputs and outputs. And this is very important since most real world data is nonlinear. \n",
    "\n",
    "Mathematically, the linear combination and activation function can simply be written as h, equals to the ouput of an activation function of the input vector multiplied by the corresponding weight matrix. \n",
    "\n",
    "$$ \\bar{h} = \\phi \\left( \\bar{x} \\cdot W_1 \\right)$$\n",
    "\n",
    "Using these functions can be a bit tricky because they contribute to the vanishing gradient problem that we mentioned before. \n",
    "\n",
    "\n",
    "Starting with **Step 2** finding $\\bar{y}$, finding the output.\n",
    "\n",
    "Using the values of $\\bar{h}$ we just calculated. Since we have more than one output $y$ will be a vector as well. We have our initial inputs h, and want to find the values of the output y. Mathematically, the idea is identical to what we just saw in step number one. We now have different inputs, we call them $h$ and a different weight matrix, we call it $W_2$.\n",
    "\n",
    "$$\\bar{h} = \n",
    "\\left[\n",
    "\\begin{array}\n",
    "     hh_1 & h_2 & h_3 \\\\\n",
    "\\end{array}     \\right]\n",
    "$$\n",
    "\n",
    "$$W_2 = \n",
    "\\left[\n",
    "\\begin{matrix}\n",
    "    w_{11} & w_{12} \\\\\n",
    "    w_{21} & w_{22} \\\\\n",
    "    w_{31} & w_{32} \\\\\n",
    "\\end{matrix}     \\right]\n",
    "$$\n",
    "\n",
    "The output will be vector $y$. \n",
    "\n",
    "$$\\bar{y} = \n",
    "\\left[\n",
    "\\begin{array}\n",
    "     yy_1 & y_2 \\\\\n",
    "\\end{array}     \\right]\n",
    "$$\n",
    "\n",
    "Notice that the weight matrix has three rows, as we have three neurons in the hidden layer, and two columns as we have only two outputs. And again, we have a vector by matrix multiplication. Vector $h$, multiplied by the weight matrix $W_2$. gives us the output vector y. We can put it in a simple equation, where y equals h times W. \n",
    "\n",
    "$$\n",
    "\\left[\n",
    "\\begin{array}\n",
    "     yy_1 & y_2 \\\\\n",
    "\\end{array}     \\right]\n",
    "=\n",
    "\\left[\n",
    "\\begin{array}\n",
    "     hh_1 & h_2 & h_3 \\\\\n",
    "\\end{array}     \\right]\n",
    "\\cdot\n",
    "\\left[\n",
    "\\begin{matrix}\n",
    "    w_{11} & w_{12} \\\\\n",
    "    w_{21} & w_{22} \\\\\n",
    "    w_{31} & w_{32} \\\\\n",
    "\\end{matrix}     \\right]\n",
    "$$\n",
    "\n",
    "Once we have the outputs, we don't necessarily need an activation function. In some applications, it can be beneficial to use for example, a softmax function, what we call sigma x, if we want the output values to be between zero and one. \n",
    "\n",
    "To have a good approximation of the output y, we need more than one level of hidden layers. Maybe even 10 or more. The number of neurons in each layer can change from one layer to the next, which can be even thousands. We can think of these neurons as building blocks or lego pieces that can be stacked. So how is this done mathematically?\n",
    "\n",
    "Just as before. A simple vector by matrix multiplication, followed by an activation function, where the vector indicated the inputs, and the matrix indicates the weights connecting one layer to the next. To generalize this model, let's look into one random KF layer. The weight $W_{ij}$ from the K F layer to the K +1, corresponds to the I F input, going into the J F output. \n",
    "\n",
    "![generating_the_output](generating_the_output.png)\n",
    "\n",
    "\n",
    "At 5:10\n",
    "\n",
    "In the training phase, we actually know the output of a given input. We calculate the output of the system in order to adjust the weights. We do that by finding the error, and trying to minimize it. Each iteration of the training phase will decrease the error just a bit, until we eventually decide that the error is small enough. Let's focus on an intuitive error calculation, which is simply finding the difference between the calculated, and the desired output. \n",
    "\n",
    "$$ \\bar{e} = \\left( \\bar{d} - \\bar{y} \\right) $$\n",
    "\n",
    "For our backpropagation calculations we will use the squarred error, also called the loss function\n",
    "\n",
    "$$ \\bar{e} = \\left( \\bar{d} - \\bar{y} \\right)^2 $$\n",
    "\n",
    "So to Summarize, the process of calculating the output vector is mathematically similar to that of calculating the vector of the hidden layer. We use, again, a vector by matrix multiplication, which can be followed by an activation function. The vector is the newly calculated hidden layer and the matrix is the one connecting the hidden layer to the output.\n",
    "\n",
    "Essentially, each new layer in an neural network is calculated by a vector by matrix multiplication, where the vector represents the inputs to the new layer and the matrix is the one connecting these new inputs to the next layer. \n",
    "\n",
    "In our example, the input vector is $\\bar{h}$ and the matrix is $W^2$, therefore $\\bar{y} = \\bar{h} W^2$. In some applications it can be beneficial to use a softmax function (if we want all output values to be between zero an 1, and their sum to be 1). \n",
    "\n",
    "The two error functions that are most commonly used are the MSE, usually in regression problems, and the cross entropy used in classification problems. \n",
    "\n",
    "## Quiz: What is the total number of multiplication operations needed for a single feedforward pass?\n",
    "\n",
    "if the first hidden layer has M neurons, and the second has N neurons then the number of operations equals\n",
    "\n",
    "$$ M+N+MN$$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": true
   },
   "source": [
    "# Backpropagation Process\n",
    "\n",
    "In this process we minimize the network error slighly with each iteration, by adjusting the weights. \n",
    "\n",
    "Now that we calculated the feedforward pass, received an output and calculated the error, we are ready to go backwards in order to change our weights with a goal of decreasing the network error. Going backwards from the output to the input while changing the weights, is what we call **back propagation**, which is essentially stochastic gradient descent computed using the chain rule. To be able to implement a basic neural network, we don't need a deep mathematical understanding as we have open source tools. But to really understand how it works and to optimize our application, its always important to know the math.\n",
    "\n",
    "Our goal is to fine a set of weights that minimizes the network error. So how do we do that? We use an iterative process presenting the network with one input at a time from our training set. During the feedforward pass for each input, we calculate the networks error. We can then use this error to slightly change the weights in the correct direction, each time reducing the error by just a bit. We continue to do so until we determine that the error is small enough. How small is small enough? How do we know if we have a good enough mapping from the inputs to the outputs? There is no simple answer to that. We will find practical solutions to some of these questions in our next section on over-fitting. \n",
    "\n",
    "Imagine a network with only one weight, $W$. Assume that during a certain point in the training process, the weight has a value of $W_a$ and the error at point A is $E_a$. To reduce the error, we need to increase the value of the weight, $W_a$.\n",
    "\n",
    "![gradient calc](gradient_calculation.png)\n",
    "\n",
    "Since the gradient, or in other words, the derivative which is the slope of the curve at point $A$ is negative, since its pointing down, we need to change the weight in its negative direction so that we actually increase the value of $W_a$. If, on the other hand, the weight has a value of $W_b$ with a network error being $E_b$, to reduce the error we need to decrease the weight of $W_b$. if you look at the gradient and point B, you'll see that its positive. In this case, changing the weight by taking a step in the negative direction of the gradient would mean that we are correctly decreasing the value of the weight. \n",
    "\n",
    "![gradclc2](gradient_calc2.png)\n",
    "\n",
    "The case we looked at was of a single weight which was oversimplifying the more practical case where the neural network has many weights. \n",
    "\n",
    "$$ W_{new} = W_{previous} + \\alpha \\left( - \\frac{\\partial E}{\\partial W} \\right) $$\n",
    "\n",
    "We can summarize the weight update process using this equation where alpha is the learning rate or the step size. We can also see that the weight, $W$, for the reasons I just mentioned, changes in the opposite direction of the partial derivative of the error with respect to $W$. You may ask yourselves, \"why are we looking at partial derivatives?\" and the answer is simply because the error is a function of many variables and the partial derivative let's us measure how the error is impacted by each weight separately. \n",
    "\n",
    "Since many weights determined the networks output, we can use a vector of partial derivatives of the network error, each with respect to a different weight. That strange inverted triangular symbol, is the gradient. \n",
    "\n",
    "$$ W_{new} = W_{previous} + \\alpha \\nabla_{W} \\left( - E \\right) $$\n",
    "\n",
    "This gradient is the vector of partial derivatives of the error with respect to each of the weights. For the purpose of proper notation, we will have quite a few indices here. In this illustration which should be familar by now, we are focusing on the connections between layer $k$ and layer $k+1$. The weight \n",
    "The weight $W_{ij}$.\n",
    "\n",
    "![backprop](backprop.png)\n",
    "\n",
    "$W_{ij}$ connects neuron $i$ in layer $k$ to neuron $J$ in layer $k+1$, just as we've seen before. Let's call the amount by which we change or update weight, $W_{ij}$, $\\nabla W_{ij}$.\n",
    "\n",
    "**The weight update**.\n",
    "\n",
    "$$ \\nabla W_{ij}^k$$\n",
    "\n",
    "The superscript $k$, indicates that the weight connects layer $k$, to layer $k+1$. Or, in other words, it originated from layer $k$. Calculating this delta weight of $W_{ij}$ is straightforward. \n",
    "\n",
    "$$ \\nabla W_{ij}^k = -\\alpha \\frac{\\partial E}{\\partial W_{ij}^k}$$ \n",
    "\n",
    "It equals to the learning rate, multiplied by the partial derivative of the error with respect to weight, $W_{ij}$ in each layer. We will take the negative of that term for the reasons I just mentioned before.\n",
    "\n",
    "Basically, back propagation boils down to calculating the partial derivative of the error, $E$, with respect to each of the weights, and then adjusting the weights according to the calculated value of delta of $W_{ij}$. \n",
    "\n",
    "$$ W_{new} = W_{previous} + \\nabla W_{ij}^k $$\n",
    "\n",
    "These calculations are done for each layer. Let's look at our system one more time. For the error, we will use the loss function which is simply \n",
    "\n",
    "$$ W = \\frac{\\left( \\bar{d} - \\bar{y} \\right)^2}{2}$$\n",
    "\n",
    "$\\bar{d}$ = the desired output\n",
    "\n",
    "$\\bar{y}$ = the calculated output\n",
    "\n",
    "The desired output minus the network output, all that squared. Here, we're also dividing this error term by two for calculation simplicity that we will see later. \n",
    "\n",
    "### Additional Notes\n",
    "\n",
    "If we look at an arbitrary layer $k$, we can define the amount by which we change the weights from neuron $i$ to neuron $j$ stemming from layer $k$ as: $\\nabla W_{ij}^k$.\n",
    "\n",
    "The superscript ($k$) indicates that the weight connects layer $k$ to layer $k+1$. Therefore the weight update rule for that neuron can be expressed as:\n",
    "\n",
    "**Equation 4**\n",
    "$$ W_{new} = W_{previous} + \\nabla W_{ij}^k $$\n",
    "\n",
    "The updated value $\\nabla W_{ij}^k$ is calculated through the use of the gradient calculation in the following way:\n",
    "\n",
    "**Equation 5**\n",
    "$\\nabla W_{ij}^k = \\alpha \\left( - \\frac{\\partial E}{\\partial W} \\right)$, where $\\alpha$ is a small positive number called the **Learning Rate**. \n",
    "\n",
    "From these derivation we can easily see that the weight updates are calculated by the following equation:\n",
    "\n",
    "**Equation 6**\n",
    "$$ W_{new} = W_{previous} +\\alpha \\left( - \\frac{\\partial E}{\\partial W} \\right)$$\n",
    "\n",
    "Since many weights determine the network's output, we can use a vector of the partial derivatives (defined by the greek letter nabla $\\nabla$) of the network error - each with respect to a different weight. \n",
    "\n",
    "**Equation 7**\n",
    "$$ W_{new} = W_{previous} +\\alpha \\nabla_{w} \\left( -E \\right) $$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": true
   },
   "source": [
    "# Overfitting\n",
    "\n",
    "When we minimize the network error using backpropagation, we may either properly fit the model to the data or overfit. Generally speaking, when we have a finite training set there's a risk of overfitting. Overfitting means that our model with fit the training data too closely. In other words, we over trained the model or the network to fit our data. As a result, we unintentionally also model the noise or random elements of the training set. If that happens, our model will not generalize well when tested on new inputs. \n",
    "\n",
    "There are generally two main approached to addressing the overfitting problem. The first is to stop the training process early, and the second is the use of regularization. \n",
    "\n",
    "When we stop the training process early, we do that in the region where the network begins to overfit. By doing so, we reduce degradation in the performance on the test set. It would be ideal if we knew precisely when we should stop the training process. However that is often difficult to determine. One way of determining when to stop the training is by carving a small data set out of the training set which we will call the **validation set**. Assuming that the accuracy of the validation set is similar to that of the test set, we can use it to estimate when the training should stop. The drawback of this approach is that we end up with fewer samples to train out model on, so our training set is smaller. An alternative mainstream approach to mitigating overfitting is to use regularization.\n",
    "\n",
    "Regularization means that we impose a constraint on the training of the network such that better generalization can be acheived. Dropout is widely used regularization scheme which helps in that manner. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Backpropagation - Example\n",
    "\n",
    "\n",
    "In our feedforward illustration, we had $n$ inputs, 3 neurons in the hidden layer, and two outputs. For this example, we will need to simplify things even more and look at a model with two inputs, $x_1$ and $x_2$, and a single output, $y$. We will have a weight matrix $W_{ij}^1$ from the input to the hidden layer, and the elements of the matrix will be $W_{ij}$ just as before. We will also have a weight matrix vector, $W_2$, from the hidden layer to the output. Notice that it's a vector and not a matrix, as we only have one output. \n",
    "\n",
    "![backprop_ex](backprop_ex.png)\n",
    "\n",
    "We begin with a feedforward pass of the inputs across the network, then calculate the output, and based on the error, use backpropagation to calculate the partial derivatives. \n",
    "\n",
    "Gradient\n",
    "$$ \\nabla_{w_{ij}^1} \\left( -E \\right) $$\n",
    "\n",
    "Calculating the values of the activations at each of the three hidden neurons is simple. We have a linear combination of the inputs with the corresponding weight elements of the matrix $W_1$. \n",
    "\n",
    "$$h_1 = \\phi \\left( \\sum_i^2 x_i W_{i1}^1 \\right)$$\n",
    "\n",
    "The same calculation is applied to get the output of each hidden layer, using a different weight matrix, but mathematically the same.\n",
    "\n",
    "All of that is followed by an activation function. The outputs are a dot product of the activations of the previous layer vector, $h$, with the weights of $W_2$. \n",
    "\n",
    "$$ h = \\left[\n",
    "\\begin{array}\n",
    "     hh_1 , h_2 , h_3 \\\\\n",
    "\\end{array} \\right] $$\n",
    "\n",
    "$$ y = \\sum_i h_i W_i^2$$\n",
    "\n",
    "As we mentioned before, the aim of the back propagation process is to minimize, in our case, the loss function or the square error. To do that, we need to calculate the partial derivatives of the square error with respect to each of the weights. Since we just found the output, we can now minimize the error by finding the updated values of $\\nabla W_{ij}^k$ and of course, in every case, we need to do so for every single layer $k$. \n",
    "\n",
    "$$ \\nabla W_{ij} = - \\alpha \\frac{\\partial E}{\\partial W_{ij}} = - \\alpha \\frac{\\partial \\frac{(d-y)^2}{2}}{\\partial W_{ij}}$$\n",
    "\n",
    "This value equals to the negative of alpha multiplied by the partial derivative of the loss function $e$. Since the error is a polynomial, finding its derivative is immediate. By using basic calculus, we can seek that this incremental value is simply the learning rate alpha multiplied by $d$ minus $y$, and by the partial derivative of $y$ with respect to each of the weights. \n",
    "\n",
    "$$ \\nabla W_{ij} = \\alpha \\left(d - y \\right) \\frac{\\partial y}{\\partial W_{ij}} $$\n",
    "\n",
    "what happened to d? that was constant so the derivative was 0. We used the chain rule here. Now that we have our gradient. \n",
    "\n",
    "We initially mentioned that backpropagation is actually stochastic gradient descent with the use of the chain rule. Well, now we have our gradient. The symbol that we use for the gradient is usually lowercase delta. \n",
    "\n",
    "$$ \\delta = \\frac{\\partial y}{\\partial W_{ij}} $$\n",
    "\n",
    "This partial derivative of the calculated output defines the gradient and we will find it by using the chain rule. \n",
    "\n",
    "### Refresher on the Chain Rule\n",
    "\n",
    "The chain rule says, if you have a variable $x$ on a fucntion $f$ that you apply to $x$ to get $f(x)$, which we're gonna call $A$. \n",
    "\n",
    "$$ A = f(x) $$\n",
    "\n",
    "and then another function $g$, which you apply to $f(x)$ to get $g \\circ f(x)$, which we're gonna call $B$. \n",
    "\n",
    "$$ B = g \\circ f(x) $$\n",
    "\n",
    "If you want to find the partial derivative of $B$ with respect to $x$, thats just a partial derivative of $B$ with respect to $A$ times the partial derivative of $A$ with respect to $x$. \n",
    "\n",
    "$$ \\frac{\\partial B}{\\partial x} = \\frac{\\partial B}{\\partial A} \\frac{\\partial A}{\\partial x} $$\n",
    "\n",
    "It says, when composing functions, that derivatives just multiply, and this is going to be helpful to us because feed forwarding is literally composing a bunch of functions, and back propagation is literally taking the derivative at each piece, and since taking the derivative of a composition is the same as multiplying the partial derivatives, then all we're gonna do is multiply a bunch of partial derivatives to get what we want. \n",
    "\n",
    "\n",
    "## Backpropagation Example - Part B\n",
    "\n",
    "We now need to calculate the gradient in the backpropagation step. Which we will do one step at a time. In our example, we only have one hidden layer. So the backpropagation process will have two steps, but let's be more precise now and decide that the gradient calculated for each element $ij$ in the matrix is called $\\delta_{ij}$.\n",
    "\n",
    "In step number one, we will calculate the gradient with respect to the weight vector $W2$ from the output to the hidden layer. \n",
    "\n",
    "And in step two, we will calculate the gradient with respect to the weight matrix $W1$ from the hidden layer to the input. \n",
    "\n",
    "$\\nabla_{i}^2(-E)$\n",
    "\n",
    "So let's start with step number one. We already calculated $y$, \n",
    "\n",
    "$$ y = \\sum_{i}^3 h_i W_{i}^2 $$\n",
    "\n",
    "**Find**\n",
    "$$\\frac{\\partial y}{\\partial W_i^2}$$\n",
    "\n",
    "We will find its partial derivative with respect to the weight vector $W_2$. given that $y$ is a linear summation over terms, you will find that the gradient is simply the value of the corresponding activation $h$, as all the other terms were zero. \n",
    "\n",
    "$$ \\frac{\\partial y}{\\partial W_i^2} = \\frac{ \\partial \\left( \\sum_i^3 h_i W_i^2 \\right)}{\\partial W_i^2} = h_i $$\n",
    "\n",
    "This will hold for gradient one, gradient two, and gradient three. \n",
    "\n",
    "![gradient_pt1](gradient_pt1.png)\n",
    "\n",
    "You probably noticed that we have only one index for delta here and that's because we have a single output. Previously, we saw that the incremental value delta of $W_{ij}$ equals to the learning rate alpha multiplied by $d - y$ and multiplied by the gradient. So at the second layer, $\\delta_i = h_i$, the incremental value of delta of $W_i$ equals alpha multipled by $d - y$ and by $h_i$. \n",
    "\n",
    "$$ \\nabla W_i = \\alpha \\left( d - y \\right) \\delta_i $$\n",
    "\n",
    "In our second step, we want to update the weights of layer one by calculating the partial derivative of $y$ with respect to the weight matrix $W_1$. \n",
    "\n",
    "**Find**\n",
    "$$\\frac{\\partial y}{\\partial W_{ij}^1}$$\n",
    "\n",
    "Here is where things get a little more interesting. When calculating the gradient, with respect to the weight matrix $W_1$, we need to use the chain rule. The approach, obtain the partial derivative of $y$ with respect to $h$, and multiply it by the partial derivative of $h$ with respect to the corresponding elements in $W_1$. In this example, we only have three neurons in the single hidden layer. Therefore, this will be a linear combination of three elements.  \n",
    "\n",
    "$$\\frac{\\partial y}{\\partial W_{ij}^1} = \\sum_{p=1}^N \\frac{\\partial y}{\\partial h_p} \\frac{\\partial h_p}{\\partial W_{ij}^1}$$\n",
    "\n",
    "Let's calculate each derivative separately. \n",
    "\n",
    "$$ \\frac{\\partial y}{\\partial h_j} = \\frac{\\partial \\sum_{i=1}^3 \\left( h_i W_i^2 \\right)}{\\partial h_j} = W_j^2$$\n",
    "\n",
    "since $y$ is a linear combination of $h$ and its corresponding weights, its partial derivative with respect to $h$ will be the weights elements a vector $W_2$. \n",
    "\n",
    "Now, what is the partial derivative of each element of vector $h$ with respect to its corresponding weights in matrix $W_1$? We've already considered each element of vector $h$ separately. \n",
    "\n",
    "_Calculating_\n",
    "$\\frac{\\partial h_i}{\\partial W_{ij}^1}$ \n",
    "\n",
    "recall $h_1$ equals \n",
    "\n",
    "$$h_1 = \\phi \\left( \\sum_i^2 x_i W_{i1}^1 \\right) $$\n",
    "\n",
    "We also have $h_2$ and $h_3$, which is\n",
    "\n",
    "$$h_2 = \\phi \\left( \\sum_i^2 x_i W_{i2}^1 \\right) $$\n",
    "\n",
    "$$h_3 = \\phi \\left( \\sum_i^2 x_i W_{i3}^1 \\right) $$\n",
    "\n",
    "If we generalize this, each element $j$ is an activation function of a corresponding linear combination. \n",
    "\n",
    "$$h_j = \\phi \\left( \\sum_i^2 x_i W_{ij}^1 \\right) $$\n",
    "\n",
    "$$ \\frac{\\partial h_j}{\\partial W_{ij}^1} = \\frac{\\partial \\Phi_j \\left( \\sum_{i=1}^2 \\left(x_i W_{ij}^1 \\right) \\right)}{\\partial \\left( \\sum_{i=1}^2 \\left( x_i W_{ij}^1 \\right) \\right)}\n",
    "\\frac{\\partial \\left( \\sum_{i=1}^2 \\left( x_i W_{ij}^1 \\right) \\right)}{\\partial W_{ij}^1}$$\n",
    "\n",
    "\n",
    "Finding its partial derivative means finding the partial derivative of the activation function and multiplying it by the partial derivative of the linear combination. All, of course, weith respect to the correct elements of the weight $W_1$. \n",
    "\n",
    "![gradient_pt3](gradient_pt3.png)\n",
    "\n",
    "As we said before, there are various activation function. So let's just call the partial derivative of the activation function $\\phi^\\prime$. Each neuron will have its own value of $\\phi$ and $\\phi^\\prime$, according to the activation function you use. The partial derivative of the linear combination with respect to $W_{ij}$ is simply $x_i$ since all of the other components are zero. So the partial derivative of $h$ with respect to the weight matrix $W_1$ is simply $\\phi^\\prime$ at neuron $j$ multiplied by $x_i$. \n",
    "\n",
    "$$ \\frac{\\partial h_j}{\\partial W_{ij}^1} = \\Phi_{j}^\\prime x_i$$\n",
    "\n",
    "We now have all the pieces required for step number two, giving us the gradient $\\delta_{ij}$\n",
    "\n",
    "$$ \\frac{\\partial h_j}{\\partial W_{ij}^1} = \\sum_{k=1}^N \\frac {\\partial y}{\\partial h_k} \\frac{\\partial h_k}{\\partial W_{ij}^1} $$\n",
    "\n",
    "We know that the gradient of the output $y$ with respect to each of the elements in matrix, $W_1$ is the multiplication of these two partial derivatives that we just calculated. \n",
    "\n",
    "$$ \\delta_{ij} = \\frac{\\partial y}{\\partial W_{ij}^1} = W_j^2 \\cdot \\Phi_j^\\prime \\cdot x_i $$\n",
    "\n",
    "Since in the example, we have two inputs and three hidden neurons, we will have six gradients to calculate, \n",
    "\n",
    "$$ \\delta_{11} = W_1^2 \\cdot \\Phi_1^\\prime \\cdot x_1 $$\n",
    "\n",
    "$$ \\delta_{12} = W_2^2 \\cdot \\Phi_2^\\prime \\cdot x_1 $$\n",
    "\n",
    "$$ \\delta_{13} = W_3^2 \\cdot \\Phi_3^\\prime \\cdot x_1 $$\n",
    "\n",
    "$$ \\delta_{21} = W_1^2 \\cdot \\Phi_1^\\prime \\cdot x_2 $$\n",
    "\n",
    "$$ \\delta_{22} = W_2^2 \\cdot \\Phi_2^\\prime \\cdot x_2 $$\n",
    "\n",
    "$$ \\delta_{23} = W_3^2 \\cdot \\Phi_3^\\prime \\cdot x_2 $$\n",
    "\n",
    "After finding the gradient in step two, finding the incremental value of $W_{ij}$ is immediate. Again, in the case of the loss function that we're using here, it's simply the multiplication of the gradient by alpha and by $(d - y)$. \n",
    "\n",
    "$$ \\nabla W_{ij} = \\alpha \\cdot \\left(d-y\\right) \\cdot \\delta_{ij} $$\n",
    "\n",
    "At the end of the back propagation part, each element in the weight matrix can be updated by the incremental values we calculated using these two steps. \n",
    "\n",
    "$$ W_{new} = W_{previous} + \\nabla W_{ij}^1 $$\n",
    "\n",
    "If we have more layers which is usually the case, we will have more steps. You can imagine that the process becomes more complicated. Luckily, we have programming tools for that. \n",
    "\n",
    "In all of these calculation we did not emphasize the biased input as it does not change any of the concepts we covered. Simply consider the bias is a constant input that is also connected to each of the neurons of the hidden layers by weight. The only difference between the bias and the other inputs is the fact that it remains the same as each of the other inputs change. \n",
    "\n",
    "In this example, for each new input we updated the weights after each calculation of the output. It is often beneficial to update the weights once every $n$ steps. This is called **mini batch training** and involves averaging the changes to the weights over multiple steps before acutally updating the weights. \n",
    "\n",
    "$$ \\delta = \\frac{1}{N} \\sum_{k}^N (\\delta_{ij})_k $$\n",
    "\n",
    "There are two primary reasons for using mini batch training. \n",
    "The first is to reduce the complexity of the training process since fewer computations are required. The second and more important is that when we average multiple, possibly noisy changes to the weights, we end up with a less noisy correction. This means that the learning process may actually converge faster and more accurately. \n",
    "\n",
    "## Quiz: Backpropagation\n",
    "\n",
    "![backpropquiz](chain_rule_quiz.png)\n",
    "\n",
    "What is the update rule of weight matrix $W_1$?\n",
    "\n",
    "There are two separate paths which $W_1$ contributes to the output in:\n",
    "\n",
    "- Path A: $Y$ > $h_3$ > $h_1$ > $X$\n",
    "- Path B: $Y$ > $h_4$ > $h_1$ > $X$\n",
    "\n",
    "### Answer:\n",
    "Path A + Path B, while applying the chain rule. \n",
    "\n",
    "$$ \\frac{\\partial y}{\\partial W_1} = \\frac{\\partial y}{\\partial h_3} \\frac{\\partial h_3}{\\partial h_1} \\frac{\\partial h_1}{\\partial W_1} + \\frac{\\partial y}{\\partial h_4} \\frac{\\partial h_4}{\\partial h_1} \\frac{\\partial h_1}{\\partial W_1}$$\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
