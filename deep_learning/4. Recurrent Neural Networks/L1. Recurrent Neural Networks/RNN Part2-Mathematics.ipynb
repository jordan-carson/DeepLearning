{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## RNNs Part A\n",
    "\n",
    "We are now ready for Recurrent Neural Networks. They are called recurrent because they repeat a task ovek. They perform the same task for each element in the input sequence. RNNs also attempt to address the need for capturing information and previous inputs by maintaining internal memory elements, also known as states. Many applications have temporal dependencies.  Meaning that the current output depends not only on the current input, but also on a memory element which takes into account past inputs. For cases like these, we need to use RNN's. A good example for the use of RNN is predicting the next word in a sentence. \n",
    "\n",
    "RNNs are based on the same principles behind feedforward neural networks, which is why we spend so much time reminding ourselves of the latter. Just as feedforward neural networks, in the RNN network, the inputs and outputs can also be many-to-many, many-to-one, and one-to-many. there are two fundamental differences between RNN's and feedforward neural networks. \n",
    "\n",
    "The first is the manner by which we define out inputs and outputs. Instead of training the network using a sinlge-input,  single-output at each time step, we train with sequences since previous inputs matter. \n",
    "\n",
    "The second difference, steps from the memory elements that RNNs host. Current inputs, as well as activations of neurons serve as inputs to the next time step. \n",
    "\n",
    "In feedforward neural networks, we saw a flow of information from the inout to the output without any feedback. Now, that feedforward scheme changes, and includes the feedback or memory elements. We will consider memory defined, as the output of the hidden layer. which will serve as an additional input to the network at the following training step. We will no longer use $h$ as the output of the hidden layer, but $s$ for state, referring to a system with memory. \n",
    "\n",
    "The basic scheme of RNN is called Simple RNN, and is also known as an Elman Network. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Backpropagation Through Time (part a)\n",
    "\n",
    "The process is similar to that of a feed forward neural network (ffnn), with the exception that we need to consider previous steps. This process is called **backpropagation through time (BPTT)** \n",
    "\n",
    "The state vector\n",
    "$$ \\bar{s_t} = \\Phi\\left(\\bar{x}_{t}\\cdot W_x + \\bar{s}_{t-1}\\cdot W_s\\right)$$\n",
    "$$ \\bar{s_t} = tahn\\left(\\bar{x}_{t}\\cdot W_x + \\bar{s}_{t-1}\\cdot W_s\\right)$$\n",
    "\n",
    "The output vector $\\bar{y}_t$ can be a product of the state vector $\\bar{s}_t$ and the corresponding weight elements of matrix $W_y$. If the desired outputs are between 0 and 1, we can use a softmax function. The following set of equations depicts these calculations:\n",
    "\n",
    "$$\\bar{y}_t = \\bar{s}_t W_y$$\n",
    "$$Or$$\n",
    "$$\\bar{y}_t = \\sigma\\left(\\bar{s}_t W_y\\right)$$\n",
    "\n",
    "For the error calculations we will use the loss function, where $E_t$ represents the output error at time $t$, $d_t$ represents the desired output at time $t$, and $y_t$ represents the calculated output at time $t$. \n",
    "\n",
    "$$E_t = (\\bar{d}_t - \\bar{y}_t)^2$$\n",
    "$d$ = desired output, $y$ = calculated output\n",
    "\n",
    "In **BPTT** we train the network at timestep $t$ as well as take into account all of the previous timesteps. \n",
    "\n",
    "To update each weight matrix, we need to find the partial derivatives of the loss function at time 3, as a function of all the weight matrices. We will modify each matrix using gradient descent while considering the previous timesteps. \n",
    "\n",
    "We will use the loss function for our error. The loss function is the square of the difference between the quare of the desired and the calculated outputs. \n",
    "\n",
    "We will use MSE for regression problems and cross entropy for classification problems. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Backpropagation Through Time (part b)\n",
    "\n",
    "Let's now unfold the model. We will see that unfolding the model in time is very helpful in visualizing the number of steps (translated into multiplication) needed in the Backpropagation Through Time process. These mutliplications stem from the chain rule and are easily visualized using this model. \n",
    "\n",
    "In this model, we have three weight matrices that we want to modify. \n",
    "\n",
    "![rnn_backprop](rnn_backprop1.png)\n",
    "\n",
    "The weight matrix $W_x$ linking the network inputs to the state or the hidden layer, the weight matrix $W_s$ connecting one stage to the next, and the weight matrix $W_y$ connecting this data to the output. \n",
    "\n",
    "\n",
    "Let's start with adjusting $\\mathbf{W_y}$, at time $t = 3$\n",
    "\n",
    "the derivative of the squarred error with respect to $W_y$ is found by a simple one step chain rule, and equals to the derivative of the squarred error with respect to the output, mutiplied by the derivative of the output with respect to the weight matrix $W_y$. As always, these derivatives will be calculated according to each element of the weight matrix. \n",
    "\n",
    "$$ \\frac{\\partial E_3}{\\partial W_y} = \\frac{\\partial E_3}{\\partial \\bar{y}_3}\\cdot\\frac{\\partial\\bar{y}_3}{\\partial W_3}$$\n",
    "\n",
    "\n",
    "\n",
    "To adjust the other two matrices, we will need to use backpropagation through time, and it doesn't matter which one we choose to adjust first. Let's choose to focus on the weight matrix $\\mathbf{W_s}$, which is the weight matrix connecting one state to the next.\n",
    "\n",
    "![ws](weight_matrix_ws.png)\n",
    "\n",
    "At first glance, it may seem that when finding the derivative with respect to $W_s$, we only need to consider state $S_3$. This way the derivative of timestep, $t=3$\n",
    "\n",
    "$$\\frac{\\partial E_3}{\\partial W_s}=\\frac{\\partial E_3}{\\partial\\bar{y}_3}\\cdot\\frac{\\partial\\bar{y}_3}{\\partial\\bar{s}_3}$$\n",
    "\n",
    "Simply equals the derivative of the squared error with respect to the output, multiplid by the derivative of the output with respect to $S_3$, and multiplied by the derivative of $S_3$ with respect to the weight matrix $W_s$. But $S_3$ also depends on $S_2$ and $S_1$ the previous states. We also need to take into account, what has happened before, and add to that contribution to our calculation. So, we will continue calculating the gradient, knowing that we need to accumulate the contributions, **accumulative gradient**, originating from each of the previous states. \n",
    "\n",
    "\n",
    "When we consider $S_2$, we have the following path contributing to the backpropagation. \n",
    "\n",
    "![s3](s3.png)\n",
    "\n",
    "We can clearly see that $S_3$ depends on $S_2$, giving us the derivative calculations using the chain rule, all the way back to the derivative of $S_2$ with respect to the matrix $W_s$. \n",
    "\n",
    "\n",
    "$$\\frac{\\partial E_3}{\\partial W_s}=\\frac{\\partial E_3}{\\partial\\bar{y}_3}\\cdot\\frac{\\partial\\bar{y}_3}{\\partial\\bar{s}_3}\\cdot\\frac{\\partial\\bar{s}_3}{\\partial W_s} + \\frac{\\partial E_3}{\\partial\\bar{y}_3}\\cdot\\frac{\\partial\\bar{y}_3}{\\partial\\bar{s}_3}\\cdot\\frac{\\partial\\bar{s}_3}{\\partial\\bar{s}_2}\\cdot\\frac{\\partial\\bar{s}_2}{\\partial W_s}+\\frac{\\partial E_3}{\\partial\\bar{y}_3}\\cdot\\frac{\\partial\\bar{y}_3}{\\partial\\bar{s}_3}\\cdot\\frac{\\partial\\bar{s}_3}{\\partial\\bar{s}_2}\\cdot\\frac{\\partial\\bar{s}_2}{\\partial\\bar{s}_1}\n",
    "\\cdot\\frac{\\partial\\bar{s}_1}{\\partial W_s}\n",
    "$$\n",
    "\n",
    "But we are not done yet, we need to go one more step back to the first state $S_1$, giving us the calculations again by using the chain rule, all the way back to the derivative of $S_1$ with respect to the weight matrix $W_s$. So now lets look at the accumulative gradient we have by using **BPTT*.\n",
    "\n",
    "$$\\frac{\\partial E_3}{\\partial W_s}=\\frac{\\partial E_3}{\\partial\\bar{y}_3}\\cdot\\frac{\\partial\\bar{y}_3}{\\partial\\bar{s}_3}\\cdot\\frac{\\partial\\bar{s}_3}{\\partial W_s} + \\frac{\\partial E_3}{\\partial\\bar{y}_3}\\cdot\\frac{\\partial\\bar{y}_3}{\\partial\\bar{s}_3}\\cdot\\frac{\\partial\\bar{s}_3}{\\partial\\bar{s}_2}\\cdot\\frac{\\partial\\bar{s}_2}{\\partial W_s}+\\frac{\\partial E_3}{\\partial\\bar{y}_3}\\cdot\\frac{\\partial\\bar{y}_3}{\\partial\\bar{s}_3}\\cdot\\frac{\\partial\\bar{s}_3}{\\partial\\bar{s}_2}\\cdot\\frac{\\partial\\bar{s}_2}{\\partial\\bar{s}_1}\n",
    "\\cdot\\frac{\\partial\\bar{s}_1}{\\partial W_s}\n",
    "$$\n",
    "\n",
    "Which we calculated considering all the state vectors that we have, state vector $S_3$, state vector $S_2$, and state vector $S_1$. Generally, speaking we consider multiple time steps back, \n",
    "\n",
    "\n",
    "**Adjusting weight matrix $W_s$**\n",
    "$$\\frac{\\partial E_N}{\\partial W_s} = \\sum_{i=1}^N \\frac{\\partial E_N}{\\partial\\bar{y}_N}\\cdot\\frac{\\partial\\bar{y}_N}{\\partial\\bar{s}_i}\\cdot\\frac{\\partial\\bar{s}_i}{\\partial W_s}$$\n",
    "\n",
    "and need a general framework to define **Backpropagation Through Time**, for the purpose of chaning $W_s$. \n",
    "\n",
    "\n",
    "\n",
    "\n",
    "## Backpropagation Through Time (part c)\n",
    "\n",
    "We still have to adjust $W_x$, the weight matrix connecting the input layer to the hidden or state layer. \n",
    "\n",
    "![w11x](wx_backprop.png)\n",
    "\n",
    "We will to see that the process we follow to adjust $W_X$ will be very similar to the one we used when updating $W_s$. \n",
    "\n",
    "![wx2](wx_2.png)\n",
    "\n",
    "\n",
    "If we look at timestep,$t =3$ , the error with respect to the matrix $W_x$ depends not only on vector $S_3$ but also on $S_2$ and its predecessor $S_1$, which are all affected by the same matrix $W_x$. At first glance it looks like we only need to consider $\\bar{s}_3$. \n",
    "\n",
    "$$\\frac{\\partial E_3}{\\partial W_x} = \\frac{\\partial E_3}{\\partial\\bar{y}_3}\\cdot\\frac{\\partial\\bar{y}_3}{\\partial\\bar{s}_3}$$\n",
    "\n",
    "The derivative of timestep $t = 3$, by using the chain rule of course, simply equals to the derivative of the square error with respect to the output $y_3$ multiplied by the derivative of the output with respect to $S_3$ and finally, multiplied by the derivative of $S_3$ with respect to the weight matrix $W_x$. \n",
    "\n",
    "$$\\frac{\\partial E_3}{\\partial W_x} = \\frac{\\partial E_3}{\\partial\\bar{y}_3}\\cdot\\frac{\\partial\\bar{y}_3}{\\partial\\bar{s}_3}\\cdot\\frac{\\partial\\bar{s}_3}{\\partial W_x}+$$\n",
    "\n",
    "_not finished_\n",
    "\n",
    "But we know that $S_3$ also depends on $S_2$ and $S_1$,\n",
    "which are all affected by the weight matrix $W_x$ so the gradient that we're looking for is not only the product of the three derivatives we just saw but it is the accumulation of all of the contributions originating from each of the previous states. So let's consider the previous state, $S_2$, again, by using the chain rule, we can see the following path giving us an additional contribution to the overall gradient. \n",
    "\n",
    "![wx3](wx_3.png)\n",
    "\n",
    "\n",
    "$$\\frac{\\partial E_3}{\\partial W_x} = \\frac{\\partial E_3}{\\partial\\bar{y}_3}\\cdot\\frac{\\partial\\bar{y}_3}{\\partial\\bar{s}_3}\\cdot\\frac{\\partial\\bar{s}_3}{\\partial W_x}+\\frac{\\partial E_3}{\\partial\\bar{y}_3}\\cdot\\frac{\\partial\\bar{y}_3}{\\partial\\bar{s}_3}\\cdot\\frac{\\partial\\bar{s}_3}{\\partial \\bar{s}_2}\\frac{\\partial\\bar{s}_2}{\\partial W_x}+$$\n",
    "\n",
    "Starting from the output and back propagating to the first state, we will provide the following additional component to the overall gradient.\n",
    "\n",
    "![wxs1](wx_s1.png)\n",
    "\n",
    "We will provide the following additional component to the overall gradient. \n",
    "\n",
    "$$\\frac{\\partial E_3}{\\partial W_x} = \\frac{\\partial E_3}{\\partial\\bar{y}_3}\\cdot\\frac{\\partial\\bar{y}_3}{\\partial\\bar{s}_3}\\cdot\\frac{\\partial\\bar{s}_3}{\\partial W_x}+\\frac{\\partial E_3}{\\partial\\bar{y}_3}\\cdot\\frac{\\partial\\bar{y}_3}{\\partial\\bar{s}_3}\\cdot\\frac{\\partial\\bar{s}_3}{\\partial \\bar{s}_2}\\cdot\\frac{\\partial\\bar{s}_2}{\\partial W_x}\n",
    "+\\frac{\\partial E_3}{\\partial\\bar{y}_3}\\cdot\\frac{\\partial\\bar{y}_3}{\\partial\\bar{s}_3}\\cdot\\frac{\\partial\\bar{s}_3}{\\partial \\bar{s}_2}\\cdot\\frac{\\partial\\bar{s}_2}{\\partial \\bar{s}_1}\\cdot\\frac{\\partial\\bar{s}_1}{\\partial W_x}\n",
    "$$\n",
    "\n",
    "Let's look again at the accumulative gradient we have using backpropagation through time which we calculated considering all the state vectors we have, state $S_3$, $S_2$, and $S_1$. \n",
    "\n",
    "![s321](s321.png)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "This is the complete gradient needed for the purpose of correctly updating the matrix $W_x$, Generally speaking, we need to consider multiple past timesteps and not just three, as in this example and need a general framework to define **backpropagation through time** for the purpose of updating $W_x$.  \n",
    "\n",
    "$$ \\frac{\\partial E_N}{\\partial W_x} = \\sum_{i=1}^N \\frac{\\partial E_N}{\\partial\\bar{y}_N}\\cdot\\frac{\\partial\\bar{y}_N}{\\partial\\bar{s}_i}\\cdot\\frac{\\partial\\bar{s}_i}{\\partial W_x}$$\n",
    "\n",
    "\n",
    "**Accumulative Gradient at time $\\mathbf{t = 3}$**\n",
    "\n",
    "\n",
    "\n",
    "$$\\frac{\\partial E_3}{\\partial W_x} = \\frac{\\partial E_3}{\\partial\\bar{y}_3}\\cdot\\frac{\\partial\\bar{y}_3}{\\partial\\bar{s}_3}\\cdot\\frac{\\partial\\bar{s}_3}{\\partial W_x}+\\frac{\\partial E_3}{\\partial\\bar{y}_3}\\cdot\\frac{\\partial\\bar{y}_3}{\\partial\\bar{s}_3}\\cdot\\frac{\\partial\\bar{s}_3}{\\partial\\bar{s}_2}\\cdot\\frac{\\partial\\bar{s}_2}{\\partial W_x}+\\frac{\\partial E_3}{\\partial\\bar{y}_3}\\cdot\\frac{\\partial\\bar{y}_3}{\\partial\\bar{s}_3}\\cdot\\frac{\\partial\\bar{s}_3}{\\partial\\bar{s}_2}\\cdot\\frac{\\partial\\bar{s}_2}{\\partial\\bar{s}_1}\\cdot\\frac{\\partial\\bar{s}_1}{\\partial W_x}$$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "![q1](quiz_question.png)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Quiz Question 1\n",
    "\n",
    "Consider the above folded RNN Model. Both states $\\mathbf{S}$ and $\\mathbf{Z}$ have multiple neurons in each layer. The mathematical derivation of state $\\mathbf{Z}$ at time $\\mathbf{t}$ is:\n",
    "\n",
    "$$\\bar{z}_t = \\phi\\left(\\bar{s}_t v_1 + \\bar{z}_{t-1}w_2\\right)$$\n",
    "\n",
    "**Solution**\n",
    "$\\bar{z}$and $\\bar{s}$ are vectors, as we indicate that they have multiple neurons in each layer. Using this logic we can understand that equations A and C are incorrect. Since $w_2$ connects the hidden state $\\bar{z}$ to itself, we know that we need to consider the previous timestep here. Therefore only equation D is the correct one."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Quiz Question 2: \n",
    "\n",
    "Lets look at the same folded model again (displayed above). Assume that the error is noted by the symbol $\\mathbf{E}$. What is the update rule of weight matrix V1 at time t, over a single timestep ?\n",
    "\n",
    "\n",
    "$$ \\Delta v_1 = - \\alpha\\frac{\\partial E_t}{\\partial v_1} = -\\alpha\\frac{\\partial E_t}{\\partial\\bar{y}_t}\\cdot\\frac{\\partial\\bar{y}_t}{\\partial\\bar{Z}_t}\\cdot\\frac{\\partial\\bar{Z}_t}{\\partial\\bar{v}_1}$$\n",
    "\n",
    "**Solution**\n",
    "Equation B is the only queation with the correct derivation of the chain rule with the proper use of the learning rate. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Quiz Question 3\n",
    "\n",
    "Lets look at the same folded model again (displayed above). Assume that the error is noted by the symbol E. What is the update rule of weight matrix U at time t+1 (over 2 timesteps) ? Hint: Use the unfolded model for a better visualization.\n",
    "\n",
    "$$\\frac{\\partial E_{t+1}}{\\partial U} = \n",
    "\\frac{\\partial E_{t+1}}{\\partial\\bar{y}_{t+1}}\\cdot\\frac{\\partial\\bar{y}_{t+1}}{\\partial\\bar{z}_{t+1}}\\cdot\\frac{\\partial\\bar{z}_{t+1}}{\\partial\\bar{s}_{t+1}}\\cdot\\frac{\\partial\\bar{s}_{t+1}}{\\partial U} + \n",
    "\\frac{\\partial E_{t+1}}{\\partial\\bar{y}_{t+1}}\\cdot\\frac{\\partial\\bar{y}_{t+1}}{\\partial\\bar{z}_{t+1}}\\cdot\\frac{\\partial\\bar{z}_{t+1}}{\\partial\\bar{z}_t}\\cdot\\frac{\\partial\\bar{z}_t}{\\partial\\bar{s}_t}\\cdot\\frac{\\partial\\bar{s}_t}{\\partial U} + \n",
    "\\frac{\\partial E_{t+1}}{\\partial\\bar{y}_{t+1}}\\cdot\\frac{\\partial\\bar{y}_{t+1}}{\\partial\\bar{z}_{t+1}}\\cdot\\frac{\\partial\\bar{z}_{t+1}}{\\partial\\bar{s}_{t+1}}\\cdot\\frac{\\partial\\bar{s}_{t+1}}{\\partial\\bar{s}_t}\\cdot\\frac{\\partial\\bar{s}_t}{\\partial U}$$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Gradient Clipping\n",
    "\n",
    "When the gradient becomes too large, what we do is we check in each time step whether the gradient exceeds a certain threshold. \n",
    "\n",
    "$$ \\delta = \\frac{\\partial y}{\\partial W_{ij}} > \\text{Threshold ?}$$\n",
    "\n",
    "If it does, we normalize the successive gradient. Normalizing means that we penalize super large gradients, more than those that are slightly larger than our threshold. Clipping large gradients this way helps avoid the Exploding Gradient Problem. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
