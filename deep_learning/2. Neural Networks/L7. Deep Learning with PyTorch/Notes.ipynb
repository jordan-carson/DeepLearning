{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Notes on PyTorch\n",
    "\n",
    "Sigmoid is good for outputting a probability, thus use the sigmoid if you want the output of your neural network to be a probability.\n",
    "\n",
    "`torch.manual_seed(n)`\n",
    "\n",
    "This will provide us a random number generation to allow our predictions to be generative. So we can generate these predictions again.\n",
    "\n",
    "`torch.randn(row, col)`\n",
    "\n",
    "This function above will create a Tensor of normal variables, values randomly distributed according to the normal distribution with a mean of zero and standard deviation of one."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([[ 0.5941, -0.1271, -0.7287,  0.7212, -0.5660]])"
      ]
     },
     "execution_count": 12,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import torch\n",
    "torch.manual_seed(18)\n",
    "torch.randn((1, 5))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "`torch.randn_like`\n",
    "This creates a random tensor looking at the shape of the input tensor, and created another tensor with a normal distribution. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "`torch.mm(features, weights)` or `torch.matmul(features, weights)`\n",
    "\n",
    "This performs a matrix multiplication, but remember the rows of the first input must match the columns of the second.\n",
    "\n",
    "Need to be careful with the shape of the tensors being passed through. \n",
    "\n",
    "`tensor.shape` similar to numpy will allow us to figure out the shapes of our inputs, if it doesn't match, we can reshape, resize or view.\n",
    "\n",
    "`torch.randn((1, 5)).view(5,1)` or `torch.randn((1, 5)).reshape(5,1)` or `torch.randn((1, 5)).resize_(5,1)`\n",
    "\n",
    "\n",
    "**Note** the '_' _underscore_ at the end implies -In-Place operations."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "torch.Size([5, 1])"
      ]
     },
     "execution_count": 16,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "torch.randn((1, 5)).resize_(5,1).shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "torch.Size([5, 1])"
      ]
     },
     "execution_count": 17,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "torch.randn((1, 5)).view(5, 1).shape"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Torchvision\n",
    "\n",
    "Is a package sits alongside PyTorch, and provides a lot of nice utilities like datasets and models for doing computer vision problems. \n",
    "\n",
    "#### Transforms\n",
    "\n",
    "It also has a module for common image transformations. And can be chained together using `transforms.Compose(...)` the `...` represents a list of transforms. \n",
    "\n",
    "\n",
    "**Example**\n",
    "```python\n",
    "from torchvision import transforms\n",
    "\n",
    "transform = transforms.Compose([transforms.Resize(28),\n",
    "                                transforms.ToTensor(),\n",
    "transforms.Normalize(mean=(0.5, 0.5, 0.5), std=(0.5, 0.5, 0.5))])\n",
    "```\n",
    "\n",
    "In the above example, Resize changes the shape of all images in the transform, `ToTensor()` changes all image types to Tensors, and Normalize will normalize a tensor image with mean and standard deviation. Given mean: `(M1, ..., Mn)` and std: `(S1,...,Sn)` for `n` channels, this transform will normalize each channel of the input `torch.*Tensor` i.e. `input[channel] = (input[channel] - mean[channel]) / std[channel]`"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Batch Size\n",
    "\n",
    "What that means and every time we get a set of images and labels out, were getting the batch size number of the images and labels from the dataloader. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": true
   },
   "source": [
    "## Backprop\n",
    "\n",
    "Training multilayer networks is done through backprop, which is just an application of the chainrule from calc. \n",
    "\n",
    "<img src='intro-to-pytorch/assets/backprop_diagram.png' width=550px>\n",
    "\n",
    "In the forward pass through our network, our data and operations go from bottom to top here. We pass the input $x$ through a linear transformation $L_1$ with Weights $W_1$ and biases $b_1$. The output then goes through the sigmoid operation $S$ and another linear transformation $L_2$. Finally we calculate the loss $\\ell$. We use the loss as a measure of how bad the network's predictions are. The goal then is to adjust the weights and biases to minimize the loss. \n",
    "\n",
    "To train the weights with gradient descent, we propagate the gradient of the loss backwards through the network. Each operation has some gradient between the inputs and outputs. As we send the gradients backwards, **we multiply the incoming gradient with the gradient for the operation.** Mathematically this is really just calculating the gradient of the loss with respect to the weights using the chain rule. \n",
    "\n",
    "$$\n",
    "\\large \\frac{\\partial \\ell}{\\partial W_1} = \\frac{\\partial L_1}{\\partial W_1} \\frac{\\partial S}{\\partial L_1} \\frac{\\partial L_2}{\\partial S} \\frac{\\partial \\ell}{\\partial L_2}\n",
    "$$\n",
    "\n",
    "\n",
    "We update our weights using this gradient with some learning rate $\\alpha$. \n",
    "\n",
    "$$\n",
    "\\large W^\\prime_1 = W_1 - \\alpha \\frac{\\partial \\ell}{\\partial W_1}\n",
    "$$\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# Build a feed-forward network\n",
    "# define the loss\n",
    "# get our data\n",
    "# flatten the images\n",
    "# do a forward pass\n",
    "# get our logits\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Autograd \n",
    "\n",
    "Now that we know how to calculate a loss, torch, provides a module called Autograd for automatically calculating the gradients of tensors. Which we will use to calculate the gradients of all our parameters with respect to the loss.\n",
    "\n",
    "Autograd works by keeping track of operations performed on tensors, then going backwards through those operations, calculating gradients along the way. To make sure torch keeps track of operations on a tensor and calculates the gradients, you need to set `requires_grad = True` on a tensor. \n",
    "\n",
    "We can also turn off gradients for a block of code using `torch.no_grad()`\n",
    "\n",
    "Example:\n",
    "\n",
    "```python\n",
    "x = torch.zeros(1, requires_grad=True)\n",
    ">>> with torch.no_grad()::\n",
    "...    y = x * 2\n",
    ">>> y.requires_grad\n",
    "False\n",
    "\n",
    "```"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor([[ 1.2412,  0.0113],\n",
      "        [-0.0857,  1.9370]], requires_grad=True)\n",
      "tensor([[1.5406e+00, 1.2746e-04],\n",
      "        [7.3512e-03, 3.7521e+00]], grad_fn=<PowBackward0>)\n",
      "<PowBackward0 object at 0x10fab3ac8>\n"
     ]
    }
   ],
   "source": [
    "x = torch.randn(2, 2, requires_grad=True)\n",
    "print(x)\n",
    "\n",
    "y = x ** 2\n",
    "print(y)\n",
    "\n",
    "print(y.grad_fn)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We can see the operation that created `y`, a div operation `DivBackward0`\n",
    "\n",
    "The autograd module keeps track of these operations and knows how to calculate the gradient for each one. In this way, it's able to calculate the gradients for a chain of operations with respect to any one tensor. \n",
    "\n",
    "Let's reduce the y tensor to a scalar value, the mean."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor(1.3251, grad_fn=<MeanBackward1>)\n",
      "None\n"
     ]
    }
   ],
   "source": [
    "z = y.mean()\n",
    "print(z)\n",
    "\n",
    "# check the gradients for x, and y, but they are empty -> None\n",
    "print(x.grad)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "To calculate the gradients, we need to run the `.backward()` method on a Variable, `z` for example. This will calculate the gradient for `z` with respect to `x`.\n",
    "\n",
    "$$\n",
    "\\frac{\\partial z}{\\partial x} = \\frac{\\partial}{\\partial x}\\left[\\frac{1}{n}\\sum_i^n x_i^2\\right] = \\frac{x}{2}\n",
    "$$"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor([[ 0.6206,  0.0056],\n",
      "        [-0.0429,  0.9685]])\n",
      "tensor([[ 0.6206,  0.0056],\n",
      "        [-0.0429,  0.9685]], grad_fn=<DivBackward0>)\n"
     ]
    }
   ],
   "source": [
    "z.backward()\n",
    "print(x.grad)\n",
    "print(x/2)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "These gradients calculations are particularly useful for neural networks. For training we need the gradients of the weights with respect to the cost. With PyTorch, we run data forward through the network to calculate the loss, then, go backwards to calculate the gradients with respect to the loss. Once we have the gradients we can make a gradient descent step. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Combining Loss and Autograd\n",
    "\n",
    "When we are creating any network with torch, all of the parameters are initialized with `requires_grad=True`. This means that when we calculate the loss and call `loss.backward()`, the gradients for the parameters are calculated. These gradients are used to update the weights with gradient descent. Below is an example of calculating the gradients using a backwards pass."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "```python\n",
    "from torch import nn\n",
    "\n",
    "model = nn.Sequential(nn.Linear(784, 128),\n",
    "                      nn.ReLU(),\n",
    "                      nn.Linear(128, 64),\n",
    "                      nn.ReLU(), \n",
    "                      nn.Linear(64, 10),\n",
    "                      nn.LogSoftmax(dim=1))\n",
    "\n",
    "criterion = nn.NLLLoss()\n",
    "images, labels = next(iter(trainloader))\n",
    "images = images.view(images.shape[0], -1)\n",
    "\n",
    "log_ps = model(images)\n",
    "loss = criterion(logps, labels)\n",
    "```"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "```python\n",
    "print('Before backward pass: \\n', model[0].weight.grad)\n",
    "\n",
    "loss.backward()\n",
    "\n",
    "print('After backward pass: \\n', model[0].weight.grad)\n",
    "```"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Training the network\n",
    "\n",
    "After all of this, we need an optimizer that we will use to update the weights with the gradients. We get these from PyTorch's optim package. For exmaple we use SGD with `optim.SGD`. Creating an optimizer is as easy as."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "from torch import optim\n",
    "\n",
    "optimizer = optim.Adam(model.parameters(), lr=0.003)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The general process for doing one learning step before looping all the data is.\n",
    "\n",
    "- Make a forward pass through the network\n",
    "- Use the network output (logits) to calculate the loss\n",
    "- Perform a backward pass through the network with `loss.backward()` to calculate the gradients. \n",
    "- Take a step with the optimizer to update the weights.\n",
    "\n",
    "We need to be careful because we for learning step, we need to zero the gradients as the gradients are accumulated. This means that we need to zero the gradients on each training pass or you'll retain the gradients from previous training batches. This can be done by using `optimizer.zero_grad()`. \n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Inference and Validation\n",
    "\n",
    "The goal of validation is to measure the model's performance on data that isn't part of the training set. Performance here is up to the developer to define though. \n",
    "\n",
    "With the probabilities, we can get the most likely class using the `ps.topk` method. This returns the $k$ highest values. Since we just want the most likely class, we can use `ps.topk(1)`. This returns a tuple of the top-$k$ and the top-$k$ indices. If the highest value is the fifth element, we'll get back 4 as the index.\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Dropout\n",
    "\n",
    "During training we want to use dropout to prevent overfitting, but during inference we want to use the entire network. So, we need to turn off dropout during validation, testing, and whenever we're using the network to make predictions. To do this, you use `model.eval()`. This sets the model to evaluation mode where the dropout probability is 0. You can turn dropout back on by setting the model to train mode with `model.train()`. In general, the pattern for the validation loop will look like this, where you turn off gradients, set the model to evaluation mode, calculate the validation loss and metric, then set the model back to train mode.\n",
    "\n",
    "```python\n",
    "# turn off gradients\n",
    "with torch.no_grad():\n",
    "    \n",
    "    # set model to evaluation mode\n",
    "    model.eval()\n",
    "    \n",
    "    # validation pass here\n",
    "    for images, labels in testloader:\n",
    "        ...\n",
    "\n",
    "# set model back to train mode\n",
    "model.train()\n",
    "```"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Saving and Loading Models\n",
    "\n",
    "It's impractical to train a network every time you need to use it. Instead, we can save trained networks then load them later to train more or use them for predictions. \n",
    "\n",
    "The parameters for PyTorch networks are stored in a model's `state_dict`. We can see the state dict contains the weight and bias matrices for each of our layers. \n",
    "\n",
    "The simplest thing to do is simply save the state dict with `torch.save`. For example, we can **save** it to a file `'checkpoint.pth'`.\n",
    "\n",
    "```python\n",
    ">>> torch.save(model.state_dict(), 'checkpoint.pth')\n",
    "```\n",
    "\n",
    "Then we can load the state dict with `torch.load`. \n",
    "\n",
    "```python\n",
    ">>> state_dict = torch.load('checkpoint.pth')\n",
    "print(state_dict.keys())\n",
    "```\n",
    "\n",
    "And to load the state dict in to the network, you do\n",
    "```python\n",
    ">>> model.load_state_dit(state_dict)\n",
    "```\n",
    "\n",
    "Seems pretty straightforward, but as usual it's a bit more complicated. Loading the state dict works only if the model architecture is exactly the same as the checkpoint architecture. If I create a model with a different architecture, this fails.\n",
    "\n",
    "```python\n",
    ">>> model = fc_model.Network(784, 10, [400, 200, 100])\n",
    "\n",
    "# This will throw an error because the tensor sizes are wrong!\n",
    ">>> model.load_state_dict(state_dict)\n",
    "```\n",
    "\n",
    "This means we need to rebuild the model exactly as it was when trained. Information about the model architecture needs to be saved in the checkpoint, along with the state dict. To do this, you build a dictionary with all the information you need to compeletely rebuild the model.\n",
    "\n",
    "```python\n",
    ">>> checkpoint = {'input_size': 784,\n",
    "              'output_size': 10,\n",
    "              'hidden_layers': [each.out_features for each in model.hidden_layers],\n",
    "              'state_dict': model.state_dict()}\n",
    "\n",
    ">>> torch.save(checkpoint, 'checkpoint.pth')\n",
    "```\n",
    "\n",
    "Now the checkpoint has all the necessary information to rebuild the trained model. You can easily make that a function if you want. Similarly, we can write a function to load checkpoints.\n",
    "\n",
    "```python\n",
    ">>> def load_checkpoint(filepath):\n",
    "...    checkpoint = torch.load(filepath)\n",
    "...    model = fc_model.Network(checkpoint['input_size'],\n",
    "                             checkpoint['output_size'],\n",
    "                             checkpoint['hidden_layers'])\n",
    "...    model.load_state_dict(checkpoint['state_dict'])\n",
    "    \n",
    "...    return model\n",
    "\n",
    ">>> model = load_checkpoint('checkpoint.pth')\n",
    ">>> print(model)\n",
    "\n",
    "```"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
