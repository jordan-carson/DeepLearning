{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Notes on PyTorch\n",
    "\n",
    "Sigmoid is good for outputting a probability, thus use the sigmoid if you want the output of your neural network to be a probability.\n",
    "\n",
    "`torch.manual_seed(n)`\n",
    "\n",
    "This will provide us a random number generation to allow our predictions to be generative. So we can generate these predictions again.\n",
    "\n",
    "`torch.randn(row, col)`\n",
    "\n",
    "This function above will create a Tensor of normal variables, values randomly distributed according to the normal distribution with a mean of zero and standard deviation of one."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([[ 0.5941, -0.1271, -0.7287,  0.7212, -0.5660]])"
      ]
     },
     "execution_count": 12,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import torch\n",
    "torch.manual_seed(18)\n",
    "torch.randn((1, 5))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "`torch.randn_like`\n",
    "This creates a random tensor looking at the shape of the input tensor, and created another tensor with a normal distribution. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "`torch.mm(features, weights)` or `torch.matmul(features, weights)`\n",
    "\n",
    "This performs a matrix multiplication, but remember the rows of the first input must match the columns of the second.\n",
    "\n",
    "Need to be careful with the shape of the tensors being passed through. \n",
    "\n",
    "`tensor.shape` similar to numpy will allow us to figure out the shapes of our inputs, if it doesn't match, we can reshape, resize or view.\n",
    "\n",
    "`torch.randn((1, 5)).view(5,1)` or `torch.randn((1, 5)).reshape(5,1)` or `torch.randn((1, 5)).resize_(5,1)`\n",
    "\n",
    "\n",
    "**Note** the '_' _underscore_ at the end implies -In-Place operations."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "torch.Size([5, 1])"
      ]
     },
     "execution_count": 16,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "torch.randn((1, 5)).resize_(5,1).shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "torch.Size([5, 1])"
      ]
     },
     "execution_count": 17,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "torch.randn((1, 5)).view(5, 1).shape"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Torchvision\n",
    "\n",
    "is a package sits alongside PyTorch, and provides a lot of nice utilities like datasets and models for doing computer vision problems. \n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Batch Size\n",
    "\n",
    "What that means and every time we get a set of images and labels out, were getting the batch size number of the images and labels from the dataloader. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": true
   },
   "source": [
    "## Backprop\n",
    "\n",
    "Training multilayer networks is done through backprop, which is just an application of the chainrule from calc. \n",
    "\n",
    "<img src='intro-to-pytorch/assets/backprop_diagram.png' width=550px>\n",
    "\n",
    "In the forward pass through our network, our data and operations go from bottom to top here. We pass the input $x$ through a linear transformation $L_1$ with Weights $W_1$ and biases $b_1$. The output then goes through the sigmoid operation $S$ and another linear transformation $L_2$. Finally we calculate the loss $\\ell$. We use the loss as a measure of how bad the network's predictions are. The goal then is to adjust the weights and biases to minimize the loss. \n",
    "\n",
    "To train the weights with gradient descent, we propagate the gradient of the loss backwards through the network. Each operation has some gradient between the inputs and outputs. As we send the gradients backwards, **we multiply the incoming gradient with the gradient for the operation.** Mathematically this is really just calculating the gradient of the loss with respect to the weights using the chain rule. \n",
    "\n",
    "$$\n",
    "\\large \\frac{\\partial \\ell}{\\partial W_1} = \\frac{\\partial L_1}{\\partial W_1} \\frac{\\partial S}{\\partial L_1} \\frac{\\partial L_2}{\\partial S} \\frac{\\partial \\ell}{\\partial L_2}\n",
    "$$\n",
    "\n",
    "\n",
    "We update our weights using this gradient with some learning rate $\\alpha$. \n",
    "\n",
    "$$\n",
    "\\large W^\\prime_1 = W_1 - \\alpha \\frac{\\partial \\ell}{\\partial W_1}\n",
    "$$\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# Build a feed-forward network\n",
    "# define the loss\n",
    "# get our data\n",
    "# flatten the images\n",
    "# do a forward pass\n",
    "# get our logits\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Autograd \n",
    "\n",
    "Now that we know how to calculate a loss, torch, provides a module called Autograd for automatically calculating the gradients of tensors. Which we will use to calculate the gradients of all our parameters with respect to the loss.\n",
    "\n",
    "Autograd works by keeping track of operations performed on tensors, then going backwards through those operations, calculating gradients along the way. To make sure torch keeps track of operations on a tensor and calculates the gradients, you need to set `requires_grad = True` on a tensor. \n",
    "\n",
    "We can also turn off gradients for a block of code using `torch.no_grad()`\n",
    "\n",
    "Example:\n",
    "\n",
    "```python\n",
    "x = torch.zeros(1, requires_grad=True)\n",
    ">>> with torch.no_grad()::\n",
    "...    y = x * 2\n",
    ">>> y.requires_grad\n",
    "False\n",
    "\n",
    "```"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor([[ 1.2412,  0.0113],\n",
      "        [-0.0857,  1.9370]], requires_grad=True)\n",
      "tensor([[1.5406e+00, 1.2746e-04],\n",
      "        [7.3512e-03, 3.7521e+00]], grad_fn=<PowBackward0>)\n",
      "<PowBackward0 object at 0x10fab3ac8>\n"
     ]
    }
   ],
   "source": [
    "x = torch.randn(2, 2, requires_grad=True)\n",
    "print(x)\n",
    "\n",
    "y = x ** 2\n",
    "print(y)\n",
    "\n",
    "print(y.grad_fn)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We can see the operation that created `y`, a div operation `DivBackward0`\n",
    "\n",
    "The autograd module keeps track of these operations and knows how to calculate the gradient for each one. In this way, it's able to calculate the gradients for a chain of operations with respect to any one tensor. \n",
    "\n",
    "Let's reduce the y tensor to a scalar value, the mean."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor(1.3251, grad_fn=<MeanBackward1>)\n",
      "None\n"
     ]
    }
   ],
   "source": [
    "z = y.mean()\n",
    "print(z)\n",
    "\n",
    "# check the gradients for x, and y, but they are empty -> None\n",
    "print(x.grad)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "To calculate the gradients, we need to run the `.backward()` method on a Variable, `z` for example. This will calculate the gradient for `z` with respect to `x`.\n",
    "\n",
    "$$\n",
    "\\frac{\\partial z}{\\partial x} = \\frac{\\partial}{\\partial x}\\left[\\frac{1}{n}\\sum_i^n x_i^2\\right] = \\frac{x}{2}\n",
    "$$"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor([[ 0.6206,  0.0056],\n",
      "        [-0.0429,  0.9685]])\n",
      "tensor([[ 0.6206,  0.0056],\n",
      "        [-0.0429,  0.9685]], grad_fn=<DivBackward0>)\n"
     ]
    }
   ],
   "source": [
    "z.backward()\n",
    "print(x.grad)\n",
    "print(x/2)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "These gradients calculations are particularly useful for neural networks. For training we need the gradients of the weights with respect to the cost. With PyTorch, we run data forward through the network to calculate the loss, then, go backwards to calculate the gradients with respect to the loss. Once we have the gradients we can make a gradient descent step. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Combining Loss and Autograd\n",
    "\n",
    "When we are creating any network with torch, all of the parameters are initialized with `requires_grad=True`. This means that when we calculate the loss and call `loss.backward()`, the gradients for the parameters are calculated. These gradients are used to update the weights with gradient descent. Below is an example of calculating the gradients using a backwards pass."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "from torch import nn\n",
    "\n",
    "model = nn.Sequential(nn.Linear(784, 128),\n",
    "                      nn.ReLU(),\n",
    "                      nn.Linear(128, 64),\n",
    "                      nn.ReLU(), \n",
    "                      nn.Linear(64, 10),\n",
    "                      nn.LogSoftmax(dim=1))\n",
    "\n",
    "criterion = nn.NLLLoss()\n",
    "images, labels = next(iter(trainloader))\n",
    "images = images.view(images.shape[0], -1)\n",
    "\n",
    "log_ps = model(images)\n",
    "loss = criterion(logps, labels)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "print('Before backward pass: \\n', model[0].weight.grad)\n",
    "\n",
    "loss.backward()\n",
    "\n",
    "print('After backward pass: \\n', model[0].weight.grad)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Training the network\n",
    "\n",
    "After all of this, we need an optimizer that we will use to update the weights with the gradients. We get these from PyTorch's optim package. For exmaple we use SGD with `optim.SGD`. Creating an optimizer is as easy as."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "from torch import optim\n",
    "\n",
    "optimizer = optim.Adam(model.parameters(), lr=0.003)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The general process for doing one learning step before looping all the data is.\n",
    "\n",
    "- Make a forward pass through the network\n",
    "- Use the network output (logits) to calculate the loss\n",
    "- Perform a backward pass through the network with `loss.backward()` to calculate the gradients. \n",
    "- Take a step with the optimizer to update the weights.\n",
    "\n",
    "We need to be careful because we for learning step, we need to zero the gradients as the gradients are accumulated. This means that we need to zero the gradients on each training pass or you'll retain the gradients from previous training batches. This can be done by using `optimizer.zero_grad()`. \n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Inference and Validation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
